# -*- coding: utf-8 -*-
"""Amazon_analysis1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X6pR1edHncI2JO0RzBWukMoGSmH4_NQG
"""

pip install econml

from google.colab import drive
drive.mount('/content/drive')

# Import necessary libraries
import pandas as pd
import numpy as np
import statsmodels.api as sm
from linearmodels.panel import PanelOLS, RandomEffects
from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats

# ===============================
# 1. Data Loading & Preprocessing
# ===============================
# Load the CSV files (update the paths as needed)
purchases = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv', parse_dates=['Order Date'])
survey = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')

# Merge the datasets on 'Survey ResponseID'
df = pd.merge(purchases, survey, on='Survey ResponseID', how='inner')

# Create a year column from the 'Order Date'
df['year'] = df['Order Date'].dt.year

# Compute spending for each order
df['spending'] = df['Purchase Price Per Unit'] * df['Quantity']

# Aggregate to the user-year level:
# - frequency: count of orders per user per year
# - spending: total spending per user per year
panel = df.groupby(['Survey ResponseID', 'year']).agg({
    'spending': 'sum',
    'ASIN/ISBN (Product Code)': 'count'
}).rename(columns={'ASIN/ISBN (Product Code)': 'frequency'}).reset_index()

# Merge the panel data with the survey (demographic) data
panel = pd.merge(panel, survey, on='Survey ResponseID', how='left')

# Create dummy variables for demographic factors
# (Adjust the column names if needed based on your actual survey column headers)
demo_cols = ['Q-demos-age', 'Q-demos-income', 'Q-demos-education', 'Q-demos-race', 'Q-demos-gender']
panel = pd.get_dummies(panel, columns=demo_cols, drop_first=True)

# Set the multi-index required for panel data methods (user and year)
panel = panel.set_index(['Survey ResponseID', 'year'])

# ===============================
# 2. Panel Data Regression Analysis
#    (Fixed Effects with drop_absorbed=True and Random Effects)
# ===============================
# Prepare independent variables.
# Note: When using fixed effects, time-invariant variables (e.g., constant or dummies that do not vary over time)
# will be absorbed by the entity effects.
# We include all demographic dummies (their names should start with 'Q-demos') plus a constant.
demo_dummy_cols = [col for col in panel.columns if col.startswith('Q-demos')]
panel['const'] = 1  # constant term (will be dropped in FE)

exog = panel[['const'] + demo_dummy_cols]

# --- (a) Fixed Effects Model for Frequency ---
# Using drop_absorbed=True to automatically drop collinear (absorbed) variables
model_fe_freq = PanelOLS(panel['frequency'], exog, entity_effects=True, drop_absorbed=True)
res_fe_freq = model_fe_freq.fit()
print("Fixed Effects Model for Frequency")
print(res_fe_freq.summary)

# --- (b) Random Effects Model for Frequency ---
model_re_freq = RandomEffects(panel['frequency'], exog)
res_re_freq = model_re_freq.fit()
print("Random Effects Model for Frequency")
print(res_re_freq.summary)

# --- (c) Fixed Effects Model for Spending ---
model_fe_spending = PanelOLS(panel['spending'], exog, entity_effects=True, drop_absorbed=True)
res_fe_spending = model_fe_spending.fit()
print("Fixed Effects Model for Spending")
print(res_fe_spending.summary)

# --- (d) Random Effects Model for Spending ---
model_re_spending = RandomEffects(panel['spending'], exog)
res_re_spending = model_re_spending.fit()
print("Random Effects Model for Spending")
print(res_re_spending.summary)

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# For machine learning models and evaluation
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# For model interpretation
import shap
shap.initjs()

# -------------------------------
# 1. Data Loading and Aggregation
# -------------------------------

# Load the purchase and survey data
# (Update file paths as needed)
purchases = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv', parse_dates=['Order Date'])
survey = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')

# Merge the two datasets using the common key "Survey ResponseID"
df = pd.merge(purchases, survey, on='Survey ResponseID', how='inner')

# Aggregate the transaction data at the user level.
# Here, we compute the "actual purchase frequency" as the number of transactions per user.
actual_freq = df.groupby('Survey ResponseID').size().reset_index(name='actual_purchase_freq')

# Merge the aggregated frequency with the survey data so that each respondent has their self‐reported survey responses plus the observed purchase frequency.
data = pd.merge(survey, actual_freq, on='Survey ResponseID', how='left')
data['actual_purchase_freq'] = data['actual_purchase_freq'].fillna(0)  # if some respondents made no purchases

# -------------------------------
# 2. Descriptive Analytics: Self‑Reported vs. Actual Behavior
# -------------------------------

# Let’s examine some self‐reported Amazon usage variables.
# (Adjust these column names as appropriate; here we assume that the survey includes questions like:
#   Q-amazon-use-howmany (self-reported purchase frequency),
#   Q-amazon-use-hh-size (household size),
#   Q-amazon-use-how-oft (how often they use Amazon), etc.)
print("Summary Statistics of Self-Reported Measures and Actual Purchase Frequency:")
print(data[['Q-amazon-use-howmany', 'Q-amazon-use-hh-size', 'Q-amazon-use-how-oft', 'actual_purchase_freq']].describe())

# Plot the distributions of self‑reported purchase frequency and observed purchase frequency
plt.figure(figsize=(10,5))
sns.histplot(data['Q-amazon-use-howmany'], color='blue', label='Self-reported frequency', kde=True, stat="density", bins=20)
sns.histplot(data['actual_purchase_freq'], color='red', label='Actual purchase frequency', kde=True, stat="density", bins=20, alpha=0.6)
plt.xlabel("Frequency")
plt.title("Distribution of Self-Reported vs. Actual Purchase Frequency")
plt.legend()
plt.show()

# Scatter plot to see the relationship between self‐reported frequency and actual frequency
plt.figure(figsize=(8,6))
sns.scatterplot(x='Q-amazon-use-howmany', y='actual_purchase_freq', data=data, alpha=0.7)
plt.xlabel("Self-Reported Purchase Frequency (Q-amazon-use-howmany)")
plt.ylabel("Actual Purchase Frequency (Count)")
plt.title("Self-Reported vs. Actual Purchase Frequency")
plt.show()

# -------------------------------
# 3. Predictive Analytics using Machine Learning
# -------------------------------
# Research Question: Can survey responses (including self-reported usage and demographics) predict the actual observed purchase frequency?

# For this demonstration, we select a subset of survey features.
# (You may adjust this list based on your research focus and available variables.)
features = ['Q-demos-age', 'Q-demos-hispanic', 'Q-demos-race', 'Q-demos-education', 'Q-demos-income',
            'Q-demos-gender', 'Q-amazon-use-howmany', 'Q-amazon-use-hh-size', 'Q-amazon-use-how-oft']

# Make sure these columns exist in the data; if necessary, print data.columns to check.
print("Available columns in survey data:")
print(data.columns)

# Define X (predictors) and y (target: actual observed purchase frequency)
X = data[features]
y = data['actual_purchase_freq']

# Preprocess the predictors:
# Since many survey questions are categorical, convert them into dummy variables.
X_encoded = pd.get_dummies(X, drop_first=True)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

# --- (a) Random Forest Regressor ---
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)

mse_rf = mean_squared_error(y_test, rf_pred)
r2_rf = r2_score(y_test, rf_pred)
print("Random Forest Regressor Performance:")
print("MSE:", mse_rf)
print("R^2:", r2_rf)

# Cross-validate the Random Forest model using 5-fold cross-validation
cv_scores = cross_val_score(rf, X_encoded, y, cv=5, scoring='neg_mean_squared_error')
cv_rmse = np.sqrt(-cv_scores)
print("Cross-Validation RMSE (Random Forest):", cv_rmse)
print("Mean CV RMSE (Random Forest):", cv_rmse.mean())

# --- (b) Gradient Boosting Regressor ---
gb = GradientBoostingRegressor(n_estimators=100, random_state=42)
gb.fit(X_train, y_train)
gb_pred = gb.predict(X_test)

mse_gb = mean_squared_error(y_test, gb_pred)
r2_gb = r2_score(y_test, gb_pred)
print("Gradient Boosting Regressor Performance:")
print("MSE:", mse_gb)
print("R^2:", r2_gb)

cv_scores_gb = cross_val_score(gb, X_encoded, y, cv=5, scoring='neg_mean_squared_error')
cv_rmse_gb = np.sqrt(-cv_scores_gb)
print("Cross-Validation RMSE (Gradient Boosting):", cv_rmse_gb)
print("Mean CV RMSE (Gradient Boosting):", cv_rmse_gb.mean())

# -------------------------------
# 4. Model Interpretation using SHAP
# -------------------------------
# We now use SHAP to understand which survey responses are most predictive of actual purchase frequency.
# For tree-based models, TreeExplainer is efficient.

explainer_rf = shap.TreeExplainer(rf)
shap_values_rf = explainer_rf.shap_values(X_test)

# Plot a SHAP summary plot (bar plot shows average absolute importance)
shap.summary_plot(shap_values_rf, X_test, plot_type="bar", show=False)
plt.title("SHAP Feature Importance (Random Forest)")
plt.show()

# A standard SHAP summary plot (beeswarm plot) shows how each feature affects predictions
shap.summary_plot(shap_values_rf, X_test, show=False)
plt.title("SHAP Beeswarm Plot (Random Forest)")
plt.show()

# Optional: Display a force plot for one individual observation
# (This opens an interactive plot; in Colab it will render in the notebook.)
shap.force_plot(explainer_rf.expected_value, shap_values_rf[0,:], X_test.iloc[0,:])

import pandas as pd
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
import matplotlib.pyplot as plt
import seaborn as sns
from linearmodels.panel import PanelOLS

# Load dataset (Assumed structure)
data = pd.read_csv("/content/drive/MyDrive/amazon/amazon-purchases.csv")

# Convert date to datetime and sort values
data['date'] = pd.to_datetime(data['date'])
data = data.sort_values(by=['consumer_id', 'date'])

# Generate pre/post event indicator
data['post_event'] = (data['date'] >= data['event_date']).astype(int)

# Creating Difference-in-Differences variable
data['treated'] = data['experienced_life_event']  # 1 if they had a life event, 0 otherwise
data['DiD'] = data['treated'] * data['post_event']

# Panel data structure: Set index
panel_data = data.set_index(['consumer_id', 'date'])

# Event Study Plot
avg_spending = data.groupby(['days_since_event'])['spending'].mean()
plt.figure(figsize=(10,5))
sns.lineplot(x=avg_spending.index, y=avg_spending.values)
plt.axvline(x=0, linestyle='--', color='red')  # Mark the event point
plt.title("Event Study: Spending Before & After Life Event")
plt.xlabel("Days Since Event")
plt.ylabel("Average Spending")
plt.show()

# Difference-in-Differences Model
model = smf.ols('spending ~ treated + post_event + DiD', data=data).fit()
print(model.summary())

# Fixed Effects Regression (Controlling for Individual-Level Heterogeneity)
fe_model = PanelOLS.from_formula(
    'spending ~ post_event + DiD + EntityEffects',
    data=panel_data
).fit()
print(fe_model.summary)

# Subgroup Analysis: Effect by Category
category_results = data.groupby(['product_category', 'days_since_event'])['spending'].mean().unstack()
category_results.plot(figsize=(12,6))
plt.axvline(x=0, linestyle='--', color='red')
plt.title("Spending Patterns by Category Before & After Life Event")
plt.xlabel("Days Since Event")
plt.ylabel("Average Spending per Category")
plt.show()

# %% [code]
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.formula.api as smf
import statsmodels.api as sm
from datetime import datetime

# Optional: For the machine learning approach (Double Machine Learning)
# Uncomment the next line if needed:
# !pip install econml
from econml.dml import LinearDML
from sklearn.linear_model import LassoCV
from sklearn.ensemble import RandomForestRegressor

# %% [code]
# ========================================
# 1. LOAD AND INSPECT THE DATASETS
# ========================================
# Set this flag to True to work on a smaller sample (e.g., 1000 users) for reduced memory/compute.
use_sample = True
sample_size = 1000

# Define file paths (adjust as needed)
purchases_path = '/content/drive/MyDrive/amazon/amazon-purchases.csv'
survey_path = '/content/drive/MyDrive/amazon/survey.csv'

# Load amazon-purchases.csv
# Specify dtypes for memory efficiency.
purchases_dtype = {
    'Order Date': 'str',
    'Purchase Price Per Unit': 'float32',
    'Quantity': 'int8',
    'Shipping Address State': 'str',
    'Title': 'str',
    'ASIN/ISBN (Product Code)': 'str',
    'Category': 'str',
    'Survey ResponseID': 'str'
}
purchases = pd.read_csv(purchases_path, dtype=purchases_dtype)
purchases.columns = purchases.columns.str.strip()
print("Columns in amazon-purchases.csv:")
print(purchases.columns)

# Rename columns using a dictionary.
rename_dict = {
    'Order Date': 'Order_date',
    'Purchase Price Per Unit': 'Price',
    'Shipping Address State': 'Shipping_Address_State',
    'Title': 'Title',
    'ASIN/ISBN (Product Code)': 'Product_Code',
    'Category': 'Category',
    'Survey ResponseID': 'Survey_ResponseID'
}
print("\nRenaming columns as follows:")
print(rename_dict)
purchases = purchases.rename(columns=rename_dict)

# Convert 'Order_date' to datetime.
# Your sample dates look like "2018-12-04", so use '%Y-%m-%d'.
purchases['Order_date'] = pd.to_datetime(purchases['Order_date'], format='%Y-%m-%d')

# Downcast numeric columns if possible.
purchases['Price'] = pd.to_numeric(purchases['Price'], errors='coerce').astype('float32')
purchases['Quantity'] = pd.to_numeric(purchases['Quantity'], errors='coerce').astype('int8')

# Convert Survey_ResponseID to a categorical string later if needed.

# %% [code]
# Load the survey data.
survey_dtype = {
    'Survey ResponseID': 'str',
    # Other survey columns are read as default (object)
}
survey = pd.read_csv(survey_path, dtype=survey_dtype)
survey.columns = survey.columns.str.strip()
# Rename Survey ResponseID in the survey data.
rename_dict_survey = {
    'Survey ResponseID': 'Survey_ResponseID'
}
survey = survey.rename(columns=rename_dict_survey)

# %% [code]
# ========================================
# 2. MERGE DATASETS
# ========================================
# Merge purchases and survey on 'Survey_ResponseID'
data = pd.merge(purchases, survey, on='Survey_ResponseID', how='left')

# Optionally reduce the dataset size by sampling a subset of users.
if use_sample:
    unique_ids = data['Survey_ResponseID'].unique()
    # Randomly sample (or take first sample_size elements)
    sampled_ids = np.random.choice(unique_ids, size=sample_size, replace=False)
    data = data[data['Survey_ResponseID'].isin(sampled_ids)]
    print(f"Processing a sample of {sample_size} users.")

# Convert key string IDs to categorical for efficiency.
data['Survey_ResponseID'] = data['Survey_ResponseID'].astype('category')

# %% [code]
# ========================================
# 3. PREPARE THE MONTHLY PANEL OF SPENDING
# ========================================
# Create a monthly period from Order_date.
data['year_month'] = data['Order_date'].dt.to_period('M')

# Compute total spending per order.
data['total_spend_order'] = data['Price'] * data['Quantity']

# Aggregate total spending per user per month.
monthly_spending = data.groupby(['Survey_ResponseID', 'year_month'], observed=True).agg({
    'total_spend_order': 'sum'
}).reset_index()
monthly_spending.rename(columns={'total_spend_order': 'total_spend'}, inplace=True)

# Convert 'year_month' from Period to string for statsmodels fixed effects.
monthly_spending['year_month'] = monthly_spending['year_month'].astype(str)
# Also convert Survey_ResponseID to category.
monthly_spending['Survey_ResponseID'] = monthly_spending['Survey_ResponseID'].astype('category')

# %% [code]
# ========================================
# 4. CREATE TREATMENT INDICATOR BASED ON LIFE EVENTS
# ========================================
# We assume that if the survey question "Q-life-changes" is nonempty, a life event occurred.
# Convert the field to string and remove extra whitespace.
data['Q-life-changes'] = data['Q-life-changes'].astype(str).str.strip()
data['life_event'] = data['Q-life-changes'].apply(
    lambda x: 1 if x and x.lower() not in ['nan', 'none', ''] else 0
)

# Create a consumer-level treatment indicator (if ever reported a life event).
consumer_events = data.groupby('Survey_ResponseID', observed=True)['life_event'].max().reset_index()
consumer_events.rename(columns={'life_event': 'treatment'}, inplace=True)

# Merge the treatment indicator into the monthly spending panel.
monthly_spending = pd.merge(monthly_spending, consumer_events, on='Survey_ResponseID', how='left')
monthly_spending['treatment'] = monthly_spending['treatment'].fillna(0)

# %% [code]
# ========================================
# 5. DEFINE EVENT TIMING & CREATE DiD VARIABLES
# ========================================
# For illustration, assume all treated users had a life event in January 2020.
event_date = pd.Period('2020-01', freq='M')

# Create a dummy variable for post-event: 1 if month is on/after event_date.
monthly_spending['post_event'] = monthly_spending['year_month'].apply(
    lambda x: 1 if pd.Period(x, freq='M') >= event_date else 0
)

# Create the DiD interaction: treatment * post_event.
monthly_spending['treat_post'] = monthly_spending['treatment'] * monthly_spending['post_event']

# %% [code]
# ========================================
# 6. DIFFERENCE-IN-DIFFERENCES (DiD) REGRESSION
# ========================================
# Convert fixed effects variables to categorical (they are already, but this is explicit)
monthly_spending['Survey_ResponseID'] = monthly_spending['Survey_ResponseID'].astype('category')
monthly_spending['year_month'] = monthly_spending['year_month'].astype('category')

# DiD regression specification.
did_formula = 'total_spend ~ treat_post + C(Survey_ResponseID) + C(year_month)'

# Estimate the DiD model with cluster robust SE (clustering by Survey_ResponseID).
did_model = smf.ols(did_formula, data=monthly_spending).fit(
    cov_type='cluster', cov_kwds={'groups': monthly_spending['Survey_ResponseID']}
)
print("=== Difference-in-Differences Results ===")
print(did_model.summary())

# %% [code]
# ========================================
# 7. EVENT STUDY ANALYSIS
# ========================================
# Compute the number of months relative to the event.
monthly_spending['months_since_event'] = monthly_spending['year_month'].apply(
    lambda x: (pd.Period(x, freq='M') - event_date).n
)

# Create dummy variables for each relative month.
dummies = pd.get_dummies(monthly_spending['months_since_event'], prefix='month')
event_study_data = pd.concat([monthly_spending, dummies], axis=1)

# Choose a baseline period to omit from the dummies (if available, e.g., 'month_-1').
baseline = 'month_-1' if 'month_-1' in dummies.columns else None
relative_month_vars = [col for col in dummies.columns if col != baseline]

# Construct the event study regression formula.
formula_event = 'total_spend ~ ' + ' + '.join(relative_month_vars) + ' + C(Survey_ResponseID) + C(year_month)'

# Estimate the event study regression.
event_model = smf.ols(formula_event, data=event_study_data).fit(
    cov_type='cluster', cov_kwds={'groups': event_study_data['Survey_ResponseID']}
)
print("\n=== Event Study Regression Results ===")
print(event_model.summary())

# Plot the dynamic treatment effects.
coef = event_model.params[relative_month_vars]
conf_int = event_model.conf_int().loc[relative_month_vars]
# Extract relative month numbers (e.g., "month_2" becomes 2)
months = [int(var.split('_')[1]) for var in relative_month_vars]

plt.figure(figsize=(10,6))
plt.errorbar(months, coef,
             yerr=[coef - conf_int[0], conf_int[1] - coef],
             fmt='o', capsize=5)
plt.axhline(0, color='gray', linestyle='--')
plt.xlabel('Months Since Event')
plt.ylabel('Coefficient')
plt.title('Event Study: Dynamic Impact on Total Monthly Spending')
plt.show()

# %% [code]
# ========================================
# 8. SUBGROUP ANALYSIS: INTERACTION WITH DEMOGRAPHICS (e.g., INCOME)
# ========================================
# Merge additional demographic data (e.g., Q-demos-income) from the survey.
monthly_spending = pd.merge(
    monthly_spending,
    survey[['Survey_ResponseID', 'Q-demos-income']],
    on='Survey_ResponseID',
    how='left'
)

# Run a DiD regression interacting treatment effect with income.
subgroup_formula = 'total_spend ~ treat_post * C(Q-demos-income) + C(Survey_ResponseID) + C(year_month)'
subgroup_model = smf.ols(subgroup_formula, data=monthly_spending).fit(
    cov_type='cluster', cov_kwds={'groups': monthly_spending['Survey_ResponseID']}
)
print("\n=== Subgroup Analysis (Interaction with Income) Results ===")
print(subgroup_model.summary())

# %% [code]
# ========================================
# 9. OPTIONAL: MACHINE LEARNING APPROACH (DOUBLE MACHINE LEARNING)
# ========================================
# Here we use econML's LinearDML as an example.
# Outcome: total_spend, Treatment: treat_post, Controls: treatment (and additional controls if needed).
X = monthly_spending[['treatment']]  # Add more control variables if desired.
T = monthly_spending['treat_post']
y = monthly_spending['total_spend']

dml_estimator = LinearDML(model_y=LassoCV(cv=3),
                          model_t=RandomForestRegressor(n_estimators=50, random_state=123),
                          discrete_treatment=True,
                          cv=3)
dml_estimator.fit(y, T, X=X)
treatment_effects = dml_estimator.effect(X)
print("\n=== Double Machine Learning (DML) Estimated Treatment Effects ===")
print(treatment_effects)

ate = dml_estimator.ate(X)
print("\nEstimated Average Treatment Effect (ATE) from DML:", ate)

# %% [code]
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.formula.api as smf
import statsmodels.api as sm
from datetime import datetime

# Optional: For the machine learning approach (Double Machine Learning)
# Uncomment the next line to install econML if you haven't already.
# !pip install econml
from econml.dml import LinearDML
from sklearn.linear_model import LassoCV
from sklearn.ensemble import RandomForestRegressor

# %% [code]
# ========================================
# 1. LOAD AND INSPECT THE DATASETS
# ========================================
# Set this flag to True to work on a smaller sample (e.g., 1000 users) for reduced memory/compute.
use_sample = True
sample_size = 1000

# Define file paths (adjust as needed)
purchases_path = '/content/drive/MyDrive/amazon/amazon-purchases.csv'
survey_path = '/content/drive/MyDrive/amazon/survey.csv'

# Specify dtypes for memory efficiency.
# Note: "Quantity" is now set as int16 instead of int8 to avoid conversion errors.
purchases_dtype = {
    'Order Date': 'str',
    'Purchase Price Per Unit': 'float32',
    'Quantity': 'int16',
    'Shipping Address State': 'str',
    'Title': 'str',
    'ASIN/ISBN (Product Code)': 'str',
    'Category': 'str',
    'Survey ResponseID': 'str'
}

# Load amazon-purchases.csv with the specified dtypes.
purchases = pd.read_csv(purchases_path, dtype=purchases_dtype)
purchases.columns = purchases.columns.str.strip()
print("Columns in amazon-purchases.csv:")
print(purchases.columns)

# Rename columns using a dictionary.
rename_dict = {
    'Order Date': 'Order_date',
    'Purchase Price Per Unit': 'Price',
    'Shipping Address State': 'Shipping_Address_State',
    'Title': 'Title',
    'ASIN/ISBN (Product Code)': 'Product_Code',
    'Category': 'Category',
    'Survey ResponseID': 'Survey_ResponseID'
}
print("\nRenaming columns as follows:")
print(rename_dict)
purchases = purchases.rename(columns=rename_dict)

# Convert 'Order_date' to datetime.
# Your dates are in the format "2018-12-04", so use '%Y-%m-%d'.
purchases['Order_date'] = pd.to_datetime(purchases['Order_date'], format='%Y-%m-%d')

# Downcast numeric columns if needed.
purchases['Price'] = pd.to_numeric(purchases['Price'], errors='coerce').astype('float32')
# "Quantity" is already set as int16 from the dtype dictionary.

# %% [code]
# Load the survey data.
survey_dtype = {
    'Survey ResponseID': 'str'
    # Other survey columns will be read as object by default.
}
survey = pd.read_csv(survey_path, dtype=survey_dtype)
survey.columns = survey.columns.str.strip()

# Rename the Survey ResponseID column in the survey data.
rename_dict_survey = {
    'Survey ResponseID': 'Survey_ResponseID'
}
survey = survey.rename(columns=rename_dict_survey)

# %% [code]
# ========================================
# 2. MERGE DATASETS
# ========================================
# Merge purchases and survey on 'Survey_ResponseID'
data = pd.merge(purchases, survey, on='Survey_ResponseID', how='left')

# Optionally reduce the dataset size by sampling a subset of users.
if use_sample:
    unique_ids = data['Survey_ResponseID'].unique()
    sampled_ids = np.random.choice(unique_ids, size=sample_size, replace=False)
    data = data[data['Survey_ResponseID'].isin(sampled_ids)]
    print(f"Processing a sample of {sample_size} users.")

# Convert key string IDs to categorical for efficiency.
data['Survey_ResponseID'] = data['Survey_ResponseID'].astype('category')

# %% [code]
# ========================================
# 3. PREPARE THE MONTHLY PANEL OF SPENDING
# ========================================
# Create a monthly period from Order_date.
data['year_month'] = data['Order_date'].dt.to_period('M')

# Compute total spending per order.
data['total_spend_order'] = data['Price'] * data['Quantity']

# Aggregate total spending per user per month.
monthly_spending = data.groupby(['Survey_ResponseID', 'year_month'], observed=True).agg({
    'total_spend_order': 'sum'
}).reset_index()
monthly_spending.rename(columns={'total_spend_order': 'total_spend'}, inplace=True)

# Convert 'year_month' from Period type to string for statsmodels fixed effects.
monthly_spending['year_month'] = monthly_spending['year_month'].astype(str)
# Also convert Survey_ResponseID to category.
monthly_spending['Survey_ResponseID'] = monthly_spending['Survey_ResponseID'].astype('category')

# %% [code]
# ========================================
# 4. CREATE TREATMENT INDICATOR BASED ON LIFE EVENTS
# ========================================
# Assume that if the survey question "Q-life-changes" is nonempty, a life event occurred.
data['Q-life-changes'] = data['Q-life-changes'].astype(str).str.strip()
data['life_event'] = data['Q-life-changes'].apply(
    lambda x: 1 if x and x.lower() not in ['nan', 'none', ''] else 0
)

# Create a consumer-level treatment indicator: if a consumer ever reported a life event.
consumer_events = data.groupby('Survey_ResponseID', observed=True)['life_event'].max().reset_index()
consumer_events.rename(columns={'life_event': 'treatment'}, inplace=True)

# Merge the treatment indicator into the monthly spending panel.
monthly_spending = pd.merge(monthly_spending, consumer_events, on='Survey_ResponseID', how='left')
monthly_spending['treatment'] = monthly_spending['treatment'].fillna(0)

# %% [code]
# ========================================
# 5. DEFINE EVENT TIMING & CREATE DiD VARIABLES
# ========================================
# For illustration, assume all treated users experienced the life event in January 2020.
event_date = pd.Period('2020-01', freq='M')

# Create a dummy variable: 1 if the month is on/after event_date.
monthly_spending['post_event'] = monthly_spending['year_month'].apply(
    lambda x: 1 if pd.Period(x, freq='M') >= event_date else 0
)

# Create the DiD interaction term.
monthly_spending['treat_post'] = monthly_spending['treatment'] * monthly_spending['post_event']

# %% [code]
# ========================================
# 6. DIFFERENCE-IN-DIFFERENCES (DiD) REGRESSION
# ========================================
# Convert fixed effects variables to categorical (if not already).
monthly_spending['Survey_ResponseID'] = monthly_spending['Survey_ResponseID'].astype('category')
monthly_spending['year_month'] = monthly_spending['year_month'].astype('category')

# Specify the DiD regression with individual and time fixed effects.
did_formula = 'total_spend ~ treat_post + C(Survey_ResponseID) + C(year_month)'

# Estimate the DiD model with cluster-robust standard errors (clustered by Survey_ResponseID).
did_model = smf.ols(did_formula, data=monthly_spending).fit(
    cov_type='cluster', cov_kwds={'groups': monthly_spending['Survey_ResponseID']}
)
print("=== Difference-in-Differences Results ===")
print(did_model.summary())

# %% [code]
# ========================================
# 7. EVENT STUDY ANALYSIS
# ========================================
# Compute the number of months relative to the event.
monthly_spending['months_since_event'] = monthly_spending['year_month'].apply(
    lambda x: (pd.Period(x, freq='M') - event_date).n
)

# Create dummy variables for each relative month.
dummies = pd.get_dummies(monthly_spending['months_since_event'], prefix='month')
event_study_data = pd.concat([monthly_spending, dummies], axis=1)

# Choose a baseline period to omit from the dummies (e.g., 'month_-1' if available).
baseline = 'month_-1' if 'month_-1' in dummies.columns else None
relative_month_vars = [col for col in dummies.columns if col != baseline]

# Construct the event study regression formula.
formula_event = 'total_spend ~ ' + ' + '.join(relative_month_vars) + ' + C(Survey_ResponseID) + C(year_month)'

# Estimate the event study regression.
event_model = smf.ols(formula_event, data=event_study_data).fit(
    cov_type='cluster', cov_kwds={'groups': event_study_data['Survey_ResponseID']}
)
print("\n=== Event Study Regression Results ===")
print(event_model.summary())

# Plot the dynamic treatment effects.
coef = event_model.params[relative_month_vars]
conf_int = event_model.conf_int().loc[relative_month_vars]
months = [int(var.split('_')[1]) for var in relative_month_vars]

plt.figure(figsize=(10,6))
plt.errorbar(months, coef,
             yerr=[coef - conf_int[0], conf_int[1] - coef],
             fmt='o', capsize=5)
plt.axhline(0, color='gray', linestyle='--')
plt.xlabel('Months Since Event')
plt.ylabel('Coefficient')
plt.title('Event Study: Dynamic Impact on Total Monthly Spending')
plt.show()

# %% [code]
# ========================================
# 8. SUBGROUP ANALYSIS: INTERACTION WITH DEMOGRAPHICS (e.g., INCOME)
# ========================================
# Merge additional demographic data (e.g., Q-demos-income) from the survey.
monthly_spending = pd.merge(
    monthly_spending,
    survey[['Survey_ResponseID', 'Q-demos-income']],
    on='Survey_ResponseID',
    how='left'
)

# Run a DiD regression that interacts the treatment effect with income.
subgroup_formula = 'total_spend ~ treat_post * C(Q-demos-income) + C(Survey_ResponseID) + C(year_month)'
subgroup_model = smf.ols(subgroup_formula, data=monthly_spending).fit(
    cov_type='cluster', cov_kwds={'groups': monthly_spending['Survey_ResponseID']}
)
print("\n=== Subgroup Analysis (Interaction with Income) Results ===")
print(subgroup_model.summary())

# %% [code]
# ========================================
# 9. OPTIONAL: MACHINE LEARNING APPROACH (DOUBLE MACHINE LEARNING)
# ========================================
# Here we use econML's LinearDML as an example.
# Outcome: total_spend, Treatment: treat_post, Controls: treatment (and additional controls if desired).
X = monthly_spending[['treatment']]  # Add additional controls if desired.
T = monthly_spending['treat_post']
y = monthly_spending['total_spend']

dml_estimator = LinearDML(model_y=LassoCV(cv=3),
                          model_t=RandomForestRegressor(n_estimators=50, random_state=123),
                          discrete_treatment=True,
                          cv=3)
dml_estimator.fit(y, T, X=X)
treatment_effects = dml_estimator.effect(X)
print("\n=== Double Machine Learning (DML) Estimated Treatment Effects ===")
print(treatment_effects)

ate = dml_estimator.ate(X)
print("\nEstimated Average Treatment Effect (ATE) from DML:", ate)

# %% [code]
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.formula.api as smf
import statsmodels.api as sm
from datetime import datetime

# Optional: For the machine learning approach (Double Machine Learning)
# Uncomment the next line if needed:
# !pip install econml
from econml.dml import LinearDML
from sklearn.linear_model import LassoCV
from sklearn.ensemble import RandomForestRegressor

# %% [code]
# ========================================
# 1. LOAD AND INSPECT THE DATASETS
# ========================================
# Set this flag to True to work on a smaller sample (e.g., 1000 users) for reduced memory/compute.
use_sample = True
sample_size = 1000

# Define file paths (adjust as needed)
purchases_path = '/content/drive/MyDrive/amazon/amazon-purchases.csv'
survey_path = '/content/drive/MyDrive/amazon/survey.csv'

# Specify dtypes for memory efficiency.
purchases_dtype = {
    'Order Date': 'str',
    'Purchase Price Per Unit': 'float32',
    'Quantity': 'int16',  # changed from int8 to int16 to avoid conversion errors
    'Shipping Address State': 'str',
    'Title': 'str',
    'ASIN/ISBN (Product Code)': 'str',
    'Category': 'str',
    'Survey ResponseID': 'str'
}

# Load amazon-purchases.csv with the specified dtypes.
purchases = pd.read_csv(purchases_path, dtype=purchases_dtype)
purchases.columns = purchases.columns.str.strip()
print("Columns in amazon-purchases.csv:")
print(purchases.columns)

# Rename columns using a dictionary.
rename_dict = {
    'Order Date': 'Order_date',
    'Purchase Price Per Unit': 'Price',
    'Shipping Address State': 'Shipping_Address_State',
    'Title': 'Title',
    'ASIN/ISBN (Product Code)': 'Product_Code',
    'Category': 'Category',
    'Survey ResponseID': 'Survey_ResponseID'
}
print("\nRenaming columns as follows:")
print(rename_dict)
purchases = purchases.rename(columns=rename_dict)

# Convert 'Order_date' to datetime.
purchases['Order_date'] = pd.to_datetime(purchases['Order_date'], format='%Y-%m-%d')

# Downcast numeric columns if needed.
purchases['Price'] = pd.to_numeric(purchases['Price'], errors='coerce').astype('float32')

# %% [code]
# Load the survey data.
survey_dtype = {
    'Survey ResponseID': 'str'
}
survey = pd.read_csv(survey_path, dtype=survey_dtype)
survey.columns = survey.columns.str.strip()
rename_dict_survey = {
    'Survey ResponseID': 'Survey_ResponseID'
}
survey = survey.rename(columns=rename_dict_survey)

# %% [code]
# ========================================
# 2. MERGE DATASETS
# ========================================
data = pd.merge(purchases, survey, on='Survey_ResponseID', how='left')

if use_sample:
    unique_ids = data['Survey_ResponseID'].unique()
    sampled_ids = np.random.choice(unique_ids, size=sample_size, replace=False)
    data = data[data['Survey_ResponseID'].isin(sampled_ids)]
    print(f"Processing a sample of {sample_size} users.")

data['Survey_ResponseID'] = data['Survey_ResponseID'].astype('category')

# %% [code]
# ========================================
# 3. PREPARE THE MONTHLY PANEL OF SPENDING
# ========================================
data['year_month'] = data['Order_date'].dt.to_period('M')
data['total_spend_order'] = data['Price'] * data['Quantity']

monthly_spending = data.groupby(['Survey_ResponseID', 'year_month'], observed=True).agg({
    'total_spend_order': 'sum'
}).reset_index()
monthly_spending.rename(columns={'total_spend_order': 'total_spend'}, inplace=True)
monthly_spending['year_month'] = monthly_spending['year_month'].astype(str)
monthly_spending['Survey_ResponseID'] = monthly_spending['Survey_ResponseID'].astype('category')

# %% [code]
# ========================================
# 4. CREATE TREATMENT INDICATOR BASED ON LIFE EVENTS
# ========================================
# Assume that if the survey question "Q-life-changes" is nonempty, a life event occurred.
data['Q-life-changes'] = data['Q-life-changes'].astype(str).str.strip()
data['life_event'] = data['Q-life-changes'].apply(
    lambda x: 1 if x and x.lower() not in ['nan', 'none', ''] else 0
)
consumer_events = data.groupby('Survey_ResponseID', observed=True)['life_event'].max().reset_index()
consumer_events.rename(columns={'life_event': 'treatment'}, inplace=True)
monthly_spending = pd.merge(monthly_spending, consumer_events, on='Survey_ResponseID', how='left')
monthly_spending['treatment'] = monthly_spending['treatment'].fillna(0)

# %% [code]
# ========================================
# 5. DEFINE EVENT TIMING & CREATE DiD VARIABLES
# ========================================
# Assume all treated users experienced the life event in January 2020.
event_date = pd.Period('2020-01', freq='M')
monthly_spending['post_event'] = monthly_spending['year_month'].apply(
    lambda x: 1 if pd.Period(x, freq='M') >= event_date else 0
)
monthly_spending['treat_post'] = monthly_spending['treatment'] * monthly_spending['post_event']

# %% [code]
# ========================================
# 6. DIFFERENCE-IN-DIFFERENCES (DiD) REGRESSION
# ========================================
monthly_spending['Survey_ResponseID'] = monthly_spending['Survey_ResponseID'].astype('category')
monthly_spending['year_month'] = monthly_spending['year_month'].astype('category')
did_formula = 'total_spend ~ treat_post + C(Survey_ResponseID) + C(year_month)'
did_model = smf.ols(did_formula, data=monthly_spending).fit(
    cov_type='cluster', cov_kwds={'groups': monthly_spending['Survey_ResponseID']}
)
print("=== Difference-in-Differences Results ===")
print(did_model.summary())

# %% [code]
# ========================================
# 7. EVENT STUDY ANALYSIS
# ========================================
# Compute the number of months relative to the event.
monthly_spending['months_since_event'] = monthly_spending['year_month'].apply(
    lambda x: (pd.Period(x, freq='M') - event_date).n
)

# Create dummy variables for each relative month.
dummies = pd.get_dummies(monthly_spending['months_since_event'], prefix='month')
event_study_data = pd.concat([monthly_spending, dummies], axis=1)

# Choose a baseline period to omit (e.g., 'month_-1' if available).
baseline = 'month_-1' if 'month_-1' in dummies.columns else None
relative_month_vars = [col for col in dummies.columns if col != baseline]

# Construct the event study regression formula.
# Wrap each dummy variable in Q() to safely reference variable names with special characters.
formula_event = 'total_spend ~ ' + ' + '.join("Q(%s)" % var for var in relative_month_vars) + ' + C(Survey_ResponseID) + C(year_month)'

# Estimate the event study regression.
event_model = smf.ols(formula_event, data=event_study_data).fit(
    cov_type='cluster', cov_kwds={'groups': event_study_data['Survey_ResponseID']}
)
print("\n=== Event Study Regression Results ===")
print(event_model.summary())

# Plot the dynamic treatment effects.
coef = event_model.params[relative_month_vars]
conf_int = event_model.conf_int().loc[relative_month_vars]
months = [int(var.split('_')[1]) for var in relative_month_vars]

plt.figure(figsize=(10,6))
plt.errorbar(months, coef,
             yerr=[coef - conf_int[0], conf_int[1] - coef],
             fmt='o', capsize=5)
plt.axhline(0, color='gray', linestyle='--')
plt.xlabel('Months Since Event')
plt.ylabel('Coefficient')
plt.title('Event Study: Dynamic Impact on Total Monthly Spending')
plt.show()

# %% [code]
# ========================================
# 8. SUBGROUP ANALYSIS: INTERACTION WITH DEMOGRAPHICS (e.g., INCOME)
# ========================================
monthly_spending = pd.merge(
    monthly_spending,
    survey[['Survey_ResponseID', 'Q-demos-income']],
    on='Survey_ResponseID',
    how='left'
)
subgroup_formula = 'total_spend ~ treat_post * C(Q-demos-income) + C(Survey_ResponseID) + C(year_month)'
subgroup_model = smf.ols(subgroup_formula, data=monthly_spending).fit(
    cov_type='cluster', cov_kwds={'groups': monthly_spending['Survey_ResponseID']}
)
print("\n=== Subgroup Analysis (Interaction with Income) Results ===")
print(subgroup_model.summary())

# %% [code]
# ========================================
# 9. OPTIONAL: MACHINE LEARNING APPROACH (DOUBLE MACHINE LEARNING)
# ========================================
X = monthly_spending[['treatment']]
T = monthly_spending['treat_post']
y = monthly_spending['total_spend']
dml_estimator = LinearDML(model_y=LassoCV(cv=3),
                          model_t=RandomForestRegressor(n_estimators=50, random_state=123),
                          discrete_treatment=True,
                          cv=3)
dml_estimator.fit(y, T, X=X)
treatment_effects = dml_estimator.effect(X)
print("\n=== Double Machine Learning (DML) Estimated Treatment Effects ===")
print(treatment_effects)
ate = dml_estimator.ate(X)
print("\nEstimated Average Treatment Effect (ATE) from DML:", ate)

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats

# -------------------------
# 1. Load and Aggregate Data
# -------------------------

# Load purchase data and survey data.
# (Replace the file paths with your actual data locations.)
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv')
survey_data = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')

# Create a new column for spending per purchase.
purchase_data['spend'] = purchase_data['Purchase Price Per Unit'] * purchase_data['Quantity']

# Aggregate purchase data by consumer.
# Here we compute:
#   - total_spend: total money spent
#   - purchase_frequency: number of purchases (using order date count)
#   - avg_price: average purchase price
#   - category_diversity: number of unique categories purchased
purchase_agg = purchase_data.groupby('Survey_ResponseID').agg(
    total_spend=('spend', 'sum'),
    purchase_frequency=('Order Date', 'count'),
    avg_price=('Purchase Price Per Unit', 'mean'),
    category_diversity=('Category', 'nunique')
).reset_index()

# -------------------------
# 2. Merge Purchase and Survey Data
# -------------------------

# Merge the aggregated purchase features with the survey data on consumer id.
# (Assuming 'Survey_ResponseID' is the common key.)
data_merged = pd.merge(survey_data, purchase_agg, on='Survey_ResponseID', how='inner')

# At this point, data_merged contains survey responses as well as the purchase summary metrics.
# For example, suppose the survey includes a column "Q_sell_YOUR_data" reflecting data privacy attitudes.

# -------------------------
# 3. Create Feature Matrix for Clustering
# -------------------------

# Choose the features that you want to use for segmentation.
# These might include both behavioral (from purchase history) and attitudinal (from survey) measures.
# For example:
features = ['total_spend', 'purchase_frequency', 'avg_price', 'category_diversity', 'Q_sell_YOUR_data']

# Create the feature matrix.
X = data_merged[features].copy()

# It is generally a good idea to scale/standardize your features for clustering.
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# -------------------------
# 4. Determine Optimal Number of Clusters via Silhouette Analysis
# -------------------------

silhouette_scores = []
k_values = range(2, 11)
for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    cluster_labels = kmeans.fit_predict(X_scaled)
    silhouette_avg = silhouette_score(X_scaled, cluster_labels)
    silhouette_scores.append(silhouette_avg)
    print(f"For n_clusters = {k}, the average silhouette_score is: {silhouette_avg:.3f}")

# Plot the silhouette scores to visually inspect the best k.
plt.figure(figsize=(8,5))
plt.plot(k_values, silhouette_scores, marker='o')
plt.xlabel("Number of clusters")
plt.ylabel("Average Silhouette Score")
plt.title("Silhouette Analysis for Determining Optimal k")
plt.show()

# Suppose the optimal number (based on the highest silhouette score) is k=4.
optimal_k = 4

# -------------------------
# 5. Cluster the Data Using K-Means
# -------------------------

kmeans_final = KMeans(n_clusters=optimal_k, random_state=42)
data_merged['cluster'] = kmeans_final.fit_predict(X_scaled)

# -------------------------
# 6. Post-Clustering Analysis
# -------------------------

# (a) Summarize cluster characteristics: compute mean values of each feature per cluster.
cluster_summary = data_merged.groupby('cluster')[features].mean()
print("\nCluster Summary (Mean Values):")
print(cluster_summary)

# (b) Visualize the clusters (if desired).
plt.figure(figsize=(10,6))
sns.boxplot(x='cluster', y='total_spend', data=data_merged)
plt.title("Distribution of Total Spend by Cluster")
plt.show()

# (c) Perform ANOVA tests to see if clusters differ significantly on key variables.
print("\nANOVA tests across clusters:")
for feature in features:
    groups = [group[feature].values for name, group in data_merged.groupby('cluster')]
    f_val, p_val = stats.f_oneway(*groups)
    print(f"{feature}: F = {f_val:.2f}, p = {p_val:.4f}")

# Import required libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns

# ---------------------------
# 1. Load the Data
# ---------------------------
# Replace these with your actual file paths or data-loading code.
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv')  # Purchase records CSV file
survey_data   = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')    # Survey responses CSV file

# ---------------------------
# 2. Verify and Align Key Identifiers
# ---------------------------
# In our segmentation, we want to merge purchase records with survey responses.
# The merge key should be a consumer identifier.
# Here we expect the column 'Survey_ResponseID' to be common.
# If your purchase data uses a different column name (e.g., 'CustomerID'),
# then rename it so that it matches the survey data.
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# Example: if purchase_data has a column called 'CustomerID' instead of 'Survey_ResponseID'
if 'CustomerID' in purchase_data.columns and 'Survey_ResponseID' not in purchase_data.columns:
    purchase_data = purchase_data.rename(columns={'CustomerID': 'Survey_ResponseID'})

# Also check that the survey data has the expected key.
if 'Survey_ResponseID' not in survey_data.columns:
    raise KeyError("The survey_data DataFrame does not have the expected 'Survey_ResponseID' column.")

# ---------------------------
# 3. Aggregate Purchase Data by Consumer
# ---------------------------
# Here we assume that purchase_data contains columns such as:
#   - 'spend': the amount spent on each purchase,
#   - 'Order Date': the date of the order (used here only to count frequency),
#   - 'Purchase Price Per Unit': the price per unit in the purchase,
#   - 'Category': the category of the product.
#
# You may need to adjust the column names below to match your data.

purchase_agg = purchase_data.groupby('Survey_ResponseID').agg(
    total_spend=('spend', 'sum'),
    purchase_frequency=('Order Date', 'count'),
    avg_price=('Purchase Price Per Unit', 'mean'),
    category_diversity=('Category', 'nunique')
).reset_index()

print("Aggregated purchase data (first 5 rows):")
print(purchase_agg.head())

# ---------------------------
# 4. Merge Purchase Aggregates with Survey Data
# ---------------------------
# Suppose that the survey_data DataFrame contains consumer attitudes,
# for example, a column 'data_privacy_attitude' that captures their privacy concern or willingness
# to monetize their data.
merged_data = pd.merge(purchase_agg, survey_data, on='Survey_ResponseID', how='inner')

print("Merged data (first 5 rows):")
print(merged_data.head())

# ---------------------------
# 5. Prepare Features for Clustering
# ---------------------------
# Define the features to be used in segmentation. You might want to combine
# purchase-derived features with survey response variables.
#
# For example, we use:
#   - total_spend
#   - purchase_frequency
#   - avg_price
#   - category_diversity
#   - data_privacy_attitude (this should be a numeric score in your survey data)
#
# Adjust the list below to include the columns you need.
features = ['total_spend', 'purchase_frequency', 'avg_price', 'category_diversity', 'data_privacy_attitude']

# Ensure that all these features are available and drop any rows with missing values.
X = merged_data[features].dropna()

# Optionally, if the survey data contains categorical responses that need encoding,
# perform that encoding (e.g., using one-hot encoding) before clustering.

# ---------------------------
# 6. Scale the Features
# ---------------------------
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ---------------------------
# 7. Determine the Optimal Number of Clusters
# ---------------------------
# Here we use the silhouette score to try cluster numbers from 2 to 10.
silhouette_scores = {}
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_scaled)
    score = silhouette_score(X_scaled, labels)
    silhouette_scores[k] = score
    print(f"For k={k}, Silhouette Score = {score:.3f}")

# Choose the k with the highest silhouette score.
best_k = max(silhouette_scores, key=silhouette_scores.get)
print(f"\nOptimal number of clusters (based on silhouette score): k = {best_k}")

# ---------------------------
# 8. Run K-Means Clustering with the Optimal k
# ---------------------------
kmeans = KMeans(n_clusters=best_k, random_state=42)
merged_data['cluster'] = kmeans.fit_predict(X_scaled)

# ---------------------------
# 9. Post-Clustering Analysis
# ---------------------------
# Get summary statistics for each cluster.
cluster_summary = merged_data.groupby('cluster').agg({
    'total_spend': ['mean', 'median'],
    'purchase_frequency': ['mean', 'median'],
    'avg_price': ['mean', 'median'],
    'category_diversity': ['mean', 'median'],
    'data_privacy_attitude': ['mean', 'median']
}).reset_index()

print("\nCluster Summary Statistics:")
print(cluster_summary)

# You can also examine the distribution of survey responses across clusters.
if 'Q_sell_YOUR_data' in merged_data.columns:
    cluster_privacy = merged_data.groupby('cluster')['Q_sell_YOUR_data'].value_counts(normalize=True).unstack()
    print("\nDistribution of 'Q_sell_YOUR_data' by Cluster:")
    print(cluster_privacy)

# ---------------------------
# 10. Visualize Cluster Profiles
# ---------------------------
# Example: boxplot of total spend by cluster.
plt.figure(figsize=(10, 6))
sns.boxplot(x='cluster', y='total_spend', data=merged_data)
plt.title('Total Spend by Cluster')
plt.xlabel('Cluster')
plt.ylabel('Total Spend')
plt.show()

# Example: scatter plot of purchase frequency vs. average price colored by cluster.
plt.figure(figsize=(10, 6))
sns.scatterplot(x='purchase_frequency', y='avg_price', hue='cluster', data=merged_data, palette='viridis')
plt.title('Purchase Frequency vs. Average Price by Cluster')
plt.xlabel('Purchase Frequency')
plt.ylabel('Average Price')
plt.legend(title='Cluster')
plt.show()

# Import required libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns

# ---------------------------
# 1. Load the Data
# ---------------------------
# Replace the filenames with the paths to your CSV files.
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv')  # Purchase records CSV file
survey_data   = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')

# Print column names for verification
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 2. Fix Key Identifier Names for Merging
# ---------------------------
# Your files use "Survey ResponseID" (with a space) instead of "Survey_ResponseID".
# Rename these columns in both dataframes so that they match.
if 'Survey_ResponseID' not in purchase_data.columns:
    if 'Survey ResponseID' in purchase_data.columns:
        purchase_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)
    else:
        raise KeyError("Purchase data does not have a column named 'Survey ResponseID' or 'Survey_ResponseID'.")

if 'Survey_ResponseID' not in survey_data.columns:
    if 'Survey ResponseID' in survey_data.columns:
        survey_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)
    else:
        raise KeyError("Survey data does not have a column named 'Survey ResponseID' or 'Survey_ResponseID'.")

print("\nAfter renaming:")
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 3. Aggregate Purchase Data by Consumer
# ---------------------------
# For each consumer (Survey_ResponseID), we compute:
#  - total_spend: total money spent.
#  - purchase_frequency: number of orders.
#  - avg_price: average purchase price per unit.
#  - category_diversity: count of unique categories purchased.
#
# Here we assume:
#   - 'Purchase Price Per Unit' is numeric,
#   - 'Quantity' is the number of units bought,
#   - 'Order Date' is used to count orders,
#   - 'Category' indicates the product category.
#
# We calculate total spend as: Quantity * Purchase Price Per Unit.
# (If your data already has a spend column, you can use that instead.)

# First, compute a total spend column in the purchase data:
purchase_data['total_spend'] = purchase_data['Quantity'] * purchase_data['Purchase Price Per Unit']

# Now group by Survey_ResponseID and aggregate the features.
purchase_agg = purchase_data.groupby('Survey_ResponseID').agg(
    total_spend=('total_spend', 'sum'),
    purchase_frequency=('Order Date', 'count'),
    avg_price=('Purchase Price Per Unit', 'mean'),
    category_diversity=('Category', 'nunique')
).reset_index()

print("\nAggregated purchase data (first 5 rows):")
print(purchase_agg.head())

# ---------------------------
# 4. Merge Purchase Aggregates with Survey Data
# ---------------------------
# Merge the aggregated purchase data with survey responses using the key 'Survey_ResponseID'.
merged_data = pd.merge(purchase_agg, survey_data, on='Survey_ResponseID', how='inner')

print("\nMerged data (first 5 rows):")
print(merged_data.head())

# ---------------------------
# 5. Prepare Features for Clustering
# ---------------------------
# For segmentation we’ll combine purchase-derived features with one survey measure.
# Here we select:
#   - total_spend
#   - purchase_frequency
#   - avg_price
#   - category_diversity
#   - Q-sell-YOUR-data  (attitude toward selling personal data)
#
# (Ensure that 'Q-sell-YOUR-data' is numeric; if not, you might need to recode it.)
features = ['total_spend', 'purchase_frequency', 'avg_price', 'category_diversity', 'Q-sell-YOUR-data']

# Check that these features exist in the merged data:
print("\nMerged data columns:", merged_data.columns.tolist())

# Drop rows with missing values in the selected features.
X = merged_data[features].dropna()

# ---------------------------
# 6. Scale the Features
# ---------------------------
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ---------------------------
# 7. Determine the Optimal Number of Clusters
# ---------------------------
# Here, we loop through k = 2 to 10 and compute the silhouette score.
silhouette_scores = {}
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_scaled)
    score = silhouette_score(X_scaled, labels)
    silhouette_scores[k] = score
    print(f"For k={k}, Silhouette Score = {score:.3f}")

# Choose the k with the highest silhouette score.
best_k = max(silhouette_scores, key=silhouette_scores.get)
print(f"\nOptimal number of clusters (based on silhouette score): k = {best_k}")

# ---------------------------
# 8. Run K-Means Clustering with the Optimal k
# ---------------------------
kmeans = KMeans(n_clusters=best_k, random_state=42)
merged_data['cluster'] = kmeans.fit_predict(X_scaled)

# ---------------------------
# 9. Post-Clustering Analysis
# ---------------------------
# Generate summary statistics for each cluster.
cluster_summary = merged_data.groupby('cluster').agg({
    'total_spend': ['mean', 'median'],
    'purchase_frequency': ['mean', 'median'],
    'avg_price': ['mean', 'median'],
    'category_diversity': ['mean', 'median'],
    'Q-sell-YOUR-data': ['mean', 'median']
}).reset_index()

print("\nCluster Summary Statistics:")
print(cluster_summary)

# If desired, you can also analyze the distribution of responses for other survey items.
if 'Q-sell-YOUR-data' in merged_data.columns:
    cluster_privacy = merged_data.groupby('cluster')['Q-sell-YOUR-data'].value_counts(normalize=True).unstack()
    print("\nDistribution of 'Q-sell-YOUR-data' by Cluster:")
    print(cluster_privacy)

# ---------------------------
# 10. Visualize Cluster Profiles
# ---------------------------
# Example 1: Boxplot of total spend by cluster.
plt.figure(figsize=(10, 6))
sns.boxplot(x='cluster', y='total_spend', data=merged_data)
plt.title('Total Spend by Cluster')
plt.xlabel('Cluster')
plt.ylabel('Total Spend')
plt.show()

# Example 2: Scatter plot of purchase frequency vs. average price, colored by cluster.
plt.figure(figsize=(10, 6))
sns.scatterplot(x='purchase_frequency', y='avg_price', hue='cluster', data=merged_data, palette='viridis')
plt.title('Purchase Frequency vs. Average Price by Cluster')
plt.xlabel('Purchase Frequency')
plt.ylabel('Average Price')
plt.legend(title='Cluster')
plt.show()

# Import required libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns

# ---------------------------
# 1. Load the Data
# ---------------------------
# Update these filenames/paths as needed.
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv')  # Purchase records CSV file
survey_data   = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')

# Print initial column names
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 2. Rename Key Identifier Columns
# ---------------------------
# Our key column should be "Survey_ResponseID" (with underscore) in both DataFrames.
if 'Survey_ResponseID' not in purchase_data.columns:
    if 'Survey ResponseID' in purchase_data.columns:
        purchase_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)
    else:
        raise KeyError("Purchase data is missing a 'Survey ResponseID' or 'Survey_ResponseID' column.")

if 'Survey_ResponseID' not in survey_data.columns:
    if 'Survey ResponseID' in survey_data.columns:
        survey_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)
    else:
        raise KeyError("Survey data is missing a 'Survey ResponseID' or 'Survey_ResponseID' column.")

print("\nAfter renaming:")
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 3. Aggregate Purchase Data by Consumer
# ---------------------------
# Compute a total spend column (assuming total spend = Quantity * Purchase Price Per Unit).
purchase_data['total_spend'] = purchase_data['Quantity'] * purchase_data['Purchase Price Per Unit']

# Group by Survey_ResponseID and aggregate features:
#   - total_spend: sum over all orders
#   - purchase_frequency: count of orders (using 'Order Date')
#   - avg_price: mean purchase price per unit
#   - category_diversity: number of unique categories purchased
purchase_agg = purchase_data.groupby('Survey_ResponseID').agg(
    total_spend=('total_spend', 'sum'),
    purchase_frequency=('Order Date', 'count'),
    avg_price=('Purchase Price Per Unit', 'mean'),
    category_diversity=('Category', 'nunique')
).reset_index()

print("\nAggregated purchase data (first 5 rows):")
print(purchase_agg.head())

# ---------------------------
# 4. Merge Purchase Aggregates with Survey Data
# ---------------------------
merged_data = pd.merge(purchase_agg, survey_data, on='Survey_ResponseID', how='inner')
print("\nMerged data (first 5 rows):")
print(merged_data.head())

# ---------------------------
# 5. Convert Categorical Survey Responses to Numeric
# ---------------------------
# The feature "Q-sell-YOUR-data" is a text response like "Yes if I get part of the profit" or "No".
# We convert it to numeric (e.g., 1 if the response includes "yes", 0 if it includes "no").
def map_sell_your_data(x):
    if isinstance(x, str):
        if "yes" in x.lower():
            return 1
        elif "no" in x.lower():
            return 0
    return np.nan

merged_data['Q-sell-YOUR-data'] = merged_data['Q-sell-YOUR-data'].apply(map_sell_your_data)
print("\nUnique values in 'Q-sell-YOUR-data' after conversion:",
      merged_data['Q-sell-YOUR-data'].unique())

# ---------------------------
# 6. Prepare Features for Clustering
# ---------------------------
# For segmentation, we select purchase-behavior features and the privacy attitude response.
features = ['total_spend', 'purchase_frequency', 'avg_price', 'category_diversity', 'Q-sell-YOUR-data']
# Drop rows with missing values in these features
X = merged_data[features].dropna()

print("\nFeatures used for clustering (first 5 rows):")
print(X.head())

# ---------------------------
# 7. Scale the Features
# ---------------------------
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ---------------------------
# 8. Determine the Optimal Number of Clusters
# ---------------------------
silhouette_scores = {}
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_scaled)
    score = silhouette_score(X_scaled, labels)
    silhouette_scores[k] = score
    print(f"For k = {k}, Silhouette Score = {score:.3f}")

best_k = max(silhouette_scores, key=silhouette_scores.get)
print(f"\nOptimal number of clusters (based on silhouette score): k = {best_k}")

# ---------------------------
# 9. Run K-Means Clustering
# ---------------------------
kmeans = KMeans(n_clusters=best_k, random_state=42)
merged_data['cluster'] = kmeans.fit_predict(X_scaled)

# ---------------------------
# 10. Post-Clustering Analysis
# ---------------------------
# Summary statistics for each cluster:
cluster_summary = merged_data.groupby('cluster').agg({
    'total_spend': ['mean', 'median'],
    'purchase_frequency': ['mean', 'median'],
    'avg_price': ['mean', 'median'],
    'category_diversity': ['mean', 'median'],
    'Q-sell-YOUR-data': ['mean', 'median']
}).reset_index()

print("\nCluster Summary Statistics:")
print(cluster_summary)

# Optionally, examine the distribution of the privacy attitude (Q-sell-YOUR-data) by cluster.
cluster_privacy = merged_data.groupby('cluster')['Q-sell-YOUR-data'].value_counts(normalize=True).unstack()
print("\nDistribution of 'Q-sell-YOUR-data' by Cluster (proportions):")
print(cluster_privacy)

# ---------------------------
# 11. Visualize Cluster Profiles
# ---------------------------
# Example 1: Boxplot of total spend by cluster.
plt.figure(figsize=(10, 6))
sns.boxplot(x='cluster', y='total_spend', data=merged_data)
plt.title('Total Spend by Cluster')
plt.xlabel('Cluster')
plt.ylabel('Total Spend')
plt.show()

# Example 2: Scatter plot of purchase frequency vs. average price, colored by cluster.
plt.figure(figsize=(10, 6))
sns.scatterplot(x='purchase_frequency', y='avg_price', hue='cluster', data=merged_data, palette='viridis')
plt.title('Purchase Frequency vs. Average Price by Cluster')
plt.xlabel('Purchase Frequency')
plt.ylabel('Average Price')
plt.legend(title='Cluster')
plt.show()

# =============================================================================
# Data Privacy Attitudes and Their Influence on Purchasing Behavior
#
# Research Question:
#   How do consumers’ attitudes toward data privacy and the monetization of
#   personal data (e.g., willingness to “sell your data” or accept profit‐sharing)
#   relate to their actual purchasing behavior and product category choices?
#   Is there evidence of a causal relationship between privacy concerns and online
#   consumption patterns?
#
# Approach:
#   - Merge purchase records (aggregated by consumer) with survey responses.
#   - Recode key survey responses into numeric form.
#   - Run OLS regressions of (for example) purchase frequency on the privacy attitude,
#     controlling for demographic and purchase behavior variables.
#   - (Optionally) Run an instrumental variable regression (e.g. two‐stage least
#     squares) if a plausible instrument (here, Q-sell-consumer-data) is available.
#   - Perform robustness checks and visualize results.
#
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# For OLS regression:
import statsmodels.api as sm
import statsmodels.formula.api as smf

# For IV regression (if desired)
from linearmodels.iv import IV2SLS

# ---------------------------
# 1. Load the Data
# ---------------------------
# (Update the file paths as necessary.)
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv')  # Purchase records CSV file
survey_data   = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')

# Print the original column names for reference:
print("Original Purchase data columns:", purchase_data.columns.tolist())
print("Original Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 2. Rename the Key Identifier Column
# ---------------------------
# Our analyses require a common identifier, "Survey_ResponseID".
# (Rename "Survey ResponseID" --> "Survey_ResponseID" if needed.)
if 'Survey_ResponseID' not in purchase_data.columns:
    if 'Survey ResponseID' in purchase_data.columns:
        purchase_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)
    else:
        raise KeyError("Purchase data does not have a 'Survey ResponseID' column.")

if 'Survey_ResponseID' not in survey_data.columns:
    if 'Survey ResponseID' in survey_data.columns:
        survey_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)
    else:
        raise KeyError("Survey data does not have a 'Survey ResponseID' column.")

print("\nAfter renaming:")
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 3. Aggregate Purchase Data by Consumer
# ---------------------------
# Compute a total spend variable from Quantity and Purchase Price Per Unit.
purchase_data['total_spend'] = purchase_data['Quantity'] * purchase_data['Purchase Price Per Unit']

# Aggregate key purchase behavior features by consumer:
purchase_agg = purchase_data.groupby('Survey_ResponseID').agg(
    total_spend=('total_spend', 'sum'),
    purchase_frequency=('Order Date', 'count'),
    avg_price=('Purchase Price Per Unit', 'mean'),
    category_diversity=('Category', 'nunique')
).reset_index()

print("\nAggregated purchase data (first 5 rows):")
print(purchase_agg.head())

# ---------------------------
# 4. Merge Purchase Aggregates with Survey Data
# ---------------------------
merged_data = pd.merge(purchase_agg, survey_data, on='Survey_ResponseID', how='inner')

print("\nMerged data (first 5 rows):")
print(merged_data.head())
print("\nMerged data columns:", merged_data.columns.tolist())

# ---------------------------
# 5. Recode Key Survey Variables to Numeric
# ---------------------------
# For our analysis, we focus on data privacy attitudes. For example, we recode
# "Q-sell-YOUR-data" (a question about willingness to sell personal data) into a numeric flag.
def map_sell_your_data(x):
    if isinstance(x, str):
        if "yes" in x.lower():
            return 1
        elif "no" in x.lower():
            return 0
    return np.nan

merged_data['Q-sell-YOUR-data'] = merged_data['Q-sell-YOUR-data'].apply(map_sell_your_data)
print("\nUnique values in 'Q-sell-YOUR-data' after conversion:",
      merged_data['Q-sell-YOUR-data'].unique())

# (Optionally, you may recode additional privacy‐attitude questions. Here we also recode
# "Q-sell-consumer-data" to use as an instrument in an IV specification.)
def map_sell_consumer_data(x):
    if isinstance(x, str):
        if "yes" in x.lower():
            return 1
        elif "no" in x.lower():
            return 0
    return np.nan

merged_data['Q-sell-consumer-data'] = merged_data['Q-sell-consumer-data'].apply(map_sell_consumer_data)
print("\nUnique values in 'Q-sell-consumer-data' after conversion:",
      merged_data['Q-sell-consumer-data'].unique())

# ---------------------------
# 6. OLS Regression Analysis
# ---------------------------
# Research question: How does the privacy attitude (Q-sell-YOUR-data) relate to purchasing behavior?
# We now model, for example, purchase_frequency as a function of Q-sell-YOUR-data and other controls.
# We include some purchase features and demographic controls.
#
# Select variables of interest. (You may add additional controls.)
ols_vars = [
    'purchase_frequency',
    'total_spend',
    'avg_price',
    'category_diversity',
    'Q-sell-YOUR-data',
    'Q-demos-age',
    'Q-demos-income',
    'Q-demos-gender',
    'Q-demos-education',
    'Q-demos-race'
]

# Drop rows with missing values in these variables.
ols_data = merged_data[ols_vars].dropna()

# Convert categorical demographic variables into dummy/indicator variables.
ols_data = pd.get_dummies(ols_data, columns=['Q-demos-age', 'Q-demos-income', 'Q-demos-gender',
                                               'Q-demos-education', 'Q-demos-race'], drop_first=True)

# Define dependent and independent variables.
y_ols = ols_data['purchase_frequency']
X_ols = ols_data.drop(columns=['purchase_frequency'])
X_ols = sm.add_constant(X_ols)

# Fit the OLS regression.
ols_model = sm.OLS(y_ols, X_ols).fit()
print("\nOLS Regression Results:")
print(ols_model.summary())

# ---------------------------
# 7. Instrumental Variables (IV) Regression (Optional)
# ---------------------------
# To help address causality, suppose we treat Q-sell-YOUR-data as endogenous and use
# Q-sell-consumer-data as an instrument.
#
# For this analysis, we again use purchase_frequency as the outcome.
iv_vars = [
    'purchase_frequency',
    'Q-sell-YOUR-data',
    'Q-sell-consumer-data',
    'total_spend',
    'avg_price',
    'category_diversity',
    'Q-demos-age',
    'Q-demos-income',
    'Q-demos-gender',
    'Q-demos-education',
    'Q-demos-race'
]

iv_data = merged_data[iv_vars].dropna()
iv_data = pd.get_dummies(iv_data, columns=['Q-demos-age', 'Q-demos-income', 'Q-demos-gender',
                                             'Q-demos-education', 'Q-demos-race'], drop_first=True)

# Define the parts for IV regression:
#   - y_iv: outcome (purchase_frequency)
#   - endog: endogenous regressor (Q-sell-YOUR-data)
#   - instruments: instrument (Q-sell-consumer-data)
#   - exog: additional control variables (total_spend, avg_price, category_diversity, and dummies)
y_iv = iv_data['purchase_frequency']
endog = iv_data['Q-sell-YOUR-data']

# Controls (exogenous regressors):
exog_vars = ['total_spend', 'avg_price', 'category_diversity']
exog = iv_data[exog_vars]
exog = sm.add_constant(exog)

# Instrument:
instr = iv_data['Q-sell-consumer-data']

# Fit the IV regression using two-stage least squares.
iv_model = IV2SLS(dependent=y_iv, exog=exog, endog=endog, instruments=instr).fit(cov_type='robust')
print("\nIV Regression Results:")
print(iv_model.summary)

# ---------------------------
# 8. Robustness Checks & Visualization
# ---------------------------
# (a) Compare alternative specifications (for example, use total_spend as the outcome).
# (b) Visualize relationships.
#
# For example, here is a scatter plot of purchase_frequency vs. Q-sell-YOUR-data:
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Q-sell-YOUR-data', y='purchase_frequency', data=ols_data, alpha=0.7)
plt.title("Purchase Frequency vs. Data Privacy Attitude (Q-sell-YOUR-data)")
plt.xlabel("Willingness to Sell Your Data (1 = Yes, 0 = No)")
plt.ylabel("Purchase Frequency")
plt.show()

# Also, you could use boxplots to show purchase frequency by privacy attitude:
plt.figure(figsize=(8, 6))
sns.boxplot(x='Q-sell-YOUR-data', y='purchase_frequency', data=ols_data)
plt.title("Purchase Frequency by Data Privacy Attitude")
plt.xlabel("Willingness to Sell Your Data (1 = Yes, 0 = No)")
plt.ylabel("Purchase Frequency")
plt.show()

# =============================================================================
# Data Privacy Attitudes and Their Influence on Purchasing Behavior
#
# Research Question:
#   How do consumers’ attitudes toward data privacy and the monetization of
#   personal data (e.g., willingness to “sell your data” or accept profit‐sharing)
#   relate to their actual purchasing behavior and product category choices?
#   Is there evidence of a causal relationship between privacy concerns and online
#   consumption patterns?
#
# Approach:
#   - Load and merge purchase and survey data.
#   - Aggregate purchase data by consumer to compute total spend, purchase
#     frequency, average price, and category diversity.
#   - Recode key survey variables (e.g., Q-sell-YOUR-data and Q-sell-consumer-data)
#     into numeric values.
#   - Ensure that all variables to be used in regression are numeric.
#   - Run an OLS regression of purchase frequency on the privacy attitude and
#     controls (including demographic variables converted to dummies).
#   - (Optionally) Run an IV regression using Q-sell-consumer-data as an instrument.
#
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# For OLS regression:
import statsmodels.api as sm
import statsmodels.formula.api as smf

# For IV regression (if desired)
from linearmodels.iv import IV2SLS

# ---------------------------
# 1. Load the Data
# ---------------------------
# Update the file paths if needed.
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv')  # Purchase records CSV file
survey_data   = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')


print("Original Purchase data columns:", purchase_data.columns.tolist())
print("Original Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 2. Rename the Key Identifier Column
# ---------------------------
# We need a common column name for merging. Rename "Survey ResponseID" to "Survey_ResponseID".
if 'Survey_ResponseID' not in purchase_data.columns:
    if 'Survey ResponseID' in purchase_data.columns:
        purchase_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)
    else:
        raise KeyError("Purchase data does not have a 'Survey ResponseID' column.")

if 'Survey_ResponseID' not in survey_data.columns:
    if 'Survey ResponseID' in survey_data.columns:
        survey_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)
    else:
        raise KeyError("Survey data does not have a 'Survey ResponseID' column.")

print("\nAfter renaming:")
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 3. Aggregate Purchase Data by Consumer
# ---------------------------
# Compute total spend as Quantity times Purchase Price Per Unit.
purchase_data['total_spend'] = purchase_data['Quantity'] * purchase_data['Purchase Price Per Unit']

# Aggregate purchase-level behavior by consumer
purchase_agg = purchase_data.groupby('Survey_ResponseID').agg(
    total_spend=('total_spend', 'sum'),
    purchase_frequency=('Order Date', 'count'),
    avg_price=('Purchase Price Per Unit', 'mean'),
    category_diversity=('Category', 'nunique')
).reset_index()

print("\nAggregated purchase data (first 5 rows):")
print(purchase_agg.head())

# ---------------------------
# 4. Merge Purchase Aggregates with Survey Data
# ---------------------------
merged_data = pd.merge(purchase_agg, survey_data, on='Survey_ResponseID', how='inner')

print("\nMerged data (first 5 rows):")
print(merged_data.head())
print("\nMerged data columns:", merged_data.columns.tolist())

# ---------------------------
# 5. Recode Key Survey Variables to Numeric
# ---------------------------
# Define mapping functions to recode text responses into 0/1 for privacy attitude questions.
def map_sell_your_data(x):
    if isinstance(x, str):
        if "yes" in x.lower():
            return 1
        elif "no" in x.lower():
            return 0
    return np.nan

def map_sell_consumer_data(x):
    if isinstance(x, str):
        if "yes" in x.lower():
            return 1
        elif "no" in x.lower():
            return 0
    return np.nan

merged_data['Q-sell-YOUR-data'] = merged_data['Q-sell-YOUR-data'].apply(map_sell_your_data)
merged_data['Q-sell-consumer-data'] = merged_data['Q-sell-consumer-data'].apply(map_sell_consumer_data)

print("\nUnique values in 'Q-sell-YOUR-data' after conversion:",
      merged_data['Q-sell-YOUR-data'].unique())
print("Unique values in 'Q-sell-consumer-data' after conversion:",
      merged_data['Q-sell-consumer-data'].unique())

# ---------------------------
# 6. OLS Regression Analysis
# ---------------------------
# Our outcome is purchase_frequency, and we want to assess the association with privacy attitude.
# We also include controls for total_spend, avg_price, category_diversity, and key demographic variables.
# Define the list of variables for the OLS model.
ols_vars = [
    'purchase_frequency',
    'total_spend',
    'avg_price',
    'category_diversity',
    'Q-sell-YOUR-data',
    'Q-demos-age',
    'Q-demos-income',
    'Q-demos-gender',
    'Q-demos-education',
    'Q-demos-race'
]

# Select and drop any rows with missing values for these variables.
ols_data = merged_data[ols_vars].dropna()

# To avoid conversion errors, explicitly convert the aggregated numeric columns
for col in ['total_spend', 'avg_price', 'category_diversity', 'purchase_frequency', 'Q-sell-YOUR-data']:
    ols_data[col] = pd.to_numeric(ols_data[col], errors='coerce')

# Convert categorical demographic variables into dummy variables.
ols_data = pd.get_dummies(ols_data,
                          columns=['Q-demos-age', 'Q-demos-income', 'Q-demos-gender',
                                   'Q-demos-education', 'Q-demos-race'],
                          drop_first=True)

# Verify that all columns are now numeric:
print("\nData types for OLS regression variables:")
print(ols_data.dtypes)

# Define dependent and independent variables.
y_ols = ols_data['purchase_frequency']
X_ols = ols_data.drop(columns=['purchase_frequency'])
X_ols = sm.add_constant(X_ols)

# Fit the OLS regression.
ols_model = sm.OLS(y_ols, X_ols).fit()
print("\nOLS Regression Results:")
print(ols_model.summary())

# ---------------------------
# 7. Instrumental Variables (IV) Regression (Optional)
# ---------------------------
# In this optional section, we address potential endogeneity of the privacy attitude.
# We use Q-sell-consumer-data as an instrument for Q-sell-YOUR-data.
iv_vars = [
    'purchase_frequency',
    'Q-sell-YOUR-data',
    'Q-sell-consumer-data',
    'total_spend',
    'avg_price',
    'category_diversity',
    'Q-demos-age',
    'Q-demos-income',
    'Q-demos-gender',
    'Q-demos-education',
    'Q-demos-race'
]

iv_data = merged_data[iv_vars].dropna()

# Convert the aggregated purchase columns and privacy variables to numeric
for col in ['total_spend', 'avg_price', 'category_diversity', 'purchase_frequency',
            'Q-sell-YOUR-data', 'Q-sell-consumer-data']:
    iv_data[col] = pd.to_numeric(iv_data[col], errors='coerce')

# Create dummy variables for the demographic variables.
iv_data = pd.get_dummies(iv_data,
                         columns=['Q-demos-age', 'Q-demos-income', 'Q-demos-gender',
                                  'Q-demos-education', 'Q-demos-race'],
                         drop_first=True)

# Define parts for the IV regression:
y_iv = iv_data['purchase_frequency']
endog = iv_data['Q-sell-YOUR-data']

# Controls (exogenous regressors)
exog_vars = ['total_spend', 'avg_price', 'category_diversity']
exog = iv_data[exog_vars]
exog = sm.add_constant(exog)

# Instrument variable:
instr = iv_data['Q-sell-consumer-data']

# Fit the IV regression (Two-Stage Least Squares) with robust standard errors.
iv_model = IV2SLS(dependent=y_iv, exog=exog, endog=endog, instruments=instr).fit(cov_type='robust')
print("\nIV Regression Results:")
print(iv_model.summary)

# ---------------------------
# 8. Robustness Checks & Visualization
# ---------------------------
# (a) You may explore alternative model specifications (for example, using total_spend as the outcome).
# (b) Visualize the relationship between the privacy attitude and purchase frequency.

# Scatter plot of purchase_frequency versus Q-sell-YOUR-data.
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Q-sell-YOUR-data', y='purchase_frequency', data=ols_data, alpha=0.7)
plt.title("Purchase Frequency vs. Data Privacy Attitude")
plt.xlabel("Willingness to Sell Your Data (1 = Yes, 0 = No)")
plt.ylabel("Purchase Frequency")
plt.show()

# Boxplot of purchase_frequency by privacy attitude.
plt.figure(figsize=(8, 6))
sns.boxplot(x='Q-sell-YOUR-data', y='purchase_frequency', data=ols_data)
plt.title("Purchase Frequency by Data Privacy Attitude")
plt.xlabel("Willingness to Sell Your Data (1 = Yes, 0 = No)")
plt.ylabel("Purchase Frequency")
plt.show()

# =============================================================================
# Data Privacy Attitudes and Their Influence on Purchasing Behavior
#
# Research Question:
#   How do consumers’ attitudes toward data privacy and the monetization of
#   personal data (e.g., willingness to “sell your data” or accept profit‐sharing)
#   relate to their actual purchasing behavior and product category choices?
#   Is there evidence of a causal relationship between privacy concerns and online
#   consumption patterns?
#
# Approach:
#   - Load and merge purchase and survey data.
#   - Aggregate purchase data by consumer to compute total spend, purchase
#     frequency, average price, and category diversity.
#   - Recode key survey variables (e.g., Q-sell-YOUR-data) into numeric values.
#   - Convert categorical demographics into dummy variables.
#   - Ensure all variables used in the regression are numeric.
#   - Run an OLS regression to test the association between privacy attitude
#     and purchase frequency (with controls).
#
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# For OLS regression:
import statsmodels.api as sm

# ---------------------------
# 1. Load the Data
# ---------------------------
# Update the file paths as needed.
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv')  # Purchase records CSV file
survey_data   = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')


print("Original Purchase data columns:", purchase_data.columns.tolist())
print("Original Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 2. Rename the Key Identifier Column
# ---------------------------
# Rename "Survey ResponseID" to "Survey_ResponseID" in both datasets.
if 'Survey_ResponseID' not in purchase_data.columns:
    if 'Survey ResponseID' in purchase_data.columns:
        purchase_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)
    else:
        raise KeyError("Purchase data does not have a 'Survey ResponseID' column.")

if 'Survey_ResponseID' not in survey_data.columns:
    if 'Survey ResponseID' in survey_data.columns:
        survey_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)
    else:
        raise KeyError("Survey data does not have a 'Survey ResponseID' column.")

print("\nAfter renaming:")
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 3. Aggregate Purchase Data by Consumer
# ---------------------------
# Create a new column for total spend.
purchase_data['total_spend'] = purchase_data['Quantity'] * purchase_data['Purchase Price Per Unit']

# Aggregate the purchase data by Survey_ResponseID.
purchase_agg = purchase_data.groupby('Survey_ResponseID').agg(
    total_spend=('total_spend', 'sum'),
    purchase_frequency=('Order Date', 'count'),
    avg_price=('Purchase Price Per Unit', 'mean'),
    category_diversity=('Category', 'nunique')
).reset_index()

print("\nAggregated purchase data (first 5 rows):")
print(purchase_agg.head())

# ---------------------------
# 4. Merge Purchase Aggregates with Survey Data
# ---------------------------
merged_data = pd.merge(purchase_agg, survey_data, on='Survey_ResponseID', how='inner')

print("\nMerged data (first 5 rows):")
print(merged_data.head())
print("\nMerged data columns:", merged_data.columns.tolist())

# ---------------------------
# 5. Recode Key Survey Variables to Numeric
# ---------------------------
# Define mapping functions to recode privacy attitude responses.
def map_sell_your_data(x):
    if isinstance(x, str):
        if "yes" in x.lower():
            return 1
        elif "no" in x.lower():
            return 0
    return np.nan

def map_sell_consumer_data(x):
    if isinstance(x, str):
        if "yes" in x.lower():
            return 1
        elif "no" in x.lower():
            return 0
    return np.nan

merged_data['Q-sell-YOUR-data'] = merged_data['Q-sell-YOUR-data'].apply(map_sell_your_data)
merged_data['Q-sell-consumer-data'] = merged_data['Q-sell-consumer-data'].apply(map_sell_consumer_data)

print("\nUnique values in 'Q-sell-YOUR-data' after conversion:",
      merged_data['Q-sell-YOUR-data'].unique())
print("Unique values in 'Q-sell-consumer-data' after conversion:",
      merged_data['Q-sell-consumer-data'].unique())

# ---------------------------
# 6. Prepare Data for OLS Regression
# ---------------------------
# We want to model purchase_frequency as a function of the privacy attitude plus controls.
# Define the variables to include.
ols_vars = [
    'purchase_frequency',
    'total_spend',
    'avg_price',
    'category_diversity',
    'Q-sell-YOUR-data',
    'Q-demos-age',
    'Q-demos-income',
    'Q-demos-gender',
    'Q-demos-education',
    'Q-demos-race'
]

# Select relevant columns and drop missing rows.
ols_data = merged_data[ols_vars].dropna()

# Convert the aggregated numeric columns explicitly.
for col in ['purchase_frequency', 'total_spend', 'avg_price', 'category_diversity', 'Q-sell-YOUR-data']:
    ols_data[col] = pd.to_numeric(ols_data[col], errors='coerce')

# Convert categorical demographic variables into dummy variables.
ols_data = pd.get_dummies(ols_data,
                          columns=['Q-demos-age', 'Q-demos-income', 'Q-demos-gender',
                                   'Q-demos-education', 'Q-demos-race'],
                          drop_first=True)

# Before fitting the model, ensure that every column in the predictors is numeric.
# Sometimes, dummy variables created as booleans cause issues so convert them to integers.
X_ols = ols_data.drop(columns=['purchase_frequency'])
for col in X_ols.columns:
    # If column is boolean, convert to int (0, 1)
    if X_ols[col].dtype == 'bool':
        X_ols[col] = X_ols[col].astype(int)
    # Otherwise, if the column is object, try to convert it
    elif X_ols[col].dtype == 'object':
        X_ols[col] = pd.to_numeric(X_ols[col], errors='raise')

# Also convert the dependent variable to numeric.
y_ols = pd.to_numeric(ols_data['purchase_frequency'], errors='raise')

# Add a constant term to the predictors.
X_ols = sm.add_constant(X_ols)

print("\nData types for OLS regression predictors:")
print(X_ols.dtypes)

# ---------------------------
# 7. Fit the OLS Regression
# ---------------------------
ols_model = sm.OLS(y_ols, X_ols).fit()
print("\nOLS Regression Results:")
print(ols_model.summary())

# ---------------------------
# 8. Visualization (Optional)
# ---------------------------
# Example: Scatter plot and boxplot of purchase frequency vs. privacy attitude.
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Q-sell-YOUR-data', y='purchase_frequency', data=ols_data, alpha=0.7)
plt.title("Purchase Frequency vs. Data Privacy Attitude")
plt.xlabel("Willingness to Sell Your Data (1 = Yes, 0 = No)")
plt.ylabel("Purchase Frequency")
plt.show()

plt.figure(figsize=(8, 6))
sns.boxplot(x='Q-sell-YOUR-data', y='purchase_frequency', data=ols_data)
plt.title("Purchase Frequency by Data Privacy Attitude")
plt.xlabel("Willingness to Sell Your Data (1 = Yes, 0 = No)")
plt.ylabel("Purchase Frequency")
plt.show()

# =============================================================================
# Price Elasticity and Quantity Purchased Across Product Categories
#
# Research Question:
#   What is the causal effect of unit price variations on the quantity purchased
#   across different product categories, and does price elasticity differ by
#   consumer demographics or segments identified via machine learning?
#
# Rationale & Methods:
#   - We estimate log–log regressions of quantity on unit price. The coefficient
#     on log(price) is the price elasticity.
#   - We include interactions with product category (and optionally demographics)
#     to test if elasticity differs across groups.
#   - As a robustness check, we also use panel fixed‐effects (consumer FE) methods
#     to control for time–invariant heterogeneity.
#
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import statsmodels.api as sm
import statsmodels.formula.api as smf

# For panel data estimation (if needed)
from linearmodels.panel import PanelOLS

# ---------------------------
# 1. Load Data
# ---------------------------
# Replace with your file paths
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv')  # Purchase records CSV file
survey_data   = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')


print("Original Purchase data columns:", purchase_data.columns.tolist())
print("Original Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 2. Rename Identifier Column
# ---------------------------
# Rename "Survey ResponseID" to "Survey_ResponseID" in both datasets.
if 'Survey ResponseID' in purchase_data.columns:
    purchase_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)
if 'Survey ResponseID' in survey_data.columns:
    survey_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)

print("\nAfter renaming:")
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 3. Merge Data
# ---------------------------
# Merge purchase data with survey data on the consumer identifier.
merged_data = pd.merge(purchase_data, survey_data, on='Survey_ResponseID', how='inner')
print("\nMerged data (first 5 rows):")
print(merged_data.head())

# ---------------------------
# 4. Data Preparation for Price Elasticity Analysis
# ---------------------------
# (a) Remove any observations with nonpositive prices or quantities
merged_data = merged_data[(merged_data['Purchase Price Per Unit'] > 0) &
                          (merged_data['Quantity'] > 0)]

# (b) Create log variables
merged_data['log_price'] = np.log(merged_data['Purchase Price Per Unit'])
merged_data['log_quantity'] = np.log(merged_data['Quantity'])

# (c) Ensure the product category is treated as categorical.
merged_data['Category'] = merged_data['Category'].astype('category')

# (d) (Optional) Create demographic dummies.
# For example, suppose you want to include age and gender.
# Adjust these variable names based on your actual survey responses.
demographic_vars = ['Q-demos-age', 'Q-demos-gender']
merged_data = pd.get_dummies(merged_data, columns=demographic_vars, drop_first=True)

# ---------------------------
# 5. OLS Regression: Log–Log Model with Interaction Terms
# ---------------------------
# Here we estimate a model of the form:
#
#   log_quantity = β₀ + β₁ log_price + β₂ log_price × C(Category) + (demographic controls) + ε
#
# We use the formula interface from statsmodels.
# First, build the regression formula.
formula = 'log_quantity ~ log_price * C(Category)'

# Optionally add demographic dummies.
# Let’s add any dummy variable whose name starts with "Q-demos-age_" or "Q-demos-gender_"
demo_dummies = [col for col in merged_data.columns
                if col.startswith('Q-demos-age_') or col.startswith('Q-demos-gender_')]
if demo_dummies:
    formula += ' + ' + ' + '.join(demo_dummies)

print("\nRunning OLS regression with formula:")
print(formula)

# Fit the model with robust (heteroskedasticity–consistent) standard errors.
ols_model = smf.ols(formula, data=merged_data).fit(cov_type='HC3')
print("\nOLS Regression Results:")
print(ols_model.summary())

# ---------------------------
# 6. Visualization
# ---------------------------
# For example, plot the relationship between log_price and log_quantity colored by Category.
plt.figure(figsize=(8, 6))
sns.scatterplot(x='log_price', y='log_quantity', hue='Category', data=merged_data, alpha=0.6)
plt.title("Log(Quantity) vs. Log(Price) by Product Category")
plt.xlabel("Log(Unit Price)")
plt.ylabel("Log(Quantity Purchased)")
plt.legend(title='Category')
plt.show()

# ---------------------------
# 7. Robustness: Panel Data (Consumer Fixed Effects)
# ---------------------------
# If your purchase data include repeated orders over time for each consumer,
# you can use panel fixed‐effects methods to control for time–invariant consumer characteristics.
# Here we use the Order Date as a time variable.
merged_data['Order Date'] = pd.to_datetime(merged_data['Order Date'])
panel_data = merged_data.set_index(['Survey_ResponseID', 'Order Date'])

# Prepare the exogenous variables for the panel regression.
# We include log_price and dummies for Category (excluding one baseline category).
exog = panel_data[['log_price']].copy()
# Create category dummies (drop the first to avoid collinearity) and join to exog.
category_dummies = pd.get_dummies(panel_data['Category'], drop_first=True)
exog = exog.join(category_dummies)
# Also join demographic dummies if available.
demo_controls = panel_data[[col for col in panel_data.columns
                            if col.startswith('Q-demos-age_') or col.startswith('Q-demos-gender_')]]
exog = exog.join(demo_controls)
exog = sm.add_constant(exog)

# The dependent variable is log_quantity.
# Fit the PanelOLS with entity (consumer) fixed effects.
panel_model = PanelOLS(panel_data['log_quantity'], exog, entity_effects=True)
fe_results = panel_model.fit(cov_type='robust')
print("\nFixed Effects Panel OLS Results:")
print(fe_results.summary)

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Price Elasticity and Quantity Purchased Across Product Categories

Research Question:
    What is the causal effect of unit price variations on the quantity purchased
    across different product categories, and does price elasticity differ by
    consumer demographics or segments identified via machine learning?

This script:
  - Loads purchase and survey data.
  - Merges them (after renaming the consumer identifier column).
  - Aggregates purchase data to the consumer level.
  - Merges aggregated purchase data with survey responses.
  - Prepares the data (creates log variables, converts categories, and
    creates dummy variables for demographics).
  - Runs an OLS regression (log–log model) with an interaction between log_price
    and product Category plus several demographic dummy controls.
  - (Optional) Runs a panel fixed‐effects regression for robustness.

Note: In the regression formula below, variable names with spaces or dashes are
wrapped in backticks so that Patsy treats them as single names.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import statsmodels.api as sm
import statsmodels.formula.api as smf

# For panel data estimation (optional)
from linearmodels.panel import PanelOLS

# ---------------------------
# 1. Load Data
# ---------------------------
# Replace these file paths with your own:
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv')  # Purchase records CSV file
survey_data   = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')

# Display original columns:
print("Original Purchase data columns:", purchase_data.columns.tolist())
print("Original Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 2. Rename the identifier column consistently
# ---------------------------
# Rename "Survey ResponseID" to "Survey_ResponseID"
if 'Survey ResponseID' in purchase_data.columns:
    purchase_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)
if 'Survey ResponseID' in survey_data.columns:
    survey_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)

print("\nAfter renaming:")
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 3. Aggregate Purchase Data
# ---------------------------
# For example, aggregate by Survey_ResponseID to compute total spend, purchase frequency,
# average unit price, and category diversity.
purchase_agg = purchase_data.groupby('Survey_ResponseID').agg(
    total_spend=('Purchase Price Per Unit', lambda x: np.sum(x * purchase_data.loc[x.index, 'Quantity'])),
    purchase_frequency=('Order Date', 'count'),
    avg_price=('Purchase Price Per Unit', 'mean'),
    category_diversity=('Category', lambda x: x.nunique())
).reset_index()

print("\nAggregated purchase data (first 5 rows):")
print(purchase_agg.head())

# ---------------------------
# 4. Merge Aggregated Purchase Data with Survey Data
# ---------------------------
merged_data = pd.merge(purchase_agg, survey_data, on='Survey_ResponseID', how='inner')
print("\nMerged data (first 5 rows):")
print(merged_data.head())
print("\nMerged data columns:", merged_data.columns.tolist())

# ---------------------------
# 5. Prepare Variables for Elasticity Analysis
# ---------------------------
# Remove rows with nonpositive values in price or frequency (if any)
merged_data = merged_data[(merged_data['avg_price'] > 0) & (merged_data['purchase_frequency'] > 0)]

# Create log–transformed variables:
merged_data['log_price'] = np.log(merged_data['avg_price'])
merged_data['log_quantity'] = np.log(merged_data['purchase_frequency'])

# Ensure Category is treated as categorical:
merged_data['Category'] = merged_data['Category'].astype('category')

# ---------------------------
# 6. Create Dummy Variables for Demographics
# ---------------------------
# Suppose we want to control for age and gender.
# If your survey columns for age and gender are categorical strings,
# you can create dummies using pd.get_dummies.
# For this example, assume the following columns exist in merged_data:
#   'Q-demos-age' and 'Q-demos-gender'
# (If these are multi-category variables, get_dummies will create multiple columns.)
if 'Q-demos-age' in merged_data.columns:
    age_dummies = pd.get_dummies(merged_data['Q-demos-age'], prefix='Q-demos-age', drop_first=True)
    merged_data = pd.concat([merged_data, age_dummies], axis=1)
if 'Q-demos-gender' in merged_data.columns:
    gender_dummies = pd.get_dummies(merged_data['Q-demos-gender'], prefix='Q-demos-gender', drop_first=True)
    merged_data = pd.concat([merged_data, gender_dummies], axis=1)

# For example, suppose the resulting dummy columns are:
# ['Q-demos-age_25 - 34 years', 'Q-demos-age_35 - 44 years',
#  'Q-demos-age_45 - 54 years', 'Q-demos-age_55 - 64 years',
#  'Q-demos-age_65 and older', 'Q-demos-gender_Male', 'Q-demos-gender_Other',
#  'Q-demos-gender_Prefer not to say']
# (Check merged_data.columns.tolist() to see the actual dummy names.)

print("\nData types for OLS regression variables:")
# List only the variables we plan to include
vars_for_reg = ['purchase_frequency', 'total_spend', 'avg_price', 'category_diversity', 'log_price', 'log_quantity']
dummy_vars = [col for col in merged_data.columns if col.startswith('Q-demos-age_') or col.startswith('Q-demos-gender_')]
print(merged_data[vars_for_reg + dummy_vars].dtypes)

# ---------------------------
# 7. Run the OLS Regression
# ---------------------------
# Build the regression formula.
# Note: Because some of the dummy variable names contain spaces or dashes,
# they must be enclosed in backticks (`...`) so that Patsy treats them as a single variable name.
formula = (
    "log_quantity ~ log_price * C(Category) + "
    "`Q-demos-age_25 - 34 years` + `Q-demos-age_35 - 44 years` + "
    "`Q-demos-age_45 - 54 years` + `Q-demos-age_55 - 64 years` + "
    "`Q-demos-age_65 and older` + "
    "`Q-demos-gender_Male` + `Q-demos-gender_Other` + `Q-demos-gender_Prefer not to say`"
)
print("\nRunning OLS regression with formula:")
print(formula)

# Fit the model using the formula interface with robust standard errors.
try:
    ols_model = smf.ols(formula, data=merged_data).fit(cov_type='HC3')
    print("\nOLS Regression Results:")
    print(ols_model.summary())
except Exception as e:
    print("\nAn error occurred in OLS regression:")
    print(e)

# ---------------------------
# 8. Visualization of the Relationship
# ---------------------------
plt.figure(figsize=(8, 6))
sns.scatterplot(x='log_price', y='log_quantity', hue='Category', data=merged_data, alpha=0.6)
plt.title("Log(Quantity Purchased) vs. Log(Unit Price) by Category")
plt.xlabel("Log(Unit Price)")
plt.ylabel("Log(Purchase Frequency)")
plt.legend(title='Category')
plt.show()

# ---------------------------
# 9. (Optional) Panel Data Regression (Consumer Fixed Effects)
# ---------------------------
# If your purchase data include multiple orders per consumer over time, you can control
# for consumer fixed effects.
if 'Order Date' in purchase_data.columns:
    # Merge the original purchase data (with Order Date) with survey data.
    merged_panel = pd.merge(purchase_data, survey_data, on='Survey_ResponseID', how='inner')
    # Keep only observations with positive prices and quantities.
    merged_panel = merged_panel[(merged_panel['Purchase Price Per Unit'] > 0) &
                                (merged_panel['Quantity'] > 0)]
    # Create log variables.
    merged_panel['log_price'] = np.log(merged_panel['Purchase Price Per Unit'])
    merged_panel['log_quantity'] = np.log(merged_panel['Quantity'])
    merged_panel['Category'] = merged_panel['Category'].astype('category')
    # Convert Order Date to datetime.
    merged_panel['Order Date'] = pd.to_datetime(merged_panel['Order Date'])
    # Set a multi-index (consumer and time).
    panel_data = merged_panel.set_index(['Survey_ResponseID', 'Order Date'])

    # Prepare exogenous variables.
    exog = panel_data[['log_price']].copy()
    # Add category dummies (drop one as baseline).
    cat_dummies = pd.get_dummies(panel_data['Category'], drop_first=True)
    exog = exog.join(cat_dummies)
    # (Optionally) add demographic dummy controls if you have them.
    # For example:
    demo_cols = [col for col in panel_data.columns if col.startswith('Q-demos-age_') or col.startswith('Q-demos-gender_')]
    if demo_cols:
        exog = exog.join(panel_data[demo_cols])
    exog = sm.add_constant(exog)

    # Fit the consumer fixed-effects model.
    panel_model = PanelOLS(panel_data['log_quantity'], exog, entity_effects=True)
    fe_results = panel_model.fit(cov_type='robust')
    print("\nFixed Effects Panel OLS Results:")
    print(fe_results.summary)

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Price Elasticity and Quantity Purchased Across Product Categories
with Consumer–Level Aggregation and a Primary Category

Research Question:
  What is the causal effect of unit price variations on the quantity purchased
  across different product categories, and does price elasticity differ by
  consumer demographics?

This script:
  1. Loads purchase and survey data from CSV files on Google Drive.
  2. Renames the identifier column to a consistent name.
  3. Aggregates purchase data by consumer (Survey_ResponseID):
     - Total spend (computed as sum of Price * Quantity)
     - Purchase frequency (number of orders)
     - Average unit price
     - Category diversity (number of unique categories)
  4. Determines each consumer’s primary product category (mode of their purchases).
  5. Merges the aggregated purchase data (with primary category) with the survey data.
  6. Creates log–transformed variables.
  7. Creates dummy variables for demographics (age and gender).
  8. Runs an OLS regression with an interaction between log_price and primary_category.
     (Note: Because some dummy variable names include spaces or dashes, we use backticks.)
  9. Visualizes the relationship.

Make sure to adjust file paths and variable names as needed.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import statsmodels.api as sm
import statsmodels.formula.api as smf

# ---------------------------
# 1. Load Data
# ---------------------------
purchase_path = '/content/drive/MyDrive/amazon/amazon-purchases.csv'
survey_path   = '/content/drive/MyDrive/amazon/survey.csv'

purchase_data = pd.read_csv(purchase_path)
survey_data   = pd.read_csv(survey_path)

print("Original Purchase data columns:", purchase_data.columns.tolist())
print("Original Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 2. Rename the Identifier Column
# ---------------------------
# Rename "Survey ResponseID" to "Survey_ResponseID" in both dataframes.
if 'Survey ResponseID' in purchase_data.columns:
    purchase_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)
if 'Survey ResponseID' in survey_data.columns:
    survey_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)

print("\nAfter renaming:")
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 3. Aggregate Purchase Data at the Consumer Level
# ---------------------------
# It is easier to compute total spend by first creating a new column.
purchase_data['spend'] = purchase_data['Purchase Price Per Unit'] * purchase_data['Quantity']

# Compute aggregated measures by consumer.
purchase_agg = purchase_data.groupby('Survey_ResponseID').agg(
    total_spend=('spend', 'sum'),
    purchase_frequency=('Order Date', 'count'),
    avg_price=('Purchase Price Per Unit', 'mean'),
    category_diversity=('Category', lambda x: x.nunique())
).reset_index()

print("\nAggregated purchase data (first 5 rows):")
print(purchase_agg.head())

# ---------------------------
# 4. Compute Consumer’s Primary Category
# ---------------------------
# For consumers who purchased from more than one category, we take the mode.
# (If there is a tie, mode() returns multiple values; here we take the first one.)
primary_category = purchase_data.groupby('Survey_ResponseID')['Category'].agg(
    lambda x: x.mode()[0] if not x.mode().empty else np.nan
).reset_index()
primary_category.rename(columns={'Category': 'primary_category'}, inplace=True)

# ---------------------------
# 5. Merge Aggregated Data with Survey Data
# ---------------------------
merged_data = pd.merge(purchase_agg, survey_data, on='Survey_ResponseID', how='inner')
merged_data = pd.merge(merged_data, primary_category, on='Survey_ResponseID', how='left')

print("\nMerged data (first 5 rows):")
print(merged_data.head())
print("\nMerged data columns:", merged_data.columns.tolist())

# ---------------------------
# 6. Prepare Variables for Elasticity Analysis
# ---------------------------
# Remove any observations with nonpositive average price or purchase frequency.
merged_data = merged_data[(merged_data['avg_price'] > 0) & (merged_data['purchase_frequency'] > 0)]

# Create log–transformed variables:
merged_data['log_price'] = np.log(merged_data['avg_price'])
merged_data['log_quantity'] = np.log(merged_data['purchase_frequency'])

# Ensure the primary_category is treated as a categorical variable.
merged_data['primary_category'] = merged_data['primary_category'].astype('category')

# ---------------------------
# 7. Create Dummy Variables for Demographics
# ---------------------------
# Create dummy variables for age and gender. (You can adjust the variable names as needed.)
if 'Q-demos-age' in merged_data.columns:
    age_dummies = pd.get_dummies(merged_data['Q-demos-age'], prefix='Q-demos-age', drop_first=True)
    merged_data = pd.concat([merged_data, age_dummies], axis=1)
if 'Q-demos-gender' in merged_data.columns:
    gender_dummies = pd.get_dummies(merged_data['Q-demos-gender'], prefix='Q-demos-gender', drop_first=True)
    merged_data = pd.concat([merged_data, gender_dummies], axis=1)

# Inspect dummy variable names (they will be used in the regression formula)
print("\nDummy variables created (age and gender):")
print([col for col in merged_data.columns if col.startswith('Q-demos-age_') or col.startswith('Q-demos-gender_')])

# ---------------------------
# 8. Run the OLS Regression
# ---------------------------
# We want to run a regression like:
# log_quantity ~ log_price * C(primary_category) + [demographic dummies]
# Because some dummy names contain spaces or other nonstandard characters,
# we wrap them in backticks.

# For example, suppose get_dummies created these columns:
#   Q-demos-age_25 - 34 years, Q-demos-age_35 - 44 years, Q-demos-age_45 - 54 years,
#   Q-demos-age_55 - 64 years, Q-demos-age_65 and older,
#   Q-demos-gender_Male, Q-demos-gender_Other, Q-demos-gender_Prefer not to say
# Adjust the variable names below to match what you see in merged_data.columns.
formula = (
    "log_quantity ~ log_price * C(primary_category) + "
    "Q('Q-demos-age_25 - 34 years') + Q('Q-demos-age_35 - 44 years') + "
    "Q('Q-demos-age_45 - 54 years') + Q('Q-demos-age_55 - 64 years') + "
    "Q('Q-demos-age_65 and older') + "
    "Q('Q-demos-gender_Male') + Q('Q-demos-gender_Other') + Q('Q-demos-gender_Prefer not to say')"
)

print("Running OLS regression with formula:")
print(formula)

# --- Fit the model with robust standard errors (HC3) ---
try:
    ols_model = smf.ols(formula, data=merged_data).fit(cov_type='HC3')
    print("\nOLS Regression Results:")
    print(ols_model.summary())
except Exception as e:
    print("\nAn error occurred in OLS regression:")
    print(e)

# ---------------------------
# 9. Visualization
# ---------------------------
plt.figure(figsize=(8, 6))
sns.scatterplot(x='log_price', y='log_quantity', hue='primary_category', data=merged_data, alpha=0.6)
plt.title("Log(Purchase Frequency) vs. Log(Unit Price) by Primary Category")
plt.xlabel("Log(Unit Price)")
plt.ylabel("Log(Purchase Frequency)")
plt.legend(title='Primary Category')
plt.show()

# =============================================================================
# Price Elasticity and Quantity Purchased Across Product Categories
#
# Research Question:
#   What is the causal effect of unit price variations on the quantity purchased
#   across different product categories, and does price elasticity differ by
#   consumer demographics or segments identified via machine learning?
#
# Rationale & Methods:
#   - We estimate log–log regressions of quantity on unit price. The coefficient
#     on log(price) is the price elasticity.
#   - We include interactions with product category (and optionally demographics)
#     to test if elasticity differs across groups.
#   - As a robustness check, we also use panel fixed‐effects (consumer FE) methods
#     to control for time–invariant heterogeneity.
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import statsmodels.api as sm
import statsmodels.formula.api as smf

# For panel data estimation (if needed)
from linearmodels.panel import PanelOLS

# ---------------------------
# 1. Load Data
# ---------------------------
# Replace with your file paths
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv')
survey_data   = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')

print("Original Purchase data columns:", purchase_data.columns.tolist())
print("Original Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 2. Rename Identifier Column
# ---------------------------
# Rename "Survey ResponseID" to "Survey_ResponseID" in both datasets.
if 'Survey ResponseID' in purchase_data.columns:
    purchase_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)
if 'Survey ResponseID' in survey_data.columns:
    survey_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)

print("\nAfter renaming:")
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 3. Merge Data
# ---------------------------
# Merge purchase data with survey data on the consumer identifier.
merged_data = pd.merge(purchase_data, survey_data, on='Survey_ResponseID', how='inner')
print("\nMerged data (first 5 rows):")
print(merged_data.head())

# ---------------------------
# 4. Data Preparation for Price Elasticity Analysis
# ---------------------------
# (a) Remove any observations with nonpositive prices or quantities.
merged_data = merged_data[(merged_data['Purchase Price Per Unit'] > 0) &
                          (merged_data['Quantity'] > 0)].copy()

# (b) Create log–transformed variables.
merged_data['log_price'] = np.log(merged_data['Purchase Price Per Unit'])
merged_data['log_quantity'] = np.log(merged_data['Quantity'])

# (c) Ensure that the product category is treated as categorical.
# In our merged data we assume that a refined product category has been stored
# in a new variable called "primary_category". If not, you can also use the original "Category".
if 'primary_category' not in merged_data.columns:
    merged_data['primary_category'] = merged_data['Category']  # fallback if needed
merged_data['primary_category'] = merged_data['primary_category'].astype('category')

# (d) Create demographic dummy variables.
# Suppose we want to include age and gender dummies. Adjust variable names as needed.
# Here we assume the original survey variable names for age and gender are:
# 'Q-demos-age' and 'Q-demos-gender'
demographic_vars = ['Q-demos-age', 'Q-demos-gender']
merged_data = pd.get_dummies(merged_data, columns=demographic_vars, drop_first=True)

# List the created demographic dummy columns.
demo_dummies = [col for col in merged_data.columns
                if col.startswith('Q-demos-age_') or col.startswith('Q-demos-gender_')]
print("\nDummy variables created (age and gender):")
print(demo_dummies)

# (e) [Optional] Create additional aggregated purchase measures (e.g., total spend,
# purchase frequency per consumer, and category diversity) if not already done.
# (For brevity, we assume that the aggregation was done earlier.)
# ------------------------------------------------------------------

# ---------------------------
# 5. OLS Regression: Log–Log Model with Interaction Terms
# ---------------------------
# Our model specification:
#   log_quantity = β₀ + β₁ log_price + β₂ log_price × C(primary_category)
#                  + (demographic dummies) + ε
#
# Since some variable names include spaces or special characters, we use Patsy’s Q() function.
# We build the regression formula as a string.
formula = 'log_quantity ~ log_price * C(primary_category)'

# We add all demographic dummies that are nonconstant.
non_constant_dummies = []
for col in demo_dummies:
    var_variance = merged_data[col].var()
    print(f"{col} variance: {var_variance}")
    if var_variance > 0:
        non_constant_dummies.append(col)
    else:
        print(f"Warning: {col} is constant and will be dropped from the model.")

if non_constant_dummies:
    # For each dummy variable, use Q("variable name") so that special characters are handled.
    demo_formula = ' + '.join([f'Q("{col}")' for col in non_constant_dummies])
    formula += ' + ' + demo_formula

print("\nRunning OLS regression with formula:")
print(formula)

# Fit the model with robust (heteroskedasticity–consistent) standard errors.
try:
    ols_model = smf.ols(formula, data=merged_data).fit(cov_type='HC3')
    print("\nOLS Regression Results:")
    print(ols_model.summary())
except Exception as e:
    print("\nAn error occurred in OLS regression:")
    print(e)

# ---------------------------
# 6. Visualization
# ---------------------------
# Plot the relationship between log_price and log_quantity, colored by product category.
plt.figure(figsize=(8, 6))
sns.scatterplot(x='log_price', y='log_quantity', hue='primary_category', data=merged_data, alpha=0.6)
plt.title("Log(Quantity) vs. Log(Unit Price) by Product Category")
plt.xlabel("Log(Unit Price)")
plt.ylabel("Log(Quantity Purchased)")
plt.legend(title='Product Category')
plt.show()

# ---------------------------
# 7. Robustness Check: Panel Data with Consumer Fixed Effects
# ---------------------------
# If the purchase data include multiple orders per consumer over time,
# we can run a panel regression to control for time–invariant consumer characteristics.
# (Here we assume that the 'Order Date' column exists and can be converted to datetime.)
merged_data['Order Date'] = pd.to_datetime(merged_data['Order Date'])
panel_data = merged_data.set_index(['Survey_ResponseID', 'Order Date']).sort_index()

# Prepare the exogenous variables for the panel regression.
# We include log_price and dummies for primary_category (dropping one level), plus any demographic dummies.
exog = panel_data[['log_price']].copy()
category_dummies = pd.get_dummies(panel_data['primary_category'], drop_first=True)
exog = exog.join(category_dummies)

# Include demographic controls if available.
demo_controls = panel_data[[col for col in panel_data.columns
                            if col.startswith('Q-demos-age_') or col.startswith('Q-demos-gender_')]]
exog = exog.join(demo_controls)
exog = sm.add_constant(exog)

# The dependent variable is log_quantity.
# Fit the PanelOLS model with entity (consumer) fixed effects.
panel_model = PanelOLS(panel_data['log_quantity'], exog, entity_effects=True)
fe_results = panel_model.fit(cov_type='robust')

print("\nFixed Effects Panel OLS Results:")
print(fe_results.summary)

# =============================================================================
# Price Elasticity and Quantity Purchased Across Product Categories
#
# Research Question:
#   What is the causal effect of unit price variations on the quantity purchased
#   across different product categories, and does price elasticity differ by
#   consumer demographics (or segments identified via machine learning)?
#
# Rationale & Methods:
#   - We estimate log–log regressions of quantity on unit price.
#   - We include an interaction between log_price and product category and
#     control for key demographic characteristics.
#   - To reduce memory usage, we load only needed columns and “clean” column names.
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.formula.api as smf
from linearmodels.panel import PanelOLS  # (if you later wish to do panel FE analysis)

# ---------------------------
# 1. Load Data (memory‐efficient)
# ---------------------------
# Define only the necessary columns.
purchase_cols = ['Order Date', 'Purchase Price Per Unit', 'Quantity', 'Category', 'Survey ResponseID']
survey_cols   = ['Survey ResponseID', 'Q-demos-age', 'Q-demos-gender']

# Read CSV files using only the needed columns.
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv',
                            usecols=purchase_cols, low_memory=True)
survey_data   = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv',
                            usecols=survey_cols, low_memory=True)

print("Original Purchase data columns:", purchase_data.columns.tolist())
print("Original Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 2. Rename Identifier Column and Clean Column Names
# ---------------------------
# Rename "Survey ResponseID" to "Survey_ResponseID" in both datasets.
purchase_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)
survey_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)

# Define a helper function to clean column names (remove spaces and dashes).
def clean_col(col):
    return col.strip().replace(' ', '_').replace('-', '_')

# Apply the cleaning to all column names.
purchase_data.columns = [clean_col(c) for c in purchase_data.columns]
survey_data.columns   = [clean_col(c) for c in survey_data.columns]

print("\nAfter renaming and cleaning:")
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 3. Merge Data
# ---------------------------
# Merge on the common identifier.
merged_data = pd.merge(purchase_data, survey_data, on='Survey_ResponseID', how='inner')
print("\nMerged data (first 5 rows):")
print(merged_data.head())

# ---------------------------
# 4. Data Preparation for Price Elasticity Analysis
# ---------------------------
# (a) Remove observations with nonpositive prices or quantities.
merged_data = merged_data[(merged_data['Purchase_Price_Per_Unit'] > 0) &
                          (merged_data['Quantity'] > 0)].copy()

# (b) Create log–transformed variables.
merged_data['log_price'] = np.log(merged_data['Purchase_Price_Per_Unit'])
merged_data['log_quantity'] = np.log(merged_data['Quantity'])

# (c) Ensure the product category is treated as categorical.
merged_data['Category'] = merged_data['Category'].astype('category')

# (d) Create demographic dummy variables.
# For this example we use the two survey items on age and gender.
merged_data = pd.get_dummies(merged_data, columns=['Q_demos_age', 'Q_demos_gender'], drop_first=True)

# List the created dummy columns (they should no longer include spaces or dashes).
demo_dummies = [col for col in merged_data.columns
                if col.startswith('Q_demos_age_') or col.startswith('Q_demos_gender_')]
print("\nDemographic dummy variables created:")
print(demo_dummies)

# ---------------------------
# 5. OLS Regression: Log–Log Model with Interaction Terms
# ---------------------------
# We estimate:
#   log_quantity = β0 + β1*log_price + β2*log_price*C(Category) + (demographic controls) + ε
# Build the regression formula.
formula = 'log_quantity ~ log_price * C(Category)'

# Add demographic dummy variables if present.
if demo_dummies:
    formula += ' + ' + ' + '.join(demo_dummies)

print("\nRunning OLS regression with formula:")
print(formula)

# Fit the model with robust standard errors.
try:
    ols_model = smf.ols(formula, data=merged_data).fit(cov_type='HC3')
    print("\nOLS Regression Results:")
    print(ols_model.summary())
except Exception as e:
    print("\nAn error occurred in OLS regression:")
    print(e)

# ---------------------------
# 6. Visualization
# ---------------------------
plt.figure(figsize=(8,6))
sns.scatterplot(x='log_price', y='log_quantity', hue='Category', data=merged_data, alpha=0.6)
plt.xlabel("Log(Unit Price)")
plt.ylabel("Log(Quantity Purchased)")
plt.title("Log(Quantity) vs. Log(Unit Price) by Product Category")
plt.legend(title='Category')
plt.show()

# ---------------------------
# 7. (Optional) Panel Data Analysis with Consumer Fixed Effects
# ---------------------------
# If the purchase data contain repeated orders per consumer over time,
# you can control for time-invariant consumer characteristics.
if 'Order_Date' in merged_data.columns:
    merged_data['Order_Date'] = pd.to_datetime(merged_data['Order_Date'])
    panel_data = merged_data.set_index(['Survey_ResponseID', 'Order_Date']).sort_index()

    # Prepare exogenous variables: log_price, category dummies, and demographics.
    exog = panel_data[['log_price']].copy()
    cat_dummies = pd.get_dummies(panel_data['Category'], drop_first=True)
    exog = exog.join(cat_dummies)
    demo_controls = panel_data[[col for col in panel_data.columns
                                if col.startswith('Q_demos_age_') or col.startswith('Q_demos_gender_')]]
    exog = exog.join(demo_controls)
    exog = sm.add_constant(exog)

    # Fit the PanelOLS with entity (consumer) fixed effects.
    panel_model = PanelOLS(panel_data['log_quantity'], exog, entity_effects=True)
    fe_results = panel_model.fit(cov_type='robust')

    print("\nFixed Effects Panel OLS Results:")
    print(fe_results.summary)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import statsmodels.formula.api as smf
from statsmodels.regression.mixed_linear_model import MixedLM

# =============================================================================
# 1. Load Data (only needed columns to save memory)
# =============================================================================
# Specify the columns we need.
purchase_cols = [
    'Order Date',
    'Purchase Price Per Unit',
    'Quantity',
    'Shipping Address State',
    'Category',
    'Survey ResponseID'
]
survey_cols = [
    'Survey ResponseID',
    'Q-demos-age',
    'Q-demos-gender'
]

# Replace the file paths with your actual file locations.
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv',
                            usecols=purchase_cols, low_memory=True)
survey_data   = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv',
                            usecols=survey_cols, low_memory=True)

print("Original Purchase data columns:", purchase_data.columns.tolist())
print("Original Survey data columns:", survey_data.columns.tolist())

# =============================================================================
# 2. Rename Identifier Columns and Clean Column Names
# =============================================================================
# Rename "Survey ResponseID" to "Survey_ResponseID" and "Shipping Address State" to a cleaner name.
purchase_data.rename(columns={
    'Survey ResponseID': 'Survey_ResponseID',
    'Shipping Address State': 'Shipping_Address_State'
}, inplace=True)
survey_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)

# Define a helper to clean column names (remove spaces and dashes)
def clean_col(col):
    return col.strip().replace(' ', '_').replace('-', '_')

purchase_data.columns = [clean_col(c) for c in purchase_data.columns]
survey_data.columns   = [clean_col(c) for c in survey_data.columns]

print("\nAfter renaming and cleaning:")
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# =============================================================================
# 3. Merge Purchase and Survey Data
# =============================================================================
# Merge on the common consumer identifier.
merged_data = pd.merge(purchase_data, survey_data, on='Survey_ResponseID', how='inner')
print("\nMerged data (first 5 rows):")
print(merged_data.head())

# =============================================================================
# 4. Compute Spending and Aggregate Purchase Metrics by Consumer
# =============================================================================
# (a) Compute spend for each purchase record.
merged_data['spend'] = merged_data['Purchase_Price_Per_Unit'] * merged_data['Quantity']

# (b) Aggregate purchase data at the consumer level.
# Here we assume that for each Survey_ResponseID, the shipping address state is constant.
agg_purchase = merged_data.groupby('Survey_ResponseID').agg(
    total_spend = ('spend', 'sum'),
    purchase_frequency = ('Quantity', 'sum'),  # Alternatively, use 'count' if number of orders is desired.
    avg_price = ('Purchase_Price_Per_Unit', 'mean'),
    category_diversity = ('Category', lambda x: x.nunique()),
    Shipping_Address_State = ('Shipping_Address_State', lambda x: x.mode()[0] if not x.mode().empty else x.iloc[0])
).reset_index()

print("\nAggregated purchase data (first 5 rows):")
print(agg_purchase.head())

# =============================================================================
# 5. Aggregate Survey Data by Consumer
# =============================================================================
# For survey responses we assume the values are constant per consumer.
agg_survey = merged_data.groupby('Survey_ResponseID').agg({
    'Q_demos_age': 'first',
    'Q_demos_gender': 'first'
}).reset_index()

print("\nAggregated survey data (first 5 rows):")
print(agg_survey.head())

# =============================================================================
# 6. Merge Aggregated Purchase and Survey Data into a Consumer-Level Dataset
# =============================================================================
consumer_data = pd.merge(agg_purchase, agg_survey, on='Survey_ResponseID', how='left')
print("\nConsumer-level data (first 5 rows):")
print(consumer_data.head())

# =============================================================================
# 7. Create Log–Transformed Variables
# =============================================================================
consumer_data['log_total_spend'] = np.log(consumer_data['total_spend'])
consumer_data['log_purchase_frequency'] = np.log(consumer_data['purchase_frequency'])
consumer_data['log_avg_price'] = np.log(consumer_data['avg_price'])

# =============================================================================
# 8. Create Demographic Dummy Variables
# =============================================================================
# Convert demographic variables to dummy variables. This avoids issues with special characters in names.
consumer_data = pd.get_dummies(consumer_data, columns=['Q_demos_age', 'Q_demos_gender'], drop_first=True)

# List created dummy variables.
demo_dummies = [col for col in consumer_data.columns
                if col.startswith('Q_demos_age_') or col.startswith('Q_demos_gender_')]
print("\nDemographic dummy variables created:")
print(demo_dummies)

# =============================================================================
# 9. Set the State Variable as a Categorical Group
# =============================================================================
consumer_data['Shipping_Address_State'] = consumer_data['Shipping_Address_State'].astype('category')

# =============================================================================
# 10. Mixed Effects Model: Multilevel Modeling with Random Intercepts by State
# =============================================================================
# We model log_total_spend as a function of:
#   - log_purchase_frequency, log_avg_price, and category_diversity (all continuous)
#   - Demographic dummies (e.g., age and gender)
# with random intercepts for Shipping_Address_State.
formula = "log_total_spend ~ log_purchase_frequency + log_avg_price + category_diversity"
if demo_dummies:
    formula += " + " + " + ".join(demo_dummies)

print("\nMixed Effects Model formula:")
print(formula)

# Fit the mixed effects model.
try:
    md = MixedLM.from_formula(formula, groups="Shipping_Address_State", data=consumer_data)
    mixedlm_result = md.fit(reml=False)
    print("\nMixed Effects Model Results:")
    print(mixedlm_result.summary())
except Exception as e:
    print("\nAn error occurred while fitting the mixed effects model:")
    print(e)

# =============================================================================
# 11. (Optional) Spatial Analysis
# =============================================================================
# If you have external state-level or regional data (e.g., economic indicators, cultural indices),
# you could merge that data with consumer_data using the Shipping_Address_State key and explore:
#   - Geographic clustering (e.g., via choropleth maps)
#   - Interactions between state-level variables and individual demographics.
#
# For example, if you had a DataFrame 'state_data' with columns:
#   ['Shipping_Address_State', 'State_Economic_Indicator']
# You could merge and then include an interaction term:
#
#   consumer_data = consumer_data.merge(state_data, on='Shipping_Address_State', how='left')
#   formula = "log_total_spend ~ log_purchase_frequency + log_avg_price + category_diversity + State_Economic_Indicator * (demographic dummies)"
#
# And then fit the mixed effects model as above.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import statsmodels.formula.api as smf
from statsmodels.regression.mixed_linear_model import MixedLM

# =============================================================================
# 1. Load Data (only needed columns to reduce memory usage)
# =============================================================================
# Specify columns needed:
purchase_cols = ['Order Date', 'Purchase Price Per Unit', 'Quantity',
                 'Shipping Address State', 'Category', 'Survey ResponseID']
survey_cols   = ['Survey ResponseID', 'Q-demos-age', 'Q-demos-gender']

# Replace these file paths with the paths on your machine.
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv',
                            usecols=purchase_cols, low_memory=True)
survey_data   = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv',
                            usecols=survey_cols, low_memory=True)

print("Original Purchase data columns:", purchase_data.columns.tolist())
print("Original Survey data columns:", survey_data.columns.tolist())

# =============================================================================
# 2. Rename Identifier Columns and Clean Column Names
# =============================================================================
# Rename "Survey ResponseID" to "Survey_ResponseID" and "Shipping Address State" to a simpler name.
purchase_data.rename(columns={
    'Survey ResponseID': 'Survey_ResponseID',
    'Shipping Address State': 'Shipping_Address_State'
}, inplace=True)
survey_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)

# Define a helper function to clean column names (remove spaces and dashes)
def clean_col(col):
    return col.strip().replace(' ', '_').replace('-', '_')

purchase_data.columns = [clean_col(c) for c in purchase_data.columns]
survey_data.columns   = [clean_col(c) for c in survey_data.columns]

print("\nAfter renaming and cleaning:")
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# =============================================================================
# 3. Merge Purchase and Survey Data
# =============================================================================
merged_data = pd.merge(purchase_data, survey_data, on='Survey_ResponseID', how='inner')
print("\nMerged data (first 5 rows):")
print(merged_data.head())

# =============================================================================
# 4. Compute Spending and Aggregate Purchase Metrics by Consumer
# =============================================================================
# (a) Compute spend per purchase record.
merged_data['spend'] = merged_data['Purchase_Price_Per_Unit'] * merged_data['Quantity']

# (b) Aggregate purchase data at the consumer level.
agg_purchase = merged_data.groupby('Survey_ResponseID').agg(
    total_spend = ('spend', 'sum'),
    purchase_frequency = ('Quantity', 'sum'),  # total quantity purchased
    avg_price = ('Purchase_Price_Per_Unit', 'mean'),
    category_diversity = ('Category', lambda x: x.nunique()),
    # For state, we assume the mode is representative.
    Shipping_Address_State = ('Shipping_Address_State', lambda x: x.mode()[0] if not x.mode().empty else x.iloc[0])
).reset_index()

print("\nAggregated purchase data (first 5 rows):")
print(agg_purchase.head())

# =============================================================================
# 5. Aggregate Survey Data by Consumer
# =============================================================================
# Since demographic responses should be constant per consumer, take the first response.
agg_survey = merged_data.groupby('Survey_ResponseID').agg({
    'Q_demos_age': 'first',
    'Q_demos_gender': 'first'
}).reset_index()

print("\nAggregated survey data (first 5 rows):")
print(agg_survey.head())

# =============================================================================
# 6. Merge Aggregated Purchase and Survey Data into a Consumer-Level Dataset
# =============================================================================
consumer_data = pd.merge(agg_purchase, agg_survey, on='Survey_ResponseID', how='left')
print("\nConsumer-level data (first 5 rows):")
print(consumer_data.head())

# =============================================================================
# 7. Create Log–Transformed Variables
# =============================================================================
consumer_data['log_total_spend'] = np.log(consumer_data['total_spend'])
consumer_data['log_purchase_frequency'] = np.log(consumer_data['purchase_frequency'])
consumer_data['log_avg_price'] = np.log(consumer_data['avg_price'])

# =============================================================================
# 8. Create Demographic Dummy Variables
# =============================================================================
# We want to create dummies from the survey responses.
consumer_data = pd.get_dummies(consumer_data, columns=['Q_demos_age', 'Q_demos_gender'], drop_first=True)

# List the new dummy variable names (they may include spaces or other non-standard characters).
demo_dummies = [col for col in consumer_data.columns
                if col.startswith('Q_demos_age_') or col.startswith('Q_demos_gender_')]
print("\nDemographic dummy variables created:")
print(demo_dummies)

# =============================================================================
# 9. Set the State Variable as a Categorical Group
# =============================================================================
consumer_data['Shipping_Address_State'] = consumer_data['Shipping_Address_State'].astype('category')

# =============================================================================
# 10. Mixed Effects Model: Multilevel Modeling with Random Intercepts by State
# =============================================================================
# We will model log_total_spend as a function of purchase behavior and demographics.
# Because some of the dummy variable names contain spaces and/or dashes, we wrap them in Q("...") in the formula.
# (If you prefer backticks, you can also enclose them in backticks.)
# Here is an example formula:
formula = (
    "log_total_spend ~ log_purchase_frequency + log_avg_price + category_diversity "
    "+ Q('Q_demos_age_25 _34 years') + Q('Q_demos_age_35 _44 years') + "
    "Q('Q_demos_age_45 _54 years') + Q('Q_demos_age_55 _64 years') + "
    "Q('Q_demos_age_65 and older') + Q('Q_demos_gender_Male') + "
    "Q('Q_demos_gender_Other') + Q('Q_demos_gender_Prefer not to say')"
)

# Note: The dummy variable names printed by pd.get_dummies may have been created exactly as:
# 'Q_demos_age_25 - 34 years', etc. However, dashes are replaced with underscores by our cleaning function.
# For clarity, check consumer_data.columns.tolist() to see the exact names.
print("\nConsumer data columns:")
print(consumer_data.columns.tolist())

# Adjust the formula variable names to match exactly.
# For example, if get_dummies produced names like "Q_demos_age_25 _34 years" (with an underscore in place of the dash),
# then use those exact names in Q(…).
# You may need to modify the strings below if your dummy names differ.
formula = (
    "log_total_spend ~ log_purchase_frequency + log_avg_price + category_diversity "
    "+ Q('Q_demos_age_25 _34 years') + Q('Q_demos_age_35 _44 years') + "
    "Q('Q_demos_age_45 _54 years') + Q('Q_demos_age_55 _64 years') + "
    "Q('Q_demos_age_65 and older') + Q('Q_demos_gender_Male')"
)
# (We dropped the 'Other' and 'Prefer not to say' if they are not present or are all zeros.)

print("\nMixed Effects Model formula:")
print(formula)

# Fit the mixed effects model using random intercepts for state.
try:
    md = MixedLM.from_formula(formula, groups="Shipping_Address_State", data=consumer_data)
    mixedlm_result = md.fit(reml=False)
    print("\nMixed Effects Model Results:")
    print(mixedlm_result.summary())
except Exception as e:
    print("\nAn error occurred while fitting the mixed effects model:")
    print(e)

# =============================================================================
# 11. Visualization: Boxplot of Log Total Spend by State
# =============================================================================
plt.figure(figsize=(10, 6))
sns.boxplot(x="Shipping_Address_State", y="log_total_spend", data=consumer_data)
plt.title("Distribution of Log(Total Spend) by State")
plt.xlabel("Shipping Address State")
plt.ylabel("Log(Total Spend)")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import statsmodels.formula.api as smf

# ---------------------------
# 1. Load Data
# ---------------------------
# Define the columns needed for a lower memory footprint.
purchase_cols = ['Order Date', 'Purchase Price Per Unit', 'Quantity',
                 'Shipping Address State', 'Category', 'Survey ResponseID']
survey_cols   = ['Survey ResponseID', 'Q-demos-age', 'Q-demos-gender']

# Replace these file paths with your actual file paths.
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv',
                            usecols=purchase_cols, low_memory=True)
survey_data   = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv',
                            usecols=survey_cols, low_memory=True)

print("Original Purchase data columns:", purchase_data.columns.tolist())
print("Original Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 2. Rename and Clean Column Names
# ---------------------------
# Rename columns to remove spaces and dashes for easier handling.
purchase_data.rename(columns={
    'Order Date': 'Order_Date',
    'Purchase Price Per Unit': 'Purchase_Price_Per_Unit',
    'Shipping Address State': 'Shipping_Address_State',
    'Survey ResponseID': 'Survey_ResponseID'
}, inplace=True)
survey_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)

# Optionally, remove spaces/dashes from all column names:
def clean_col(col):
    return col.strip().replace(' ', '_').replace('-', '_')
purchase_data.columns = [clean_col(c) for c in purchase_data.columns]
survey_data.columns   = [clean_col(c) for c in survey_data.columns]

print("\nAfter renaming and cleaning:")
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 3. Merge Purchase and Survey Data
# ---------------------------
# For a COVID lockdown analysis we need purchase record dates and state.
merged_data = pd.merge(purchase_data, survey_data, on='Survey_ResponseID', how='inner')
print("\nMerged data (first 5 rows):")
print(merged_data.head())

# ---------------------------
# 4. Convert Dates and Compute Spending
# ---------------------------
# Convert order dates to datetime.
merged_data['Order_Date'] = pd.to_datetime(merged_data['Order_Date'], errors='coerce')

# Create a new variable for spend.
merged_data['spend'] = merged_data['Purchase_Price_Per_Unit'] * merged_data['Quantity']

# (Optional) Remove records with missing or nonpositive values.
merged_data = merged_data[(merged_data['Purchase_Price_Per_Unit'] > 0) &
                          (merged_data['Quantity'] > 0) &
                          (~merged_data['Order_Date'].isna())]

# ---------------------------
# 5. Incorporate State-level COVID Lockdown Dates
# ---------------------------
# Here we “simulate” a lockdown dataset for illustration.
# In practice you might load this from a file with actual lockdown start/end dates.
lockdown_data = pd.DataFrame({
    'State': ['NJ', 'PA', 'CA', 'VA', 'SD'],
    'lockdown_start': ['2020-03-16', '2020-03-20', '2020-03-19', '2020-03-24', '2020-03-26'],
    'lockdown_end':   ['2020-06-08', '2020-06-15', '2020-06-30', '2020-06-10', '2020-05-01']
})
# Convert to datetime.
lockdown_data['lockdown_start'] = pd.to_datetime(lockdown_data['lockdown_start'])
lockdown_data['lockdown_end'] = pd.to_datetime(lockdown_data['lockdown_end'])
# Compute lockdown length in days.
lockdown_data['lockdown_length'] = (lockdown_data['lockdown_end'] - lockdown_data['lockdown_start']).dt.days

print("\nLockdown data:")
print(lockdown_data)

# Merge lockdown data into merged_data.
# (Assume the purchase state matches the "State" in lockdown_data.)
merged_data = pd.merge(merged_data, lockdown_data, left_on='Shipping_Address_State', right_on='State', how='left')

# ---------------------------
# 6. Create a 'during_lockdown' Indicator
# ---------------------------
# For each purchase, mark as 1 if the Order_Date falls between the lockdown start and end for that state.
merged_data['during_lockdown'] = merged_data.apply(
    lambda row: 1 if (row['Order_Date'] >= row['lockdown_start']) and (row['Order_Date'] <= row['lockdown_end'])
                else 0, axis=1)

# ---------------------------
# 7. Create Log–Transformed Outcome Variable
# ---------------------------
# We analyze how lockdown conditions affect spending; here we use log(spend) as the outcome.
merged_data['log_spend'] = np.log(merged_data['spend'])

# ---------------------------
# 8. Run Regression Analysis
# ---------------------------
# We want to estimate a model such as:
#
#   log_spend = β0 + β1 * during_lockdown + β2 * lockdown_length + β3 * (during_lockdown * lockdown_length) + controls + ε
#
# You might also control for demographics (if desired) and product category.
# Here is a simple example with the lockdown variables only.
formula = ("log_spend ~ during_lockdown + lockdown_length + "
           "during_lockdown:lockdown_length")

print("\nRunning OLS regression with formula:")
print(formula)

# Run the regression with robust standard errors.
try:
    lockdown_model = smf.ols(formula, data=merged_data).fit(cov_type='HC3')
    print("\nOLS Regression Results on COVID Lockdown Impact:")
    print(lockdown_model.summary())
except Exception as e:
    print("\nAn error occurred during OLS regression:")
    print(e)

# ---------------------------
# 9. (Optional) Aggregated Consumer-level Analysis
# ---------------------------
# Alternatively, you might wish to compare (for each consumer) purchase behavior pre–and post–lockdown.
# For example, aggregate purchases by consumer separately for orders during lockdown and not.
consumer_lockdown = merged_data.groupby(['Survey_ResponseID', 'during_lockdown']).agg(
    total_spend=('spend', 'sum'),
    purchase_frequency=('Quantity', 'sum')
).reset_index()

print("\nConsumer-level purchase data by during_lockdown (first 5 rows):")
print(consumer_lockdown.head())

# ---------------------------
# 10. Visualization
# ---------------------------
# For example, plot the distribution of log_spend by whether the purchase occurred during lockdown.
plt.figure(figsize=(8,6))
sns.boxplot(x='during_lockdown', y='log_spend', data=merged_data)
plt.title("Distribution of Log(Spend) for Purchases: During vs. Outside Lockdown")
plt.xlabel("During Lockdown (0 = No, 1 = Yes)")
plt.ylabel("Log(Spend)")
plt.show()

# You can also examine state-level differences by plotting log_spend by state.
plt.figure(figsize=(10,6))
sns.boxplot(x='Shipping_Address_State', y='log_spend', data=merged_data)
plt.title("Log(Spend) by Shipping Address State")
plt.xlabel("State")
plt.ylabel("Log(Spend)")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.formula.api as smf

# ---------------------------
# 1. Load and Prepare Data
# ---------------------------
# File paths for purchase and survey files.
purchase_path = '/content/drive/MyDrive/amazon/amazon-purchases.csv'
survey_path   = '/content/drive/MyDrive/amazon/survey.csv'

# Load the needed columns.
purchase_cols = ['Order Date', 'Purchase Price Per Unit', 'Quantity',
                 'Shipping Address State', 'Category', 'Survey ResponseID']
survey_cols   = ['Survey ResponseID']  # (Add additional survey columns if needed)

purchase_data = pd.read_csv(purchase_path, usecols=purchase_cols)
survey_data   = pd.read_csv(survey_path, usecols=survey_cols)

# Print original columns.
print("Original Purchase data columns:", purchase_data.columns.tolist())
print("Original Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 2. Rename Columns (remove spaces)
# ---------------------------
purchase_data.rename(columns={
    'Order Date': 'Order_Date',
    'Purchase Price Per Unit': 'Purchase_Price_Per_Unit',
    'Shipping Address State': 'Shipping_Address_State',
    'Survey ResponseID': 'Survey_ResponseID'
}, inplace=True)
survey_data.rename(columns={'Survey ResponseID': 'Survey_ResponseID'}, inplace=True)

# Optionally, clean column names further.
def clean_col(col):
    return col.strip().replace(' ', '_').replace('-', '_')
purchase_data.columns = [clean_col(c) for c in purchase_data.columns]
survey_data.columns   = [clean_col(c) for c in survey_data.columns]

print("\nAfter renaming:")
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 3. Merge Purchase and Survey Data
# ---------------------------
merged_data = pd.merge(purchase_data, survey_data, on='Survey_ResponseID', how='inner')
print("\nMerged data (first 5 rows):")
print(merged_data.head())

# ---------------------------
# 4. Process Purchase Data
# ---------------------------
# Convert Order_Date to datetime; drop observations with missing or nonpositive values.
merged_data['Order_Date'] = pd.to_datetime(merged_data['Order_Date'], errors='coerce')
merged_data = merged_data[(merged_data['Purchase_Price_Per_Unit'] > 0) &
                          (merged_data['Quantity'] > 0) &
                          (~merged_data['Order_Date'].isna())]

# Compute spend per order.
merged_data['spend'] = merged_data['Purchase_Price_Per_Unit'] * merged_data['Quantity']

# Create the log of spend.
merged_data['log_spend'] = np.log(merged_data['spend'])

# ---------------------------
# 5. Incorporate Lockdown Data
# ---------------------------
# For this example, we simulate a lockdown dataset that provides the start date of the lockdown per state.
# (In practice you would load an external dataset with the actual lockdown dates.)
lockdown_data = pd.DataFrame({
    'State': ['NJ', 'PA', 'CA', 'VA', 'SD'],
    'lockdown_start': ['2020-03-16', '2020-03-20', '2020-03-19', '2020-03-24', '2020-03-26']
})
lockdown_data['lockdown_start'] = pd.to_datetime(lockdown_data['lockdown_start'])

# Merge the lockdown start date into the merged data via the shipping state.
merged_data = pd.merge(merged_data, lockdown_data, left_on='Shipping_Address_State', right_on='State', how='left')

# Check for missing lockdown dates.
print("\nSummary of lockdown_start by state:")
print(merged_data.groupby('Shipping_Address_State')['lockdown_start'].first())

# ---------------------------
# 6. Create Running Variable and Treatment Indicator for RDD
# ---------------------------
# Compute the running variable: number of days between Order_Date and the lockdown_start.
merged_data['days_from_lockdown'] = (merged_data['Order_Date'] - merged_data['lockdown_start']).dt.days

# Define treatment indicator: 1 if order occurred on or after lockdown start, 0 if before.
merged_data['post_lockdown'] = (merged_data['days_from_lockdown'] >= 0).astype(int)

# ---------------------------
# 7. Restrict Sample to a Bandwidth Around the Cutoff
# ---------------------------
# Choose a bandwidth (e.g., 30 days around the cutoff).
bandwidth = 30
rdd_data = merged_data[(merged_data['days_from_lockdown'].abs() <= bandwidth)].copy()
print("\nNumber of observations within ±{} days of lockdown start: {}".format(bandwidth, len(rdd_data)))

# ---------------------------
# 8. Regression Discontinuity Estimation
# ---------------------------
# We use a local linear regression specification:
#
#   log_spend = β0 + β1 * post_lockdown + β2 * days_from_lockdown + β3 * (post_lockdown * days_from_lockdown) + ε
#
formula_rdd = 'log_spend ~ post_lockdown + days_from_lockdown + post_lockdown:days_from_lockdown'
print("\nRunning RDD OLS regression with formula:")
print(formula_rdd)

try:
    rdd_model = smf.ols(formula_rdd, data=rdd_data).fit(cov_type='HC3')
    print("\nRegression Discontinuity OLS Regression Results:")
    print(rdd_model.summary())
except Exception as e:
    print("\nAn error occurred during the RDD regression:")
    print(e)

# ---------------------------
# 9. Visualization of the Regression Discontinuity
# ---------------------------
# We create a scatterplot of log_spend vs. days_from_lockdown, and overlay local linear fits for pre- and post-lockdown.
plt.figure(figsize=(8, 6))
sns.scatterplot(x='days_from_lockdown', y='log_spend', data=rdd_data, alpha=0.3, label='Observations')

# Fit separate linear models on either side of the cutoff.
pre_data = rdd_data[rdd_data['days_from_lockdown'] < 0]
post_data = rdd_data[rdd_data['days_from_lockdown'] >= 0]

# Fit linear regressions for pre and post groups.
pre_model = smf.ols('log_spend ~ days_from_lockdown', data=pre_data).fit()
post_model = smf.ols('log_spend ~ days_from_lockdown', data=post_data).fit()

# Generate predictions along the running variable.
x_vals = np.linspace(-bandwidth, bandwidth, 100)
pre_pred = pre_model.predict(pd.DataFrame({'days_from_lockdown': x_vals[x_vals < 0]}))
post_pred = post_model.predict(pd.DataFrame({'days_from_lockdown': x_vals[x_vals >= 0]}))

plt.plot(x_vals[x_vals < 0], pre_pred, color='blue', label='Pre-lockdown Fit')
plt.plot(x_vals[x_vals >= 0], post_pred, color='red', label='Post-lockdown Fit')
plt.axvline(0, color='black', linestyle='--', label='Lockdown Start')
plt.xlabel("Days from Lockdown Start")
plt.ylabel("Log(Spend)")
plt.title("Regression Discontinuity: Impact of Lockdown Start on Log(Spend)")
plt.legend()
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.formula.api as smf

# ---------------------------
# 1. Load Data
# ---------------------------
# File paths (update these paths as needed)
purchase_path = '/content/drive/MyDrive/amazon/amazon-purchases.csv'
survey_path   = '/content/drive/MyDrive/amazon/survey.csv'

# Read only the needed columns:
purchase_cols = ['Order Date', 'Purchase Price Per Unit', 'Quantity',
                 'Shipping Address State', 'Category', 'Survey ResponseID']
survey_cols   = ['Survey ResponseID', 'Q-demos-age', 'Q-demos-gender']  # for example

purchase_data = pd.read_csv(purchase_path, usecols=purchase_cols)
survey_data   = pd.read_csv(survey_path, usecols=survey_cols)

print("Original Purchase data columns:", purchase_data.columns.tolist())
print("Original Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 2. Rename Columns (to remove spaces and special characters)
# ---------------------------
# Rename purchase data columns.
purchase_data.rename(columns={
    'Order Date': 'Order_Date',
    'Purchase Price Per Unit': 'Purchase_Price_Per_Unit',
    'Shipping Address State': 'Shipping_Address_State',
    'Survey ResponseID': 'Survey_ResponseID'
}, inplace=True)

# Rename survey data columns.
survey_data.rename(columns={
    'Survey ResponseID': 'Survey_ResponseID',
    'Q-demos-age': 'Q_demos_age',
    'Q-demos-gender': 'Q_demos_gender'
}, inplace=True)

print("\nAfter renaming:")
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 3. Merge Purchase and Survey Data
# ---------------------------
# (If survey info is needed, merge by the common consumer ID.)
merged_data = pd.merge(purchase_data, survey_data, on='Survey_ResponseID', how='inner')

# ---------------------------
# 4. Data Cleaning and Variable Creation
# ---------------------------
# Convert order date to datetime and filter out any rows with missing dates or nonpositive values.
merged_data['Order_Date'] = pd.to_datetime(merged_data['Order_Date'], errors='coerce')
merged_data = merged_data[(merged_data['Purchase_Price_Per_Unit'] > 0) &
                          (merged_data['Quantity'] > 0) &
                          (~merged_data['Order_Date'].isna())]

# Compute spend for each order and create a log variable.
merged_data['spend'] = merged_data['Purchase_Price_Per_Unit'] * merged_data['Quantity']
merged_data['log_spend'] = np.log(merged_data['spend'])

# ---------------------------
# 5. Merge in Lockdown Dates by State
# ---------------------------
# In practice, load a dataset with the actual lockdown start dates per state.
# Here we simulate a small example.
lockdown_data = pd.DataFrame({
    'State': ['NJ', 'PA', 'CA', 'VA', 'SD'],
    'lockdown_start': ['2020-03-16', '2020-03-20', '2020-03-19', '2020-03-24', '2020-03-26']
})
lockdown_data['lockdown_start'] = pd.to_datetime(lockdown_data['lockdown_start'])

# Merge lockdown data based on the Shipping Address State.
merged_data = pd.merge(merged_data, lockdown_data, left_on='Shipping_Address_State', right_on='State', how='left')

# ---------------------------
# 6. Create Period Indicator
# ---------------------------
# We compute the number of days between the Order_Date and the lockdown_start.
merged_data['days_from_lockdown'] = (merged_data['Order_Date'] - merged_data['lockdown_start']).dt.days

# Define three periods:
#   - pre_lockdown: order date is before the lockdown start (days_from_lockdown < 0)
#   - during_lockdown: order date is within the first 30 days of the lockdown (0 <= days_from_lockdown < 30)
#   - post_lockdown: order date is 30 or more days after lockdown start (days_from_lockdown >= 30)
merged_data['period'] = np.where(merged_data['days_from_lockdown'] < 0, 'pre_lockdown',
                          np.where(merged_data['days_from_lockdown'] < 30, 'during_lockdown', 'post_lockdown'))

# Check a few rows.
print("\nMerged data (first 5 rows):")
print(merged_data[['Order_Date', 'lockdown_start', 'days_from_lockdown', 'period', 'Category', 'spend']].head())

# ---------------------------
# 7. Aggregate Data by Product Category and Period
# ---------------------------
# Group by Category and period, and calculate summary statistics.
agg_category = merged_data.groupby(['Category', 'period']).agg(
    avg_spend=('spend', 'mean'),
    total_orders=('spend', 'count'),
    avg_quantity=('Quantity', 'mean')
).reset_index()

print("\nAggregated purchase data by Category and period:")
print(agg_category.head(10))

# ---------------------------
# 8. Visualization: Compare Average Spend and Quantity by Category and Period
# ---------------------------
plt.figure(figsize=(12, 6))
sns.barplot(x='Category', y='avg_spend', hue='period', data=agg_category)
plt.title("Average Spend by Product Category and Period")
plt.ylabel("Average Spend")
plt.xlabel("Product Category")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 6))
sns.barplot(x='Category', y='avg_quantity', hue='period', data=agg_category)
plt.title("Average Quantity Purchased by Product Category and Period")
plt.ylabel("Average Quantity")
plt.xlabel("Product Category")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# ---------------------------
# 9. Regression Analysis: Testing for Differences Across Periods and Categories
# ---------------------------
# We can also test for statistically significant differences using OLS.
# For example, we estimate a model of the form:
#   log_spend = β0 + β1 * C(period) + β2 * C(Category) + β3 * C(period):C(Category) + ε
#
# Here the interaction term tests whether the effect (jump in spend) differs by product category.
formula = 'log_spend ~ C(period) * C(Category)'
print("\nRunning OLS regression with formula:")
print(formula)

try:
    ols_model = smf.ols(formula, data=merged_data).fit(cov_type='HC3')
    print("\nOLS Regression Results:")
    print(ols_model.summary())
except Exception as e:
    print("\nAn error occurred during the OLS regression:")
    print(e)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.formula.api as smf

# ---------------------------
# 1. Load Data (only needed columns; use low_memory and specify dtypes if possible)
# ---------------------------
# Define the columns we need
purchase_cols = ['Order Date', 'Purchase Price Per Unit', 'Quantity',
                 'Shipping Address State', 'Category', 'Survey ResponseID']
survey_cols   = ['Survey ResponseID', 'Q-demos-age', 'Q-demos-gender']

# Read in the CSV files. (Set low_memory=True; adjust file paths as needed.)
purchase_data = pd.read_csv(
    '/content/drive/MyDrive/amazon/amazon-purchases.csv',
    usecols=purchase_cols,
    low_memory=True
)
survey_data = pd.read_csv(
    '/content/drive/MyDrive/amazon/survey.csv',
    usecols=survey_cols,
    low_memory=True
)

print("Original Purchase data columns:", purchase_data.columns.tolist())
print("Original Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 2. Rename Columns to remove spaces and special characters
# ---------------------------
# Rename purchase columns.
purchase_data.rename(columns={
    'Order Date': 'Order_Date',
    'Purchase Price Per Unit': 'Purchase_Price_Per_Unit',
    'Shipping Address State': 'Shipping_Address_State',
    'Survey ResponseID': 'Survey_ResponseID'
}, inplace=True)

# Rename survey columns.
survey_data.rename(columns={
    'Survey ResponseID': 'Survey_ResponseID',
    'Q-demos-age': 'Q_demos_age',
    'Q-demos-gender': 'Q_demos_gender'
}, inplace=True)

print("\nAfter renaming:")
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 3. (Optional) Sample the data to reduce memory usage
# ---------------------------
# If the dataset is very large, you may want to work with a random sample.
# Uncomment the following lines to sample 100,000 rows from purchases.
# purchase_data = purchase_data.sample(n=100000, random_state=42)

# (Also, if survey_data has duplicate consumer IDs, keep only one per ID.)
survey_data = survey_data.drop_duplicates(subset=['Survey_ResponseID'])

# ---------------------------
# 4. Merge Purchase and Survey Data
# ---------------------------
merged_data = pd.merge(purchase_data, survey_data, on='Survey_ResponseID', how='inner')
print("\nMerged data (first 5 rows):")
print(merged_data.head())

# ---------------------------
# 5. Data Cleaning and Feature Creation
# ---------------------------
# Convert Order_Date to datetime; drop rows with invalid dates.
merged_data['Order_Date'] = pd.to_datetime(merged_data['Order_Date'], errors='coerce')
merged_data = merged_data[merged_data['Order_Date'].notnull()]

# Ensure numeric columns are properly typed and downcast if possible.
merged_data['Purchase_Price_Per_Unit'] = pd.to_numeric(merged_data['Purchase_Price_Per_Unit'], errors='coerce', downcast='float')
merged_data['Quantity'] = pd.to_numeric(merged_data['Quantity'], errors='coerce', downcast='integer')

# Drop rows with nonpositive prices or quantities.
merged_data = merged_data[(merged_data['Purchase_Price_Per_Unit'] > 0) & (merged_data['Quantity'] > 0)]

# Create a spend variable per order.
merged_data['spend'] = merged_data['Purchase_Price_Per_Unit'] * merged_data['Quantity']
# Create a log-spend variable (handle zeros if needed).
merged_data['log_spend'] = np.log(merged_data['spend'])

# ---------------------------
# 6. Incorporate Lockdown Dates by State
# ---------------------------
# Here we simulate a small lockdown dataset.
lockdown_data = pd.DataFrame({
    'State': ['NJ', 'PA', 'CA', 'VA', 'SD'],
    'lockdown_start': ['2020-03-16', '2020-03-20', '2020-03-19', '2020-03-24', '2020-03-26']
})
lockdown_data['lockdown_start'] = pd.to_datetime(lockdown_data['lockdown_start'])

# Merge lockdown info onto the merged data (by state)
merged_data = pd.merge(merged_data, lockdown_data, left_on='Shipping_Address_State', right_on='State', how='left')

# Compute days from lockdown start.
merged_data['days_from_lockdown'] = (merged_data['Order_Date'] - merged_data['lockdown_start']).dt.days

# ---------------------------
# 7. Define Periods: pre_lockdown, during_lockdown, post_lockdown
# ---------------------------
# For example, let:
#   - pre_lockdown: days_from_lockdown < 0
#   - during_lockdown: 0 <= days_from_lockdown < 30
#   - post_lockdown: days_from_lockdown >= 30
merged_data['period'] = np.where(merged_data['days_from_lockdown'] < 0, 'pre_lockdown',
                          np.where(merged_data['days_from_lockdown'] < 30, 'during_lockdown', 'post_lockdown'))

# ---------------------------
# 8. Ensure Category is a Categorical Variable
# ---------------------------
# (Convert product Category to a categorical type to save memory.)
merged_data['Category'] = merged_data['Category'].astype('category')

# ---------------------------
# 9. Aggregate Data by Category and Period
# ---------------------------
# Group by Category and period and calculate summary statistics.
agg_category = merged_data.groupby(['Category', 'period'], observed=True).agg(
    avg_spend=('spend', 'mean'),
    total_orders=('spend', 'count'),
    avg_quantity=('Quantity', 'mean')
).reset_index()

print("\nAggregated purchase data by Category and period (first 10 rows):")
print(agg_category.head(10))

# ---------------------------
# 10. Visualization: Category-Level Differences Pre, During, and Post Lockdown
# ---------------------------
plt.figure(figsize=(10, 5))
sns.barplot(x='Category', y='avg_spend', hue='period', data=agg_category)
plt.title("Average Spend by Product Category and Period")
plt.ylabel("Average Spend")
plt.xlabel("Product Category")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))
sns.barplot(x='Category', y='avg_quantity', hue='period', data=agg_category)
plt.title("Average Quantity Purchased by Product Category and Period")
plt.ylabel("Average Quantity")
plt.xlabel("Product Category")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# ---------------------------
# 11. (Optional) Save the Aggregated Data for Further Analysis
# ---------------------------
# agg_category.to_csv('agg_category_by_period.csv', index=False)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.formula.api as smf

# ---------------------------
# 1. Load Data (only needed columns)
# ---------------------------
purchase_cols = ['Order Date', 'Purchase Price Per Unit', 'Quantity',
                 'Shipping Address State', 'Category', 'Survey ResponseID']
survey_cols   = ['Survey ResponseID', 'Q-demos-age', 'Q-demos-gender']

# Adjust file paths as needed. Using low_memory=True to conserve RAM.
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv',
                            usecols=purchase_cols, low_memory=True)
survey_data   = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv',
                            usecols=survey_cols, low_memory=True)

print("Original Purchase data columns:", purchase_data.columns.tolist())
print("Original Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 2. Rename Columns to Remove Spaces and Special Characters
# ---------------------------
purchase_data.rename(columns={
    'Order Date': 'Order_Date',
    'Purchase Price Per Unit': 'Purchase_Price_Per_Unit',
    'Shipping Address State': 'Shipping_Address_State',
    'Survey ResponseID': 'Survey_ResponseID'
}, inplace=True)

survey_data.rename(columns={
    'Survey ResponseID': 'Survey_ResponseID',
    'Q-demos-age': 'Q_demos_age',
    'Q-demos-gender': 'Q_demos_gender'
}, inplace=True)

print("\nAfter renaming:")
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 3. (Optional) Sample Data if Necessary
# ---------------------------
# Uncomment the following line to work with a random sample (e.g., 100,000 rows) if the data is huge.
# purchase_data = purchase_data.sample(n=100000, random_state=42)

# Remove any duplicate survey responses.
survey_data = survey_data.drop_duplicates(subset=['Survey_ResponseID'])

# ---------------------------
# 4. Merge Data
# ---------------------------
merged_data = pd.merge(purchase_data, survey_data, on='Survey_ResponseID', how='inner')
print("\nMerged data (first 5 rows):")
print(merged_data.head())

# ---------------------------
# 5. Data Cleaning and Feature Creation
# ---------------------------
# Convert Order_Date to datetime and drop rows with invalid dates.
merged_data['Order_Date'] = pd.to_datetime(merged_data['Order_Date'], errors='coerce')
merged_data = merged_data[merged_data['Order_Date'].notnull()]

# Ensure numeric columns are proper types and downcast if possible.
merged_data['Purchase_Price_Per_Unit'] = pd.to_numeric(merged_data['Purchase_Price_Per_Unit'], errors='coerce', downcast='float')
merged_data['Quantity'] = pd.to_numeric(merged_data['Quantity'], errors='coerce', downcast='integer')

# Remove rows with nonpositive prices or quantities.
merged_data = merged_data[(merged_data['Purchase_Price_Per_Unit'] > 0) & (merged_data['Quantity'] > 0)]

# Create a spend variable per order and its logarithm.
merged_data['spend'] = merged_data['Purchase_Price_Per_Unit'] * merged_data['Quantity']
merged_data['log_spend'] = np.log(merged_data['spend'])

# ---------------------------
# 6. Incorporate Lockdown Dates by State
# ---------------------------
# For illustration, we create a small lockdown dataset. In practice, replace these dates with actual data.
lockdown_data = pd.DataFrame({
    'State': ['NJ', 'PA', 'CA', 'VA', 'SD'],
    'lockdown_start': ['2020-03-16', '2020-03-20', '2020-03-19', '2020-03-24', '2020-03-26']
})
lockdown_data['lockdown_start'] = pd.to_datetime(lockdown_data['lockdown_start'])

# Merge lockdown information onto the merged_data by state.
merged_data = pd.merge(merged_data, lockdown_data, left_on='Shipping_Address_State', right_on='State', how='left')

# Compute the number of days from the lockdown start date.
merged_data['days_from_lockdown'] = (merged_data['Order_Date'] - merged_data['lockdown_start']).dt.days

# ---------------------------
# 7. Define Periods: pre_lockdown, during_lockdown, and post_lockdown
# ---------------------------
# For example:
#   - pre_lockdown: days_from_lockdown < 0
#   - during_lockdown: 0 <= days_from_lockdown < 30
#   - post_lockdown: days_from_lockdown >= 30
merged_data['period'] = np.where(merged_data['days_from_lockdown'] < 0, 'pre_lockdown',
                          np.where(merged_data['days_from_lockdown'] < 30, 'during_lockdown', 'post_lockdown'))

# ---------------------------
# 8. Ensure Category is Categorical
# ---------------------------
merged_data['Category'] = merged_data['Category'].astype('category')

# ---------------------------
# 9. Aggregate Data by Product Category and Period
# ---------------------------
agg_category = merged_data.groupby(['Category', 'period'], observed=True).agg(
    avg_spend=('spend', 'mean'),
    total_orders=('spend', 'count'),
    avg_quantity=('Quantity', 'mean')
).reset_index()

print("\nAggregated purchase data by Category and period (first 10 rows):")
print(agg_category.head(10))

# ---------------------------
# 10. Visualization
# ---------------------------
plt.figure(figsize=(10, 5))
sns.barplot(x='Category', y='avg_spend', hue='period', data=agg_category)
plt.title("Average Spend by Product Category and Period")
plt.ylabel("Average Spend")
plt.xlabel("Product Category")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))
sns.barplot(x='Category', y='avg_quantity', hue='period', data=agg_category)
plt.title("Average Quantity Purchased by Product Category and Period")
plt.ylabel("Average Quantity")
plt.xlabel("Product Category")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# ---------------------------
# 11. Export Aggregated Output to CSV Files
# ---------------------------
# Option 1: Export the full aggregated data (all categories and periods combined)
agg_category.to_csv('agg_category_by_period.csv', index=False)
print("\nAggregated data for all product categories and periods has been saved to 'agg_category_by_period.csv'.")

# Option 2: Export separate CSV files for each period
agg_pre = agg_category[agg_category['period'] == 'pre_lockdown']
agg_during = agg_category[agg_category['period'] == 'during_lockdown']
agg_post = agg_category[agg_category['period'] == 'post_lockdown']

agg_pre.to_csv('agg_category_pre_lockdown.csv', index=False)
agg_during.to_csv('agg_category_during_lockdown.csv', index=False)
agg_post.to_csv('agg_category_post_lockdown.csv', index=False)

print("Separate CSV files have been saved:")
print(" - 'agg_category_pre_lockdown.csv'")
print(" - 'agg_category_during_lockdown.csv'")
print(" - 'agg_category_post_lockdown.csv'")

import pandas as pd
import numpy as np

# =============================================================================
# 1. Load Aggregated Data from CSV
# =============================================================================
# Replace with your actual file path.
csv_file = '/content/agg_category_by_period.csv'
# The CSV is expected to have at least these columns:
#   Category, period, avg_spend, total_orders, avg_quantity
agg_data = pd.read_csv(csv_file, low_memory=True)

print("Aggregated Data (first 5 rows):")
print(agg_data.head())
print("\nColumns:", agg_data.columns.tolist())

# (Optional) Ensure proper data types to reduce memory usage.
agg_data['Category'] = agg_data['Category'].astype(str)
agg_data['period'] = agg_data['period'].astype(str)
agg_data['avg_spend'] = pd.to_numeric(agg_data['avg_spend'], errors='coerce')
agg_data['total_orders'] = pd.to_numeric(agg_data['total_orders'], errors='coerce')
agg_data['avg_quantity'] = pd.to_numeric(agg_data['avg_quantity'], errors='coerce')

# =============================================================================
# 2. Pivot the Data by Category and Period
# =============================================================================
# We assume that the period variable has three values: "pre_lockdown", "during_lockdown", "post_lockdown"
# Pivot for average spend, total orders, and average quantity.
pivot_avg = agg_data.pivot(index='Category', columns='period', values='avg_spend')
pivot_orders = agg_data.pivot(index='Category', columns='period', values='total_orders')
pivot_qty = agg_data.pivot(index='Category', columns='period', values='avg_quantity')

# For verification, print the first few rows of the pivot tables.
print("\nPivot Table for avg_spend (first 5 rows):")
print(pivot_avg.head())
print("\nPivot Table for total_orders (first 5 rows):")
print(pivot_orders.head())

# =============================================================================
# 3. Compute Percent Changes Relative to Pre-Lockdown
# =============================================================================
def compute_pct_change(pivot_df):
    # Percent change = (value in period - pre_lockdown value) / pre_lockdown value
    pivot_df['pct_change_during'] = (pivot_df['during_lockdown'] - pivot_df['pre_lockdown']) / pivot_df['pre_lockdown']
    pivot_df['pct_change_post'] = (pivot_df['post_lockdown'] - pivot_df['pre_lockdown']) / pivot_df['pre_lockdown']
    return pivot_df

pivot_avg = compute_pct_change(pivot_avg)
pivot_orders = compute_pct_change(pivot_orders)
pivot_qty = compute_pct_change(pivot_qty)

# =============================================================================
# 4. Classify the Changes
# =============================================================================
def classify_change(x, threshold=0.10):
    """
    Classify the percent change:
      - If x > threshold: "increasing"
      - If x < -threshold: "decreasing"
      - Otherwise: "no change"
      - If x is NaN: "no data"
    """
    if pd.isnull(x):
        return 'no data'
    elif x > threshold:
        return 'increasing'
    elif x < -threshold:
        return 'decreasing'
    else:
        return 'no change'

pivot_avg['change_during'] = pivot_avg['pct_change_during'].apply(lambda x: classify_change(x, threshold=0.10))
pivot_avg['change_post']   = pivot_avg['pct_change_post'].apply(lambda x: classify_change(x, threshold=0.10))

pivot_orders['change_during'] = pivot_orders['pct_change_during'].apply(lambda x: classify_change(x, threshold=0.10))
pivot_orders['change_post']   = pivot_orders['pct_change_post'].apply(lambda x: classify_change(x, threshold=0.10))

pivot_qty['change_during'] = pivot_qty['pct_change_during'].apply(lambda x: classify_change(x, threshold=0.10))
pivot_qty['change_post']   = pivot_qty['pct_change_post'].apply(lambda x: classify_change(x, threshold=0.10))

# =============================================================================
# 5. Identify Categories with Very High Differences
# =============================================================================
# Define “very high” as more than a 50% change in avg_spend.
very_high_increase = pivot_avg[pivot_avg['pct_change_post'] > 0.50]
very_high_decrease = pivot_avg[pivot_avg['pct_change_post'] < -0.50]

print("\nCategories with VERY HIGH INCREASE in avg_spend (post vs. pre):")
print(very_high_increase[['pre_lockdown', 'post_lockdown', 'pct_change_post']])

print("\nCategories with VERY HIGH DECREASE in avg_spend (post vs. pre):")
print(very_high_decrease[['pre_lockdown', 'post_lockdown', 'pct_change_post']])

# =============================================================================
# 6. Summary Analysis: Count Classifications for All Categories
# =============================================================================
summary_avg = pivot_avg[['change_during', 'change_post']].apply(pd.value_counts)
summary_orders = pivot_orders[['change_during', 'change_post']].apply(pd.value_counts)
summary_qty = pivot_qty[['change_during', 'change_post']].apply(pd.value_counts)

print("\nSummary of Classification (avg_spend):")
print(summary_avg)
print("\nSummary of Classification (total_orders):")
print(summary_orders)
print("\nSummary of Classification (avg_quantity):")
print(summary_qty)

# =============================================================================
# 7. Export the Results to CSV Files
# =============================================================================
pivot_avg.to_csv('pivot_avg_spend_by_category.csv')
pivot_orders.to_csv('pivot_total_orders_by_category.csv')
pivot_qty.to_csv('pivot_avg_quantity_by_category.csv')

# Optionally, combine the three pivot tables into one DataFrame.
pivot_combined = pivot_avg.join(pivot_orders, lsuffix='_avg', rsuffix='_orders')
pivot_combined = pivot_combined.join(pivot_qty, rsuffix='_qty')
pivot_combined.to_csv('pivot_combined_by_category.csv')

# Also export the summary tables.
summary_avg.to_csv('summary_classification_avg_spend.csv')
summary_orders.to_csv('summary_classification_total_orders.csv')
summary_qty.to_csv('summary_classification_avg_quantity.csv')

print("\nExported the following CSV files:")
print("  - pivot_avg_spend_by_category.csv")
print("  - pivot_total_orders_by_category.csv")
print("  - pivot_avg_quantity_by_category.csv")
print("  - pivot_combined_by_category.csv")
print("  - summary_classification_avg_spend.csv")
print("  - summary_classification_total_orders.csv")
print("  - summary_classification_avg_quantity.csv")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# For clustering and scaling
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# =============================================================================
# 1. Load Aggregated Data
# =============================================================================
# This CSV should have one row per product category per period.
# Expected columns: Category, period, avg_spend, total_orders, avg_quantity
csv_file = '/content/agg_category_by_period.csv'  # Change this to your actual file path
agg_data = pd.read_csv(csv_file, low_memory=True)

print("Aggregated Data (first 5 rows):")
print(agg_data.head())
print("\nColumns:", agg_data.columns.tolist())

# =============================================================================
# 2. Pivot the Data to Wide Format
# =============================================================================
# We assume that the period column contains exactly these three values:
# "pre_lockdown", "during_lockdown", "post_lockdown"
# Pivot the metrics (avg_spend, total_orders, avg_quantity) so each row is a Category.
pivot_spend = agg_data.pivot(index='Category', columns='period', values='avg_spend')
pivot_orders = agg_data.pivot(index='Category', columns='period', values='total_orders')
pivot_qty = agg_data.pivot(index='Category', columns='period', values='avg_quantity')

# Optionally, rename the columns for clarity:
pivot_spend.columns = [col + '_spend' for col in pivot_spend.columns]
pivot_orders.columns = [col + '_orders' for col in pivot_orders.columns]
pivot_qty.columns = [col + '_qty' for col in pivot_qty.columns]

# Combine all metrics into one DataFrame.
combined = pivot_spend.join(pivot_orders).join(pivot_qty)

# =============================================================================
# 3. Compute Percent Changes (Relative to Pre–Lockdown)
# =============================================================================
# For each metric, compute percent change from pre_lockdown.
combined['pct_change_spend_during'] = (combined['during_lockdown_spend'] - combined['pre_lockdown_spend']) / combined['pre_lockdown_spend']
combined['pct_change_spend_post']   = (combined['post_lockdown_spend'] - combined['pre_lockdown_spend']) / combined['pre_lockdown_spend']

combined['pct_change_orders_during'] = (combined['during_lockdown_orders'] - combined['pre_lockdown_orders']) / combined['pre_lockdown_orders']
combined['pct_change_orders_post']   = (combined['post_lockdown_orders'] - combined['pre_lockdown_orders']) / combined['pre_lockdown_orders']

combined['pct_change_qty_during'] = (combined['during_lockdown_qty'] - combined['pre_lockdown_qty']) / combined['pre_lockdown_qty']
combined['pct_change_qty_post']   = (combined['post_lockdown_qty'] - combined['pre_lockdown_qty']) / combined['pre_lockdown_qty']

# =============================================================================
# 4. Select Features and Prepare for Clustering
# =============================================================================
features = combined[['pct_change_spend_during', 'pct_change_spend_post',
                       'pct_change_orders_during', 'pct_change_orders_post',
                       'pct_change_qty_during', 'pct_change_qty_post']].copy()

# Replace any missing values (e.g., if a category is missing one period) with 0.
features.fillna(0, inplace=True)

# Scale features (k-means is sensitive to scale)
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# =============================================================================
# 5. Cluster the Product Categories
# =============================================================================
# Let’s choose 4 clusters (this can be adjusted or determined via an elbow plot)
k = 4
kmeans = KMeans(n_clusters=k, random_state=42)
combined['cluster'] = kmeans.fit_predict(features_scaled)

# Examine cluster centers (in original percent-change units)
centers_scaled = kmeans.cluster_centers_
centers = scaler.inverse_transform(centers_scaled)
centers_df = pd.DataFrame(centers, columns=features.columns)
print("\nCluster Centers (original units):")
print(centers_df)

# =============================================================================
# 6. Map Clusters to Segmentation Labels
# =============================================================================
# Based on theory, we expect:
#   - Boom–Bust–Boom: Steep decline during lockdown then strong recovery post lockdown.
#   - Consistently Essential: Minimal drop and stable (or slight) growth.
#   - Luxury & Discretionary Spending: Sharp decline and slow recovery.
#   - New Post–Lockdown Trends: Little change during lockdown but a sustained increase post lockdown.
#
# The following mapping function is heuristic. You should adjust the thresholds based on
# the cluster centers you observe.
def map_cluster_to_label(row):
    # Using pct_change_spend as an example indicator.
    if row['pct_change_spend_during'] < -0.3 and row['pct_change_spend_post'] > 0.2:
        return 'Boom-Bust-Boom'
    elif abs(row['pct_change_spend_during']) < 0.1 and row['pct_change_spend_post'] > 0:
        return 'Consistently Essential'
    elif row['pct_change_spend_during'] < -0.3 and row['pct_change_spend_post'] < -0.1:
        return 'Luxury & Discretionary Spending'
    elif row['pct_change_spend_post'] > 0.2:
        return 'New Post-Lockdown Trends'
    else:
        return 'Other'

combined['segment'] = combined.apply(map_cluster_to_label, axis=1)

# Print segmentation summary
segmentation_summary = combined['segment'].value_counts()
print("\nSegmentation Summary:")
print(segmentation_summary)

# =============================================================================
# 7. Visualization of Segmentation (Example: Spend Changes)
# =============================================================================
plt.figure(figsize=(8, 6))
sns.scatterplot(data=combined,
                x='pct_change_spend_during',
                y='pct_change_spend_post',
                hue='segment',
                palette='Set2',
                s=100)
plt.title("Segmentation Based on Avg Spend Percent Changes")
plt.xlabel("Percent Change in Avg Spend (During vs. Pre)")
plt.ylabel("Percent Change in Avg Spend (Post vs. Pre)")
plt.legend(title="Segment")
plt.tight_layout()
plt.show()

# =============================================================================
# 8. Export the Segmentation Results
# =============================================================================
# Export the full segmentation results (one row per product category)
combined.to_csv('product_category_segmentation.csv')

# Also export the cluster centers and segmentation summary.
centers_df.to_csv('cluster_centers.csv', index=False)
segmentation_summary.to_csv('segmentation_summary.csv')

# =============================================================================
# 9. (Optional) Detailed Analysis of Each Segment
# =============================================================================
# For example, list the product categories in each segment:
segment_groups = combined.groupby('segment').apply(lambda df: df.index.tolist())
print("\nProduct Categories by Segment:")
print(segment_groups)

# Convert to DataFrame and export.
segment_groups_df = segment_groups.reset_index()
segment_groups_df.columns = ['segment', 'categories']
segment_groups_df.to_csv('categories_by_segment.csv', index=False)

print("\nExported CSV files:")
print(" - product_category_segmentation.csv")
print(" - cluster_centers.csv")
print(" - segmentation_summary.csv")
print(" - categories_by_segment.csv")



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ---------------------------
# 1. Load Data (only needed columns)
# ---------------------------
purchase_cols = ['Order Date', 'Purchase Price Per Unit', 'Quantity',
                 'Shipping Address State', 'Category', 'Survey ResponseID']
survey_cols   = ['Survey ResponseID', 'Q-demos-age', 'Q-demos-gender']

# Adjust file paths as needed. Using low_memory=True to conserve RAM.
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv',
                             usecols=purchase_cols, low_memory=True)
survey_data   = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv',
                             usecols=survey_cols, low_memory=True)

print("Original Purchase data columns:", purchase_data.columns.tolist())
print("Original Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 2. Rename Columns to Remove Spaces and Special Characters
# ---------------------------
purchase_data.rename(columns={
    'Order Date': 'Order_Date',
    'Purchase Price Per Unit': 'Purchase_Price_Per_Unit',
    'Shipping Address State': 'Shipping_Address_State',
    'Survey ResponseID': 'Survey_ResponseID'
}, inplace=True)

survey_data.rename(columns={
    'Survey ResponseID': 'Survey_ResponseID',
    'Q-demos-age': 'Q_demos_age',
    'Q-demos-gender': 'Q_demos_gender'
}, inplace=True)

print("\nAfter renaming:")
print("Purchase data columns:", purchase_data.columns.tolist())
print("Survey data columns:", survey_data.columns.tolist())

# ---------------------------
# 3. Remove Duplicate Survey Responses
# ---------------------------
survey_data = survey_data.drop_duplicates(subset=['Survey_ResponseID'])

# ---------------------------
# 4. Merge Purchase and Survey Data
# ---------------------------
merged_data = pd.merge(purchase_data, survey_data, on='Survey_ResponseID', how='inner')
print("\nMerged data (first 5 rows):")
print(merged_data.head())

# ---------------------------
# 5. Data Cleaning and Feature Creation
# ---------------------------
# Convert Order_Date to datetime and remove invalid rows.
merged_data['Order_Date'] = pd.to_datetime(merged_data['Order_Date'], errors='coerce')
merged_data = merged_data[merged_data['Order_Date'].notnull()]

# Convert price and quantity to numeric types and drop nonpositive entries.
merged_data['Purchase_Price_Per_Unit'] = pd.to_numeric(merged_data['Purchase_Price_Per_Unit'], errors='coerce', downcast='float')
merged_data['Quantity'] = pd.to_numeric(merged_data['Quantity'], errors='coerce', downcast='integer')
merged_data = merged_data[(merged_data['Purchase_Price_Per_Unit'] > 0) & (merged_data['Quantity'] > 0)]

# Compute spend per order and its logarithm.
merged_data['spend'] = merged_data['Purchase_Price_Per_Unit'] * merged_data['Quantity']
merged_data['log_spend'] = np.log(merged_data['spend'])

# ---------------------------
# 6. Incorporate Lockdown Dates by State
# ---------------------------
# Load the lockdown dates CSV file.
# The CSV file is assumed to have columns: state_id, start_date, end_date.
lockdown_data = pd.read_csv('/content/drive/MyDrive/amazon/Finalized_Disruption_Periods.csv', low_memory=True)
print("\nLockdown data (first 5 rows):")
print(lockdown_data.head())

# Convert start_date and end_date to datetime using day-month-year format.
lockdown_data['start_date'] = pd.to_datetime(lockdown_data['start_date'], format='%d-%m-%Y', errors='coerce')
lockdown_data['end_date'] = pd.to_datetime(lockdown_data['end_date'], format='%d-%m-%Y', errors='coerce')

# Merge lockdown information onto the merged_data by state.
# Note: 'Shipping_Address_State' in merged_data should match 'state_id' in lockdown_data.
merged_data = pd.merge(merged_data, lockdown_data, left_on='Shipping_Address_State', right_on='state_id', how='left')

# ---------------------------
# 7. Define Lockdown Periods for Each Order
# ---------------------------
# Here we define:
#   - pre_lockdown: Order_Date is before the state's lockdown start date.
#   - during_lockdown: Order_Date is between the state's lockdown start and end dates.
#   - post_lockdown: Order_Date is after the state's lockdown end date.
merged_data['period'] = np.where(
    merged_data['Order_Date'] < merged_data['start_date'], 'pre_lockdown',
    np.where(merged_data['Order_Date'] <= merged_data['end_date'], 'during_lockdown', 'post_lockdown')
)

# ---------------------------
# 8. Ensure Category is Categorical
# ---------------------------
merged_data['Category'] = merged_data['Category'].astype('category')

# ---------------------------
# 9. Aggregate Data by Product Category and Period
# ---------------------------
agg_category = merged_data.groupby(['Category', 'period'], observed=True).agg(
    avg_spend=('spend', 'mean'),
    total_orders=('spend', 'count'),
    avg_quantity=('Quantity', 'mean')
).reset_index()

print("\nAggregated purchase data by Category and period (first 10 rows):")
print(agg_category.head(10))

# ---------------------------
# 10. Visualization
# ---------------------------
plt.figure(figsize=(10, 5))
sns.barplot(x='Category', y='avg_spend', hue='period', data=agg_category)
plt.title("Average Spend by Product Category and Period")
plt.ylabel("Average Spend")
plt.xlabel("Product Category")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))
sns.barplot(x='Category', y='avg_quantity', hue='period', data=agg_category)
plt.title("Average Quantity Purchased by Product Category and Period")
plt.ylabel("Average Quantity")
plt.xlabel("Product Category")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# ---------------------------
# 11. Export Aggregated Output to CSV Files
# ---------------------------
# Export full aggregated data.
agg_category.to_csv('agg_category_by_period.csv', index=False)
print("\nAggregated data for all product categories and periods saved to 'agg_category_by_period.csv'.")

# Export separate CSV files for each period.
agg_pre = agg_category[agg_category['period'] == 'pre_lockdown']
agg_during = agg_category[agg_category['period'] == 'during_lockdown']
agg_post = agg_category[agg_category['period'] == 'post_lockdown']

agg_pre.to_csv('agg_category_pre_lockdown.csv', index=False)
agg_during.to_csv('agg_category_during_lockdown.csv', index=False)
agg_post.to_csv('agg_category_post_lockdown.csv', index=False)

print("Separate CSV files saved:")
print(" - 'agg_category_pre_lockdown.csv'")
print(" - 'agg_category_during_lockdown.csv'")
print(" - 'agg_category_post_lockdown.csv'")

import pandas as pd
import numpy as np

# =============================================================================
# 1. Load Aggregated Data from CSV
# =============================================================================
# Replace with your actual file path.
csv_file = '/content/agg_category_by_period.csv'
# The CSV is expected to have at least these columns:
#   Category, period, avg_spend, total_orders, avg_quantity
agg_data = pd.read_csv(csv_file, low_memory=True)

print("Aggregated Data (first 5 rows):")
print(agg_data.head())
print("\nColumns:", agg_data.columns.tolist())

# (Optional) Ensure proper data types to reduce memory usage.
agg_data['Category'] = agg_data['Category'].astype(str)
agg_data['period'] = agg_data['period'].astype(str)
agg_data['avg_spend'] = pd.to_numeric(agg_data['avg_spend'], errors='coerce')
agg_data['total_orders'] = pd.to_numeric(agg_data['total_orders'], errors='coerce')
agg_data['avg_quantity'] = pd.to_numeric(agg_data['avg_quantity'], errors='coerce')

# =============================================================================
# 2. Pivot the Data by Category and Period
# =============================================================================
# We assume that the period variable has three values: "pre_lockdown", "during_lockdown", "post_lockdown"
# Pivot for average spend, total orders, and average quantity.
pivot_avg = agg_data.pivot(index='Category', columns='period', values='avg_spend')
pivot_orders = agg_data.pivot(index='Category', columns='period', values='total_orders')
pivot_qty = agg_data.pivot(index='Category', columns='period', values='avg_quantity')

# For verification, print the first few rows of the pivot tables.
print("\nPivot Table for avg_spend (first 5 rows):")
print(pivot_avg.head())
print("\nPivot Table for total_orders (first 5 rows):")
print(pivot_orders.head())

# =============================================================================
# 3. Compute Percent Changes Relative to Pre-Lockdown
# =============================================================================
def compute_pct_change(pivot_df):
    # Percent change = (value in period - pre_lockdown value) / pre_lockdown value
    pivot_df['pct_change_during'] = (pivot_df['during_lockdown'] - pivot_df['pre_lockdown']) / pivot_df['pre_lockdown']
    pivot_df['pct_change_post'] = (pivot_df['post_lockdown'] - pivot_df['pre_lockdown']) / pivot_df['pre_lockdown']
    return pivot_df

pivot_avg = compute_pct_change(pivot_avg)
pivot_orders = compute_pct_change(pivot_orders)
pivot_qty = compute_pct_change(pivot_qty)

# =============================================================================
# 4. Classify the Changes
# =============================================================================
def classify_change(x, threshold=0.10):
    """
    Classify the percent change:
      - If x > threshold: "increasing"
      - If x < -threshold: "decreasing"
      - Otherwise: "no change"
      - If x is NaN: "no data"
    """
    if pd.isnull(x):
        return 'no data'
    elif x > threshold:
        return 'increasing'
    elif x < -threshold:
        return 'decreasing'
    else:
        return 'no change'

pivot_avg['change_during'] = pivot_avg['pct_change_during'].apply(lambda x: classify_change(x, threshold=0.10))
pivot_avg['change_post']   = pivot_avg['pct_change_post'].apply(lambda x: classify_change(x, threshold=0.10))

pivot_orders['change_during'] = pivot_orders['pct_change_during'].apply(lambda x: classify_change(x, threshold=0.10))
pivot_orders['change_post']   = pivot_orders['pct_change_post'].apply(lambda x: classify_change(x, threshold=0.10))

pivot_qty['change_during'] = pivot_qty['pct_change_during'].apply(lambda x: classify_change(x, threshold=0.10))
pivot_qty['change_post']   = pivot_qty['pct_change_post'].apply(lambda x: classify_change(x, threshold=0.10))

# =============================================================================
# 5. Identify Categories with Very High Differences
# =============================================================================
# Define “very high” as more than a 50% change in avg_spend.
very_high_increase = pivot_avg[pivot_avg['pct_change_post'] > 0.50]
very_high_decrease = pivot_avg[pivot_avg['pct_change_post'] < -0.50]

print("\nCategories with VERY HIGH INCREASE in avg_spend (post vs. pre):")
print(very_high_increase[['pre_lockdown', 'post_lockdown', 'pct_change_post']])

print("\nCategories with VERY HIGH DECREASE in avg_spend (post vs. pre):")
print(very_high_decrease[['pre_lockdown', 'post_lockdown', 'pct_change_post']])

# =============================================================================
# 6. Summary Analysis: Count Classifications for All Categories
# =============================================================================
summary_avg = pivot_avg[['change_during', 'change_post']].apply(pd.value_counts)
summary_orders = pivot_orders[['change_during', 'change_post']].apply(pd.value_counts)
summary_qty = pivot_qty[['change_during', 'change_post']].apply(pd.value_counts)

print("\nSummary of Classification (avg_spend):")
print(summary_avg)
print("\nSummary of Classification (total_orders):")
print(summary_orders)
print("\nSummary of Classification (avg_quantity):")
print(summary_qty)

# =============================================================================
# 7. Export the Results to CSV Files
# =============================================================================
pivot_avg.to_csv('pivot_avg_spend_by_category.csv')
pivot_orders.to_csv('pivot_total_orders_by_category.csv')
pivot_qty.to_csv('pivot_avg_quantity_by_category.csv')

# Optionally, combine the three pivot tables into one DataFrame.
pivot_combined = pivot_avg.join(pivot_orders, lsuffix='_avg', rsuffix='_orders')
pivot_combined = pivot_combined.join(pivot_qty, rsuffix='_qty')
pivot_combined.to_csv('pivot_combined_by_category.csv')

# Also export the summary tables.
summary_avg.to_csv('summary_classification_avg_spend.csv')
summary_orders.to_csv('summary_classification_total_orders.csv')
summary_qty.to_csv('summary_classification_avg_quantity.csv')

print("\nExported the following CSV files:")
print("  - pivot_avg_spend_by_category.csv")
print("  - pivot_total_orders_by_category.csv")
print("  - pivot_avg_quantity_by_category.csv")
print("  - pivot_combined_by_category.csv")
print("  - summary_classification_avg_spend.csv")
print("  - summary_classification_total_orders.csv")
print("  - summary_classification_avg_quantity.csv")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# For clustering and scaling
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# =============================================================================
# 1. Load Aggregated Data
# =============================================================================
# The CSV file is expected to be in wide format with columns such as:
#   - during_lockdown_avg, post_lockdown_avg, pre_lockdown_avg,
#   - pct_change_during_avg, pct_change_post_avg,
#   - during_lockdown_orders, post_lockdown_orders, pre_lockdown_orders,
#   - pct_change_during_orders, pct_change_post_orders,
#   - pct_change_during, pct_change_post, etc.
csv_file = '/content/pivot_combined_by_category.csv'  # Adjust this file path as needed
agg_data = pd.read_csv(csv_file, low_memory=True)

print("Aggregated Data (first 5 rows):")
print(agg_data.head())
print("\nColumns:", agg_data.columns.tolist())

# =============================================================================
# 2. Prepare the Data (No Pivot Needed)
# =============================================================================
# Since the data is already in wide format, set 'Category' as the index.
combined = agg_data.set_index('Category')

# (Optional) Check if all expected columns are present
expected_columns = [
    'during_lockdown_avg', 'post_lockdown_avg', 'pre_lockdown_avg',
    'pct_change_during_avg', 'pct_change_post_avg',
    'during_lockdown_orders', 'post_lockdown_orders', 'pre_lockdown_orders',
    'pct_change_during_orders', 'pct_change_post_orders',
    'pct_change_during', 'pct_change_post'
]
missing_cols = [col for col in expected_columns if col not in combined.columns]
if missing_cols:
    print("Warning: The following expected columns are missing:", missing_cols)

# =============================================================================
# 3. Select Features for Clustering
# =============================================================================
# Here we use:
#   - Spending changes: pct_change_during_avg and pct_change_post_avg
#   - Orders changes: pct_change_during_orders and pct_change_post_orders
#   - Overall changes (if applicable): pct_change_during and pct_change_post
features = combined[['pct_change_during_avg', 'pct_change_post_avg',
                     'pct_change_during_orders', 'pct_change_post_orders',
                     'pct_change_during', 'pct_change_post']].copy()

# Replace any missing values with 0.
features.fillna(0, inplace=True)

# =============================================================================
# 4. Scale Features and Perform Clustering
# =============================================================================
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Choose the number of clusters (e.g., 4)
k = 4
kmeans = KMeans(n_clusters=k, random_state=42)
combined['cluster'] = kmeans.fit_predict(features_scaled)

# Compute and print cluster centers (in original units)
centers_scaled = kmeans.cluster_centers_
centers = scaler.inverse_transform(centers_scaled)
centers_df = pd.DataFrame(centers, columns=features.columns)
print("\nCluster Centers (original units):")
print(centers_df)

# =============================================================================
# 5. Map Clusters to Segmentation Labels
# =============================================================================
# The heuristic below uses spending changes (the avg columns) to label the segments.
def map_cluster_to_label(row):
    if row['pct_change_during_avg'] < -0.3 and row['pct_change_post_avg'] > 0.2:
        return 'Boom-Bust-Boom'
    elif abs(row['pct_change_during_avg']) < 0.1 and row['pct_change_post_avg'] > 0:
        return 'Consistently Essential'
    elif row['pct_change_during_avg'] < -0.3 and row['pct_change_post_avg'] < -0.1:
        return 'Luxury & Discretionary Spending'
    elif row['pct_change_post_avg'] > 0.2:
        return 'New Post-Lockdown Trends'
    else:
        return 'Other'

combined['segment'] = combined.apply(map_cluster_to_label, axis=1)

# Print segmentation summary.
segmentation_summary = combined['segment'].value_counts()
print("\nSegmentation Summary:")
print(segmentation_summary)

# =============================================================================
# 6. Visualization of Segmentation (Example: Avg Spend Changes)
# =============================================================================
plt.figure(figsize=(8, 6))
sns.scatterplot(data=combined,
                x='pct_change_during_avg',
                y='pct_change_post_avg',
                hue='segment',
                palette='Set2',
                s=100)
plt.title("Segmentation Based on Avg Spend Percent Changes")
plt.xlabel("Percent Change in Avg Spend (During vs Pre)")
plt.ylabel("Percent Change in Avg Spend (Post vs Pre)")
plt.legend(title="Segment")
plt.tight_layout()
plt.show()

# =============================================================================
# 7. Additional Insights & Visualizations
# =============================================================================

# 7a. Distribution Plots for Percent Changes
fig, axes = plt.subplots(1, 2, figsize=(14, 6))
sns.histplot(combined['pct_change_post_avg'], bins=20, kde=True, ax=axes[0], color='skyblue')
axes[0].set_title("Distribution: Avg Spend Percent Change (Post vs Pre)")
axes[0].set_xlabel("Percent Change")

sns.histplot(combined['pct_change_post_orders'], bins=20, kde=True, ax=axes[1], color='salmon')
axes[1].set_title("Distribution: Orders Percent Change (Post vs Pre)")
axes[1].set_xlabel("Percent Change")

plt.tight_layout()
plt.show()

# 7b. Identify Main Categories Based on Spending Changes (Avg)
# Define thresholds (adjust as needed)
increase_threshold = 0.20   # More than a 20% increase
decrease_threshold = -0.10  # More than a 10% decrease

increasing = combined[combined['pct_change_post_avg'] > increase_threshold]
stable = combined[(combined['pct_change_post_avg'] >= decrease_threshold) &
                  (combined['pct_change_post_avg'] <= increase_threshold)]
decreasing = combined[combined['pct_change_post_avg'] < decrease_threshold]

print("\nNumber of categories increasing (Avg Spend):", increasing.shape[0])
print("Number of categories stable (Avg Spend):", stable.shape[0])
print("Number of categories decreasing (Avg Spend):", decreasing.shape[0])

# 7c. Bar Plots for Top 10 Increasing and Decreasing Categories
# Top 10 Increasing Categories
top_increasing = increasing.sort_values('pct_change_post_avg', ascending=False).head(10)
plt.figure(figsize=(10, 6))
sns.barplot(x=top_increasing.index, y=top_increasing['pct_change_post_avg'], palette='viridis')
plt.title("Top 10 Increasing Categories (Avg Spend)")
plt.xlabel("Category")
plt.ylabel("Percent Change in Avg Spend (Post vs Pre)")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Top 10 Decreasing Categories
top_decreasing = decreasing.sort_values('pct_change_post_avg').head(10)
plt.figure(figsize=(10, 6))
sns.barplot(x=top_decreasing.index, y=top_decreasing['pct_change_post_avg'], palette='magma')
plt.title("Top 10 Decreasing Categories (Avg Spend)")
plt.xlabel("Category")
plt.ylabel("Percent Change in Avg Spend (Post vs Pre)")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 7d. Correlation Heatmap of Percent Change Metrics
plt.figure(figsize=(8, 6))
corr_matrix = combined[['pct_change_during_avg', 'pct_change_post_avg',
                          'pct_change_during_orders', 'pct_change_post_orders',
                          'pct_change_during', 'pct_change_post']].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix of Percent Change Metrics")
plt.tight_layout()
plt.show()

# =============================================================================
# 8. Export the Segmentation Results & Additional Insights
# =============================================================================
combined.to_csv('product_category_segmentation.csv')
centers_df.to_csv('cluster_centers.csv', index=False)
segmentation_summary.to_csv('segmentation_summary.csv')

# Export product categories grouped by segment.
segment_groups = combined.groupby('segment').apply(lambda df: df.index.tolist())
segment_groups_df = segment_groups.reset_index()
segment_groups_df.columns = ['segment', 'categories']
segment_groups_df.to_csv('categories_by_segment.csv', index=False)

print("\nExported CSV files:")
print(" - product_category_segmentation.csv")
print(" - cluster_centers.csv")
print(" - segmentation_summary.csv")
print(" - categories_by_segment.csv")