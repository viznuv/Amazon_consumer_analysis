# -*- coding: utf-8 -*-
"""Analysis_amazon_part 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11u_2Eq3Idi3LqkATRcjL1rjcHFZCDzkE
"""

# %% [code]
# Install DoWhy if needed (uncomment the next line)
!pip install dowhy

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from sklearn.preprocessing import MultiLabelBinarizer
import dowhy
from dowhy import CausalModel

# ---------------------------
# Step 1: Load and Preprocess Data
# ---------------------------
# Load your purchase and survey datasets. Adjust file names and separators as needed.
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv', sep='\t')
survey_data = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv', sep='\t')

# Create life change dummy variables.
# Replace missing values with empty strings and split the life changes (which may be comma-separated).
survey_data['Q-life-changes'] = survey_data['Q-life-changes'].fillna('')
life_change_lists = survey_data['Q-life-changes'].apply(
    lambda x: [item.strip() for item in x.split(',') if item.strip() != '']
)

# Use MultiLabelBinarizer to create one-hot encoded (dummy) columns for each life change event.
mlb = MultiLabelBinarizer()
life_change_dummies = pd.DataFrame(mlb.fit_transform(life_change_lists),
                                   columns=mlb.classes_,
                                   index=survey_data.index)
# Append these dummies to the survey data.
survey_data = pd.concat([survey_data, life_change_dummies], axis=1)

# ---------------------------
# Step 2: Merge Purchase and Survey Data
# ---------------------------
# Merge on the common Survey ResponseID
df = pd.merge(purchase_data, survey_data, on='Survey ResponseID', how='left')

# Compute a total purchase value for each order.
df['total_purchase'] = df['Purchase Price Per Unit'] * df['Quantity']

# ---------------------------
# Step 3: Aggregate Data at the Customer Level
# ---------------------------
# Aggregate purchase-level data by Survey ResponseID
customer_agg = df.groupby('Survey ResponseID').agg({
    'total_purchase': 'sum',
    'Order Date': 'count'  # count the number of orders per customer
}).rename(columns={'Order Date': 'num_purchases'}).reset_index()

# Merge the aggregated purchase metrics with the survey (and life change dummy) data.
customer_df = pd.merge(survey_data, customer_agg, on='Survey ResponseID', how='left')
customer_df['total_purchase'] = customer_df['total_purchase'].fillna(0)
customer_df['num_purchases'] = customer_df['num_purchases'].fillna(0)

# Optionally, inspect the merged data
print("Customer-level data sample:")
print(customer_df.head())

# ---------------------------
# Step 4: Data Visualization
# ---------------------------
# Example visualization: Compare total purchase by whether a customer experienced "Lost a job"
plt.figure(figsize=(8,6))
sns.boxplot(x='Lost a job', y='total_purchase', data=customer_df)
plt.title("Total Purchase by 'Lost a job' Status")
plt.xlabel("'Lost a job' (0 = No, 1 = Yes)")
plt.ylabel("Total Purchase Value")
plt.show()

# Histogram of number of purchases for each group
plt.figure(figsize=(8,6))
sns.histplot(data=customer_df, x='num_purchases', hue='Lost a job', bins=20, kde=True)
plt.title("Distribution of Number of Purchases by 'Lost a job' Status")
plt.xlabel("Number of Purchases")
plt.ylabel("Frequency")
plt.show()

# You can repeat similar plots for other life change events as needed.

# ---------------------------
# Step 5: Causal Analysis Using DoWhy
# ---------------------------
# For a causal analysis example, we will examine the effect of the life event "Lost a job" on total_purchase.
# (This is just one example. You can similarly examine other life changes.)

# Note: Since demographic variables are provided as strings, we create dummy variables for them.
# Choose demographic variables you wish to control for.
demo_vars = ['Q-demos-age', 'Q-demos-income', 'Q-demos-education']
demo_dummies = pd.get_dummies(customer_df[demo_vars], drop_first=True)
# Append these dummy columns to customer_df so they are available for the causal model.
customer_df = pd.concat([customer_df, demo_dummies], axis=1)

# List the names of the created demographic dummy columns.
demo_dummy_cols = demo_dummies.columns.tolist()

# Define the treatment, outcome, and confounders.
treatment = 'Lost a job'  # binary column (0/1) created by MultiLabelBinarizer
outcome = 'total_purchase'
# In this example we control for the demographic dummies.
confounders = demo_dummy_cols

# Construct a causal model.
# The graph below is a simple representation.
model = CausalModel(
    data=customer_df,
    treatment=treatment,
    outcome=outcome,
    common_causes=confounders
)

# Visualize the assumed causal graph. (This will create a DOT file and render it if Graphviz is installed.)
model.view_model(layout="dot")
# To display the graph inline in Jupyter/Colab, you can use:
from IPython.display import Image, display
display(Image(filename="causal_model.png"))

# Identify the causal effect (this uses the back-door criterion).
identified_estimand = model.identify_effect()
print("\nIdentified estimand:")
print(identified_estimand)

# Estimate the causal effect using a back-door linear regression method.
estimate = model.estimate_effect(identified_estimand,
                                 method_name="backdoor.linear_regression")
print("\nCausal Effect Estimate (back-door linear regression):")
print(estimate.value)

# Refutation: Check the robustness of the estimate by adding a random common cause.
refute_results = model.refute_estimate(identified_estimand, estimate,
                                       method_name="random_common_cause")
print("\nRefutation Results:")
print(refute_results)

# Optionally, you can try additional refutation methods such as data subset refuters:
refute_subset = model.refute_estimate(identified_estimand, estimate,
                                      method_name="data_subset_refuter", subset_fraction=0.8)
print("\nData Subset Refuter Results:")
print(refute_subset)

# ---------------------------
# Step 6: Additional Visualizations of Regression Results (Optional)
# ---------------------------
# For example, you might plot the relationship between predicted and actual purchase amounts.
# Here we use the customer-level regression from before as an illustration.
X_total = customer_df[mlb.classes_.tolist()]
X_total = sm.add_constant(X_total)
y_total = customer_df['total_purchase']
model_total = sm.OLS(y_total, X_total, missing='drop').fit()
print("\nCustomer-Level Regression (Total Purchase Value) Summary:")
print(model_total.summary())

# Plot actual vs predicted values.
customer_df['pred_total'] = model_total.predict(X_total)
plt.figure(figsize=(8,6))
sns.scatterplot(x='pred_total', y='total_purchase', data=customer_df)
plt.plot([customer_df['pred_total'].min(), customer_df['pred_total'].max()],
         [customer_df['pred_total'].min(), customer_df['pred_total'].max()], color='red', lw=2)
plt.xlabel("Predicted Total Purchase")
plt.ylabel("Actual Total Purchase")
plt.title("Actual vs. Predicted Total Purchase")
plt.show()

# Uncomment the following line if DoWhy is not already installed:
# !pip install dowhy

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from sklearn.preprocessing import MultiLabelBinarizer
import dowhy
from dowhy import CausalModel
from IPython.display import Image, display

# =============================
# Step 1: Load the Data
# =============================

# Purchase data is tab-delimited.
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv', sep='\t')

# Survey data is comma-separated.
survey_data = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')  # Default delimiter is comma

# -----------------------------
# Debug: Inspect Survey Data Columns
# -----------------------------
print("Survey Data Columns (Original):")
print(survey_data.columns.tolist())

# Clean column names by stripping any extra whitespace.
survey_data.columns = survey_data.columns.str.strip()
print("Survey Data Columns (Cleaned):")
print(survey_data.columns.tolist())

# =============================
# Step 2: Process the Survey Data – Create Life Change Dummies
# =============================

# Ensure the "Q-life-changes" column exists and fill missing values.
survey_data['Q-life-changes'] = survey_data['Q-life-changes'].fillna('')

# Split the "Q-life-changes" string into a list for each respondent.
# (Assumes the events are comma-separated.)
life_change_lists = survey_data['Q-life-changes'].apply(
    lambda x: [item.strip() for item in x.split(',') if item.strip() != '']
)

# Use MultiLabelBinarizer to create a dummy (one-hot encoded) column for each unique life change.
mlb = MultiLabelBinarizer()
life_change_dummies = pd.DataFrame(mlb.fit_transform(life_change_lists),
                                   columns=mlb.classes_,
                                   index=survey_data.index)

# Concatenate the dummy columns with the original survey DataFrame.
survey_data = pd.concat([survey_data, life_change_dummies], axis=1)

# =============================
# Step 3: Merge Purchase Data with Survey Data
# =============================

# Merge the purchase and survey data on the common "Survey ResponseID".
df = pd.merge(purchase_data, survey_data, on='Survey ResponseID', how='left')

# Compute total purchase value for each order.
df['total_purchase'] = df['Purchase Price Per Unit'] * df['Quantity']

# =============================
# Step 4: Aggregate Data at the Customer Level
# =============================

# Aggregate purchase-level data by "Survey ResponseID" to get:
#   - Total purchase value.
#   - Number of purchases.
customer_agg = df.groupby('Survey ResponseID').agg({
    'total_purchase': 'sum',
    'Order Date': 'count'  # Counting the orders per customer.
}).rename(columns={'Order Date': 'num_purchases'}).reset_index()

# Merge these aggregated metrics with the survey data (which now includes life-change dummies).
customer_df = pd.merge(survey_data, customer_agg, on='Survey ResponseID', how='left')
customer_df['total_purchase'] = customer_df['total_purchase'].fillna(0)
customer_df['num_purchases'] = customer_df['num_purchases'].fillna(0)

# =============================
# Step 5: Data Visualization
# =============================

# Example Visualization 1:
# Boxplot of Total Purchase Value by "Lost a job" status.
plt.figure(figsize=(8,6))
sns.boxplot(x='Lost a job', y='total_purchase', data=customer_df)
plt.title("Total Purchase Value by 'Lost a job' Status")
plt.xlabel("'Lost a job' (0 = No, 1 = Yes)")
plt.ylabel("Total Purchase Value")
plt.show()

# Example Visualization 2:
# Histogram of Number of Purchases by "Lost a job" status.
plt.figure(figsize=(8,6))
sns.histplot(data=customer_df, x='num_purchases', hue='Lost a job', bins=20, kde=True)
plt.title("Distribution of Number of Purchases by 'Lost a job' Status")
plt.xlabel("Number of Purchases")
plt.ylabel("Frequency")
plt.show()

# =============================
# Step 6: Causal Analysis Using DoWhy
# =============================

# For the causal analysis, we will assess the effect of the life event "Lost a job"
# on the total purchase value (total_purchase). We also control for a few demographic variables.
# (For example, we'll control for "Q-demos-age", "Q-demos-income", and "Q-demos-education".)

# Select the demographic variables and create dummy variables for categorical features.
demo_vars = ['Q-demos-age', 'Q-demos-income', 'Q-demos-education']
demo_dummies = pd.get_dummies(customer_df[demo_vars], drop_first=True)

# Concatenate these demographic dummy variables to the customer-level DataFrame.
customer_df = pd.concat([customer_df, demo_dummies], axis=1)
demo_dummy_cols = demo_dummies.columns.tolist()  # List of demographic confounders

# Define treatment, outcome, and confounders.
treatment = 'Lost a job'  # This column is created from the Q-life-changes processing.
outcome = 'total_purchase'
confounders = demo_dummy_cols  # Demographic controls

# Construct the causal model using DoWhy.
model = CausalModel(
    data=customer_df,
    treatment=treatment,
    outcome=outcome,
    common_causes=confounders
)

# Visualize the causal graph.
model.view_model(layout="dot")
# The causal graph is saved as "causal_model.png". To display it inline:
display(Image(filename="causal_model.png"))

# Identify the causal effect using the back-door criterion.
identified_estimand = model.identify_effect()
print("\nIdentified Estimand:")
print(identified_estimand)

# Estimate the causal effect using a backdoor linear regression method.
estimate = model.estimate_effect(identified_estimand,
                                 method_name="backdoor.linear_regression")
print("\nCausal Effect Estimate (back-door linear regression):")
print(estimate.value)

# Run refutation tests to check the robustness of the estimate.
refute_results = model.refute_estimate(identified_estimand, estimate,
                                       method_name="random_common_cause")
print("\nRandom Common Cause Refutation:")
print(refute_results)

refute_subset = model.refute_estimate(identified_estimand, estimate,
                                      method_name="data_subset_refuter", subset_fraction=0.8)
print("\nData Subset Refuter:")
print(refute_subset)

# =============================
# Step 7: Additional Regression & Visualization (Optional)
# =============================

# Run a customer-level regression on total_purchase using all the life-change dummies.
X_total = customer_df[mlb.classes_.tolist()]
X_total = sm.add_constant(X_total)  # Add an intercept term.
y_total = customer_df['total_purchase']

model_total = sm.OLS(y_total, X_total, missing='drop').fit()
print("\nCustomer-Level Regression (Total Purchase Value) Summary:")
print(model_total.summary())

# Plot actual vs. predicted total_purchase values.
customer_df['pred_total'] = model_total.predict(X_total)
plt.figure(figsize=(8,6))
sns.scatterplot(x='pred_total', y='total_purchase', data=customer_df)
plt.plot([customer_df['pred_total'].min(), customer_df['pred_total'].max()],
         [customer_df['pred_total'].min(), customer_df['pred_total'].max()],
         color='red', lw=2)
plt.xlabel("Predicted Total Purchase")
plt.ylabel("Actual Total Purchase")
plt.title("Actual vs. Predicted Total Purchase")
plt.show()

# Uncomment the following line if DoWhy is not already installed:
# !pip install dowhy

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from sklearn.preprocessing import MultiLabelBinarizer
import dowhy
from dowhy import CausalModel
from IPython.display import Image, display

# =============================
# Step 1: Load the Data
# =============================

# --- Load Purchase Data ---
# If the purchase file is tab-separated:
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv', sep='\t')

# Debug: Inspect and clean purchase_data columns
print("Purchase Data Columns (Original):")
print(purchase_data.columns.tolist())
purchase_data.columns = purchase_data.columns.str.strip()
print("Purchase Data Columns (Cleaned):")
print(purchase_data.columns.tolist())

# If you do not see "Survey ResponseID" in the output,
# inspect the header of the file or try a different delimiter.
if 'Survey ResponseID' not in purchase_data.columns:
    print("Column 'Survey ResponseID' not found in purchase_data. Available columns:")
    print(purchase_data.columns.tolist())
    # Optionally, adjust the delimiter if needed:
    # purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv', sep=',')
    # purchase_data.columns = purchase_data.columns.str.strip()
    # print("Adjusted Purchase Data Columns:")
    # print(purchase_data.columns.tolist())

# --- Load Survey Data ---
# The survey file is comma-separated.
survey_data = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')

# Debug: Inspect and clean survey_data columns
print("Survey Data Columns (Original):")
print(survey_data.columns.tolist())
survey_data.columns = survey_data.columns.str.strip()
print("Survey Data Columns (Cleaned):")
print(survey_data.columns.tolist())

# =============================
# Step 2: Process the Survey Data – Create Life Change Dummies
# =============================

# Ensure the "Q-life-changes" column exists and fill missing values.
survey_data['Q-life-changes'] = survey_data['Q-life-changes'].fillna('')

# Split the "Q-life-changes" string into a list for each respondent.
# (Assumes the events are comma-separated.)
life_change_lists = survey_data['Q-life-changes'].apply(
    lambda x: [item.strip() for item in x.split(',') if item.strip() != '']
)

# Use MultiLabelBinarizer to create dummy (one-hot encoded) columns for each unique life change.
mlb = MultiLabelBinarizer()
life_change_dummies = pd.DataFrame(mlb.fit_transform(life_change_lists),
                                   columns=mlb.classes_,
                                   index=survey_data.index)

# Concatenate the dummy columns with the original survey DataFrame.
survey_data = pd.concat([survey_data, life_change_dummies], axis=1)

# =============================
# Step 3: Merge Purchase Data with Survey Data
# =============================

# Check if both DataFrames have the key column before merging.
if 'Survey ResponseID' not in purchase_data.columns:
    raise KeyError("Column 'Survey ResponseID' is missing from purchase_data. Please check your file.")
if 'Survey ResponseID' not in survey_data.columns:
    raise KeyError("Column 'Survey ResponseID' is missing from survey_data. Please check your file.")

# Merge on the common "Survey ResponseID".
df = pd.merge(purchase_data, survey_data, on='Survey ResponseID', how='left')

# Compute the total purchase value for each order.
df['total_purchase'] = df['Purchase Price Per Unit'] * df['Quantity']

# =============================
# Step 4: Aggregate Data at the Customer Level
# =============================

# Aggregate purchase-level data by "Survey ResponseID" to get:
#   - Sum of total_purchase,
#   - Count of orders (num_purchases).
customer_agg = df.groupby('Survey ResponseID').agg({
    'total_purchase': 'sum',
    'Order Date': 'count'  # Counting the orders per customer.
}).rename(columns={'Order Date': 'num_purchases'}).reset_index()

# Merge these aggregated metrics with the survey data (which now includes life-change dummies).
customer_df = pd.merge(survey_data, customer_agg, on='Survey ResponseID', how='left')
customer_df['total_purchase'] = customer_df['total_purchase'].fillna(0)
customer_df['num_purchases'] = customer_df['num_purchases'].fillna(0)

# =============================
# Step 5: Data Visualization
# =============================

# Example Visualization 1:
# Boxplot of Total Purchase Value by "Lost a job" status.
plt.figure(figsize=(8,6))
sns.boxplot(x='Lost a job', y='total_purchase', data=customer_df)
plt.title("Total Purchase Value by 'Lost a job' Status")
plt.xlabel("'Lost a job' (0 = No, 1 = Yes)")
plt.ylabel("Total Purchase Value")
plt.show()

# Example Visualization 2:
# Histogram of Number of Purchases by "Lost a job" status.
plt.figure(figsize=(8,6))
sns.histplot(data=customer_df, x='num_purchases', hue='Lost a job', bins=20, kde=True)
plt.title("Distribution of Number of Purchases by 'Lost a job' Status")
plt.xlabel("Number of Purchases")
plt.ylabel("Frequency")
plt.show()

# =============================
# Step 6: Causal Analysis Using DoWhy
# =============================

# For the causal analysis, we will assess the effect of the life event "Lost a job"
# on the total purchase value (total_purchase). We control for a few demographic variables.
# (For example: "Q-demos-age", "Q-demos-income", and "Q-demos-education".)

# Select demographic variables and create dummy variables for categorical features.
demo_vars = ['Q-demos-age', 'Q-demos-income', 'Q-demos-education']
demo_dummies = pd.get_dummies(customer_df[demo_vars], drop_first=True)

# Concatenate these demographic dummy variables to the customer-level DataFrame.
customer_df = pd.concat([customer_df, demo_dummies], axis=1)
demo_dummy_cols = demo_dummies.columns.tolist()  # List of demographic confounders

# Define treatment, outcome, and confounders.
treatment = 'Lost a job'  # This column is created from the Q-life-changes processing.
outcome = 'total_purchase'
confounders = demo_dummy_cols  # Demographic controls

# Construct the causal model using DoWhy.
model = CausalModel(
    data=customer_df,
    treatment=treatment,
    outcome=outcome,
    common_causes=confounders
)

# Visualize the causal graph.
model.view_model(layout="dot")
# The causal graph is saved as "causal_model.png". To display it inline:
display(Image(filename="causal_model.png"))

# Identify the causal effect using the back-door criterion.
identified_estimand = model.identify_effect()
print("\nIdentified Estimand:")
print(identified_estimand)

# Estimate the causal effect using backdoor linear regression.
estimate = model.estimate_effect(identified_estimand,
                                 method_name="backdoor.linear_regression")
print("\nCausal Effect Estimate (back-door linear regression):")
print(estimate.value)

# Run refutation tests to check the robustness of the causal estimate.
refute_results = model.refute_estimate(identified_estimand, estimate,
                                       method_name="random_common_cause")
print("\nRandom Common Cause Refutation:")
print(refute_results)

refute_subset = model.refute_estimate(identified_estimand, estimate,
                                      method_name="data_subset_refuter", subset_fraction=0.8)
print("\nData Subset Refuter:")
print(refute_subset)

# =============================
# Step 7: Additional Regression & Visualization (Optional)
# =============================

# Run a customer-level regression on total_purchase using all the life-change dummies.
X_total = customer_df[mlb.classes_.tolist()]
X_total = sm.add_constant(X_total)  # Add an intercept term.
y_total = customer_df['total_purchase']

model_total = sm.OLS(y_total, X_total, missing='drop').fit()
print("\nCustomer-Level Regression (Total Purchase Value) Summary:")
print(model_total.summary())

# Plot actual vs. predicted total_purchase values.
customer_df['pred_total'] = model_total.predict(X_total)
plt.figure(figsize=(8,6))
sns.scatterplot(x='pred_total', y='total_purchase', data=customer_df)
plt.plot([customer_df['pred_total'].min(), customer_df['pred_total'].max()],
         [customer_df['pred_total'].min(), customer_df['pred_total'].max()],
         color='red', lw=2)
plt.xlabel("Predicted Total Purchase")
plt.ylabel("Actual Total Purchase")
plt.title("Actual vs. Predicted Total Purchase")
plt.show()

# Uncomment the following line if DoWhy is not installed:
# !pip install dowhy

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from sklearn.preprocessing import MultiLabelBinarizer
import dowhy
from dowhy import CausalModel
from IPython.display import Image, display

# =============================
# Step 1: Load the Data
# =============================

# --- Load Purchase Data ---
# Since the file is comma-separated, use the default delimiter.
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv')

# Debug: Inspect and clean purchase_data columns
print("Purchase Data Columns (Original):")
print(purchase_data.columns.tolist())
purchase_data.columns = purchase_data.columns.str.strip()  # Remove any extra whitespace
print("Purchase Data Columns (Cleaned):")
print(purchase_data.columns.tolist())

# --- Load Survey Data ---
# The survey file is comma-separated.
survey_data = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')

# Debug: Inspect and clean survey_data columns
print("Survey Data Columns (Original):")
print(survey_data.columns.tolist())
survey_data.columns = survey_data.columns.str.strip()
print("Survey Data Columns (Cleaned):")
print(survey_data.columns.tolist())

# =============================
# Step 2: Process the Survey Data – Create Life Change Dummies
# =============================

# Ensure the "Q-life-changes" column exists and fill missing values.
survey_data['Q-life-changes'] = survey_data['Q-life-changes'].fillna('')

# Split the "Q-life-changes" string into a list for each respondent.
# (Assumes the events are comma-separated.)
life_change_lists = survey_data['Q-life-changes'].apply(
    lambda x: [item.strip() for item in x.split(',') if item.strip() != '']
)

# Use MultiLabelBinarizer to create dummy (one-hot encoded) columns for each unique life change.
mlb = MultiLabelBinarizer()
life_change_dummies = pd.DataFrame(mlb.fit_transform(life_change_lists),
                                   columns=mlb.classes_,
                                   index=survey_data.index)

# Concatenate the dummy columns with the original survey DataFrame.
survey_data = pd.concat([survey_data, life_change_dummies], axis=1)

# =============================
# Step 3: Merge Purchase Data with Survey Data
# =============================

# Check if both DataFrames have the key column before merging.
if 'Survey ResponseID' not in purchase_data.columns:
    raise KeyError("Column 'Survey ResponseID' is missing from purchase_data. Please check your file.")
if 'Survey ResponseID' not in survey_data.columns:
    raise KeyError("Column 'Survey ResponseID' is missing from survey_data. Please check your file.")

# Merge on the common "Survey ResponseID".
df = pd.merge(purchase_data, survey_data, on='Survey ResponseID', how='left')

# Compute the total purchase value for each order.
df['total_purchase'] = df['Purchase Price Per Unit'] * df['Quantity']

# =============================
# Step 4: Aggregate Data at the Customer Level
# =============================

# Aggregate purchase-level data by "Survey ResponseID" to get:
#   - Sum of total_purchase,
#   - Count of orders (num_purchases).
customer_agg = df.groupby('Survey ResponseID').agg({
    'total_purchase': 'sum',
    'Order Date': 'count'  # Counting the orders per customer.
}).rename(columns={'Order Date': 'num_purchases'}).reset_index()

# Merge these aggregated metrics with the survey data (which now includes life-change dummies).
customer_df = pd.merge(survey_data, customer_agg, on='Survey ResponseID', how='left')
customer_df['total_purchase'] = customer_df['total_purchase'].fillna(0)
customer_df['num_purchases'] = customer_df['num_purchases'].fillna(0)

# =============================
# Step 5: Data Visualization
# =============================

# Example Visualization 1:
# Boxplot of Total Purchase Value by "Lost a job" status.
plt.figure(figsize=(8,6))
sns.boxplot(x='Lost a job', y='total_purchase', data=customer_df)
plt.title("Total Purchase Value by 'Lost a job' Status")
plt.xlabel("'Lost a job' (0 = No, 1 = Yes)")
plt.ylabel("Total Purchase Value")
plt.show()

# Example Visualization 2:
# Histogram of Number of Purchases by "Lost a job" status.
plt.figure(figsize=(8,6))
sns.histplot(data=customer_df, x='num_purchases', hue='Lost a job', bins=20, kde=True)
plt.title("Distribution of Number of Purchases by 'Lost a job' Status")
plt.xlabel("Number of Purchases")
plt.ylabel("Frequency")
plt.show()

# =============================
# Step 6: Causal Analysis Using DoWhy
# =============================

# For the causal analysis, we will assess the effect of the life event "Lost a job"
# on the total purchase value (total_purchase). We control for a few demographic variables.
# (For example: "Q-demos-age", "Q-demos-income", and "Q-demos-education".)

# Select demographic variables and create dummy variables for categorical features.
demo_vars = ['Q-demos-age', 'Q-demos-income', 'Q-demos-education']
demo_dummies = pd.get_dummies(customer_df[demo_vars], drop_first=True)

# Concatenate these demographic dummy variables to the customer-level DataFrame.
customer_df = pd.concat([customer_df, demo_dummies], axis=1)
demo_dummy_cols = demo_dummies.columns.tolist()  # List of demographic confounders

# Define treatment, outcome, and confounders.
treatment = 'Lost a job'  # This column is created from the Q-life-changes processing.
outcome = 'total_purchase'
confounders = demo_dummy_cols  # Demographic controls

# Construct the causal model using DoWhy.
model = CausalModel(
    data=customer_df,
    treatment=treatment,
    outcome=outcome,
    common_causes=confounders
)

# Visualize the causal graph.
model.view_model(layout="dot")
# The causal graph is saved as "causal_model.png". To display it inline:
display(Image(filename="causal_model.png"))

# Identify the causal effect using the back-door criterion.
identified_estimand = model.identify_effect()
print("\nIdentified Estimand:")
print(identified_estimand)

# Estimate the causal effect using backdoor linear regression.
estimate = model.estimate_effect(identified_estimand,
                                 method_name="backdoor.linear_regression")
print("\nCausal Effect Estimate (back-door linear regression):")
print(estimate.value)

# Run refutation tests to check the robustness of the causal estimate.
refute_results = model.refute_estimate(identified_estimand, estimate,
                                       method_name="random_common_cause")
print("\nRandom Common Cause Refutation:")
print(refute_results)

refute_subset = model.refute_estimate(identified_estimand, estimate,
                                      method_name="data_subset_refuter", subset_fraction=0.8)
print("\nData Subset Refuter:")
print(refute_subset)

# =============================
# Step 7: Additional Regression & Visualization (Optional)
# =============================

# Run a customer-level regression on total_purchase using all the life-change dummies.
X_total = customer_df[mlb.classes_.tolist()]
X_total = sm.add_constant(X_total)  # Add an intercept term.
y_total = customer_df['total_purchase']

model_total = sm.OLS(y_total, X_total, missing='drop').fit()
print("\nCustomer-Level Regression (Total Purchase Value) Summary:")
print(model_total.summary())

# Plot actual vs. predicted total_purchase values.
customer_df['pred_total'] = model_total.predict(X_total)
plt.figure(figsize=(8,6))
sns.scatterplot(x='pred_total', y='total_purchase', data=customer_df)
plt.plot([customer_df['pred_total'].min(), customer_df['pred_total'].max()],
         [customer_df['pred_total'].min(), customer_df['pred_total'].max()],
         color='red', lw=2)
plt.xlabel("Predicted Total Purchase")
plt.ylabel("Actual Total Purchase")
plt.title("Actual vs. Predicted Total Purchase")
plt.show()

# Uncomment these lines if needed:
!pip install dowhy
!pip install scikit-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import dowhy
from dowhy import CausalModel
from IPython.display import Image, display

# Machine learning imports:
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, classification_report
from sklearn.preprocessing import LabelEncoder

##############################################
# STEP 1: Data Preparation and Aggregation
##############################################

# --- Load Purchase Data (comma-separated) ---
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv')
purchase_data.columns = purchase_data.columns.str.strip()  # clean columns

# --- Load Survey Data (comma-separated) ---
survey_data = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')
survey_data.columns = survey_data.columns.str.strip()

# --- Inspect columns for debugging ---
print("Purchase Data Columns:")
print(purchase_data.columns.tolist())
print("Survey Data Columns:")
print(survey_data.columns.tolist())

# --- Process Survey Data: Create Life Change Dummies ---
# (Assumes that the survey file has a column "Q-life-changes")
survey_data['Q-life-changes'] = survey_data['Q-life-changes'].fillna('')

# Split comma-separated life changes into a list per respondent
life_change_lists = survey_data['Q-life-changes'].apply(
    lambda x: [item.strip() for item in x.split(',') if item.strip() != '']
)

# Use MultiLabelBinarizer to one-hot encode the life change events
from sklearn.preprocessing import MultiLabelBinarizer
mlb = MultiLabelBinarizer()
life_change_dummies = pd.DataFrame(mlb.fit_transform(life_change_lists),
                                   columns=mlb.classes_,
                                   index=survey_data.index)
survey_data = pd.concat([survey_data, life_change_dummies], axis=1)

# --- Merge Data on "Survey ResponseID" ---
# (Make sure both DataFrames have the key "Survey ResponseID")
if 'Survey ResponseID' not in purchase_data.columns:
    raise KeyError("Column 'Survey ResponseID' is missing from purchase_data.")
if 'Survey ResponseID' not in survey_data.columns:
    raise KeyError("Column 'Survey ResponseID' is missing from survey_data.")

df = pd.merge(purchase_data, survey_data, on='Survey ResponseID', how='left')

# Compute a new variable for each order: total purchase value
df['total_purchase'] = df['Purchase Price Per Unit'] * df['Quantity']

# --- Aggregate at Customer Level ---
# We compute total spending and number of purchases per customer.
customer_agg = df.groupby('Survey ResponseID').agg({
    'total_purchase': 'sum',
    'Order Date': 'count'  # number of orders per customer
}).rename(columns={'Order Date': 'num_purchases'}).reset_index()

# Merge these metrics back with survey data
customer_df = pd.merge(survey_data, customer_agg, on='Survey ResponseID', how='left')
customer_df['total_purchase'] = customer_df['total_purchase'].fillna(0)
customer_df['num_purchases'] = customer_df['num_purchases'].fillna(0)

##############################################
# STEP 2: Define "Compulsive Purchase" Outcome
##############################################

# We define "compulsive_purchase" as those customers in the top 10% of number of purchases.
threshold = customer_df['num_purchases'].quantile(0.9)
customer_df['compulsive_purchase'] = (customer_df['num_purchases'] >= threshold).astype(int)

print("Threshold for compulsive purchase (top 10% of num_purchases):", threshold)
print(customer_df[['Survey ResponseID','num_purchases','compulsive_purchase']].head())

##############################################
# STEP 3: Visualization & Exploratory Analysis
##############################################

# Histogram: Distribution of number of purchases with threshold line
plt.figure(figsize=(8,6))
sns.histplot(customer_df['num_purchases'], bins=30, kde=True)
plt.axvline(threshold, color='red', linestyle='--', label='90th Percentile')
plt.title("Distribution of Number of Purchases")
plt.xlabel("Number of Purchases")
plt.ylabel("Count")
plt.legend()
plt.show()

# Boxplot: Total purchase by compulsive purchase status
plt.figure(figsize=(8,6))
sns.boxplot(x='compulsive_purchase', y='total_purchase', data=customer_df)
plt.title("Total Purchase Value by Compulsive Purchase Status")
plt.xlabel("Compulsive Purchase (0 = No, 1 = Yes)")
plt.ylabel("Total Purchase Value")
plt.show()

##############################################
# STEP 4: Regression Analysis (Logistic Regression)
##############################################

# We will use demographic variables to predict compulsive purchase.
# For this example, we use: Q-demos-age, Q-demos-income, Q-demos-education, Q-demos-gender
# (Convert categorical variables to dummies.)

demo_vars = ['Q-demos-age', 'Q-demos-income', 'Q-demos-education', 'Q-demos-gender']
demo_data = customer_df[demo_vars].copy()
demo_dummies = pd.get_dummies(demo_data, drop_first=True)
X = demo_dummies
y = customer_df['compulsive_purchase']

# Add a constant for statsmodels
X_const = sm.add_constant(X)
logit_model = sm.Logit(y, X_const).fit(disp=False)
print("\nLogistic Regression Summary:")
print(logit_model.summary())

# Get predicted probabilities and plot ROC curve.
pred_probs = logit_model.predict(X_const)
from sklearn.metrics import roc_curve, auc
fpr, tpr, thresholds = roc_curve(y, pred_probs)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:0.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Logistic Regression')
plt.legend(loc="lower right")
plt.show()

##############################################
# STEP 5: Machine Learning Approach (Random Forest)
##############################################

# Prepare data for classification using RandomForestClassifier.
# We will use the same demo variables.
X_ml = demo_dummies.copy()
y_ml = customer_df['compulsive_purchase']

# Split data into training and testing sets.
X_train, X_test, y_train, y_test = train_test_split(X_ml, y_ml, test_size=0.3, random_state=42, stratify=y_ml)

# Train RandomForestClassifier.
rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
roc_auc_ml = roc_auc_score(y_test, rf.predict_proba(X_test)[:,1])

print("\nRandom Forest Classifier Results:")
print("Accuracy: {:.2f}%".format(accuracy * 100))
print("ROC-AUC: {:.2f}".format(roc_auc_ml))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Feature importance visualization:
importances = rf.feature_importances_
feat_names = X_ml.columns
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10,6))
sns.barplot(x=importances[indices], y=feat_names[indices], palette="viridis")
plt.xlabel("Feature Importance")
plt.ylabel("Feature")
plt.title("Random Forest Feature Importances")
plt.show()

##############################################
# STEP 6: Causal Analysis Using DoWhy
##############################################

# For an example causal analysis, let’s test whether being Male
# (treatment) affects the likelihood of compulsive purchasing.
# We create a binary treatment variable from Q-demos-gender.
# (Assume that "Male" is coded as such and all others as 0.)

# Create treatment: 1 if Q-demos-gender is "Male", else 0.
customer_df['is_male'] = (customer_df['Q-demos-gender'].str.strip().str.lower() == 'male').astype(int)

# For confounders, we use other demographic variables.
# Convert age, income, education into dummies.
confounder_vars = ['Q-demos-age', 'Q-demos-income', 'Q-demos-education']
confounders = pd.get_dummies(customer_df[confounder_vars], drop_first=True)
# Merge the confounders with the main DataFrame.
df_causal = pd.concat([customer_df[['compulsive_purchase', 'is_male']], confounders], axis=1)

# Build a causal model where:
# - Treatment: is_male
# - Outcome: compulsive_purchase (binary)
# - Common Causes: the confounders we just created.
model_causal = CausalModel(
    data=df_causal,
    treatment='is_male',
    outcome='compulsive_purchase',
    common_causes=confounders.columns.tolist()
)

# Visualize the assumed causal graph.
model_causal.view_model(layout="dot")
display(Image(filename="causal_model.png"))

# Identify and estimate the causal effect using a back-door adjustment.
identified_estimand = model_causal.identify_effect()
print("\nIdentified Causal Estimand:")
print(identified_estimand)

# Estimate effect via logistic regression (backdoor adjustment).
estimate_causal = model_causal.estimate_effect(identified_estimand, method_name="backdoor.propensity_score_matching")
print("\nEstimated Causal Effect (Propensity Score Matching):")
print(estimate_causal)

# Run a refutation test (adding a random common cause).
refute_results = model_causal.refute_estimate(identified_estimand, estimate_causal,
                                              method_name="random_common_cause")
print("\nCausal Refutation Test (Random Common Cause):")
print(refute_results)

##############################################
# END OF ANALYSIS
##############################################

# Uncomment the following lines if needed:
# !pip install dowhy
# !pip install scikit-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import dowhy
from dowhy import CausalModel
from IPython.display import Image, display

# Machine learning imports:
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, classification_report
from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer

##############################################
# STEP 1: Data Preparation and Aggregation
##############################################

# --- Load Purchase Data ---
# Purchase file is comma-separated.
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv')
purchase_data.columns = purchase_data.columns.str.strip()  # clean column names

# --- Load Survey Data ---
survey_data = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')
survey_data.columns = survey_data.columns.str.strip()

# --- Inspect columns for debugging ---
print("Purchase Data Columns:")
print(purchase_data.columns.tolist())
print("Survey Data Columns:")
print(survey_data.columns.tolist())

# --- Process Survey Data: Create Life Change Dummies ---
# (Assumes that the survey file has a column "Q-life-changes")
survey_data['Q-life-changes'] = survey_data['Q-life-changes'].fillna('')

# Split the "Q-life-changes" string into a list per respondent
life_change_lists = survey_data['Q-life-changes'].apply(
    lambda x: [item.strip() for item in x.split(',') if item.strip() != '']
)

# Use MultiLabelBinarizer to one-hot encode the life change events.
mlb = MultiLabelBinarizer()
life_change_dummies = pd.DataFrame(mlb.fit_transform(life_change_lists),
                                   columns=mlb.classes_,
                                   index=survey_data.index)
survey_data = pd.concat([survey_data, life_change_dummies], axis=1)

# --- Merge Data on "Survey ResponseID" ---
# Ensure both DataFrames contain the key "Survey ResponseID"
if 'Survey ResponseID' not in purchase_data.columns:
    raise KeyError("Column 'Survey ResponseID' is missing from purchase_data.")
if 'Survey ResponseID' not in survey_data.columns:
    raise KeyError("Column 'Survey ResponseID' is missing from survey_data.")

df = pd.merge(purchase_data, survey_data, on='Survey ResponseID', how='left')

# Compute total purchase value for each order.
df['total_purchase'] = df['Purchase Price Per Unit'] * df['Quantity']

# --- Aggregate at Customer Level ---
# Compute total spending and number of purchases per customer.
customer_agg = df.groupby('Survey ResponseID').agg({
    'total_purchase': 'sum',
    'Order Date': 'count'  # number of orders per customer
}).rename(columns={'Order Date': 'num_purchases'}).reset_index()

# Merge these metrics back with survey data.
customer_df = pd.merge(survey_data, customer_agg, on='Survey ResponseID', how='left')
customer_df['total_purchase'] = customer_df['total_purchase'].fillna(0)
customer_df['num_purchases'] = customer_df['num_purchases'].fillna(0)

##############################################
# STEP 2: Define "Compulsive Purchase" Outcome
##############################################

# Define "compulsive_purchase" as customers in the top 10% by number of purchases.
threshold = customer_df['num_purchases'].quantile(0.9)
customer_df['compulsive_purchase'] = (customer_df['num_purchases'] >= threshold).astype(int)

print("Threshold for compulsive purchase (top 10%):", threshold)
print(customer_df[['Survey ResponseID','num_purchases','compulsive_purchase']].head())

##############################################
# STEP 3: Visualization & Exploratory Analysis
##############################################

# Histogram: Distribution of number of purchases with threshold line.
plt.figure(figsize=(8,6))
sns.histplot(customer_df['num_purchases'], bins=30, kde=True)
plt.axvline(threshold, color='red', linestyle='--', label='90th Percentile')
plt.title("Distribution of Number of Purchases")
plt.xlabel("Number of Purchases")
plt.ylabel("Count")
plt.legend()
plt.show()

# Boxplot: Total purchase value by compulsive purchase status.
plt.figure(figsize=(8,6))
sns.boxplot(x='compulsive_purchase', y='total_purchase', data=customer_df)
plt.title("Total Purchase Value by Compulsive Purchase Status")
plt.xlabel("Compulsive Purchase (0 = No, 1 = Yes)")
plt.ylabel("Total Purchase Value")
plt.show()

##############################################
# STEP 4: Regression Analysis (Logistic Regression)
##############################################

# We use demographic variables to predict compulsive purchase.
# For this example, we use: Q-demos-age, Q-demos-income, Q-demos-education, Q-demos-gender.
demo_vars = ['Q-demos-age', 'Q-demos-income', 'Q-demos-education', 'Q-demos-gender']
demo_data = customer_df[demo_vars].copy()

# Convert categorical variables to dummy variables.
demo_dummies = pd.get_dummies(demo_data, drop_first=True)
X = demo_dummies.copy()
y = customer_df['compulsive_purchase']

# Add a constant for statsmodels.
X_const = sm.add_constant(X)

# ***** FIX: Convert to numeric *****
# Convert all predictors and response to numeric types.
X_const = X_const.apply(pd.to_numeric, errors='coerce')
y = pd.to_numeric(y, errors='coerce')

# Fit a logistic regression model.
logit_model = sm.Logit(y, X_const).fit(disp=False)
print("\nLogistic Regression Summary:")
print(logit_model.summary())

# Calculate predicted probabilities and plot ROC curve.
pred_probs = logit_model.predict(X_const)
from sklearn.metrics import roc_curve, auc
fpr, tpr, thresholds = roc_curve(y, pred_probs)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:0.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Logistic Regression')
plt.legend(loc="lower right")
plt.show()

##############################################
# STEP 5: Machine Learning Approach (Random Forest)
##############################################

# Use the same demographic dummy variables for Random Forest classification.
X_ml = demo_dummies.copy()
y_ml = customer_df['compulsive_purchase']

# Split data into training and testing sets.
X_train, X_test, y_train, y_test = train_test_split(X_ml, y_ml, test_size=0.3, random_state=42, stratify=y_ml)

# Train a RandomForestClassifier.
rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
roc_auc_ml = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])

print("\nRandom Forest Classifier Results:")
print("Accuracy: {:.2f}%".format(accuracy * 100))
print("ROC-AUC: {:.2f}".format(roc_auc_ml))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Plot feature importances.
importances = rf.feature_importances_
feat_names = X_ml.columns
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10,6))
sns.barplot(x=importances[indices], y=feat_names[indices], palette="viridis")
plt.xlabel("Feature Importance")
plt.ylabel("Feature")
plt.title("Random Forest Feature Importances")
plt.show()

##############################################
# STEP 6: Causal Analysis Using DoWhy
##############################################

# For an example causal analysis, test whether being Male affects compulsive purchasing.
# Create a binary treatment variable: 1 if Q-demos-gender is "Male", else 0.
customer_df['is_male'] = (customer_df['Q-demos-gender'].str.strip().str.lower() == 'male').astype(int)

# For confounders, use other demographic variables.
confounder_vars = ['Q-demos-age', 'Q-demos-income', 'Q-demos-education']
confounders = pd.get_dummies(customer_df[confounder_vars], drop_first=True)
df_causal = pd.concat([customer_df[['compulsive_purchase', 'is_male']], confounders], axis=1)

# Build a causal model:
#   - Treatment: is_male
#   - Outcome: compulsive_purchase (binary)
#   - Common causes: the confounders.
model_causal = CausalModel(
    data=df_causal,
    treatment='is_male',
    outcome='compulsive_purchase',
    common_causes=confounders.columns.tolist()
)

# Visualize the causal graph.
model_causal.view_model(layout="dot")
display(Image(filename="causal_model.png"))

# Identify the causal effect using the back-door criterion.
identified_estimand = model_causal.identify_effect()
print("\nIdentified Causal Estimand:")
print(identified_estimand)

# Estimate the causal effect using propensity score matching.
estimate_causal = model_causal.estimate_effect(identified_estimand,
                                               method_name="backdoor.propensity_score_matching")
print("\nEstimated Causal Effect (Propensity Score Matching):")
print(estimate_causal)

# Run a refutation test by adding a random common cause.
refute_results = model_causal.refute_estimate(identified_estimand, estimate_causal,
                                              method_name="random_common_cause")
print("\nCausal Refutation Test (Random Common Cause):")
print(refute_results)

##############################################
# END OF ANALYSIS
##############################################

# Uncomment these lines if needed:
# !pip install dowhy
# !pip install scikit-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import dowhy
from dowhy import CausalModel
from IPython.display import Image, display

# Machine learning imports:
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, classification_report
from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer

##############################################
# STEP 1: Data Preparation and Aggregation
##############################################

# --- Load Purchase Data ---
# Purchase file is comma-separated.
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv')
purchase_data.columns = purchase_data.columns.str.strip()  # Clean column names

# --- Load Survey Data ---
survey_data = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')
survey_data.columns = survey_data.columns.str.strip()

# --- Inspect columns for debugging ---
print("Purchase Data Columns:")
print(purchase_data.columns.tolist())
print("Survey Data Columns:")
print(survey_data.columns.tolist())

# --- Process Survey Data: Create Life Change Dummies ---
# (Assumes that the survey file has a column "Q-life-changes")
survey_data['Q-life-changes'] = survey_data['Q-life-changes'].fillna('')

# Split the "Q-life-changes" string into a list per respondent.
life_change_lists = survey_data['Q-life-changes'].apply(
    lambda x: [item.strip() for item in x.split(',') if item.strip() != '']
)

# Use MultiLabelBinarizer to one-hot encode the life change events.
mlb = MultiLabelBinarizer()
life_change_dummies = pd.DataFrame(mlb.fit_transform(life_change_lists),
                                   columns=mlb.classes_,
                                   index=survey_data.index)
survey_data = pd.concat([survey_data, life_change_dummies], axis=1)

# --- Merge Data on "Survey ResponseID" ---
if 'Survey ResponseID' not in purchase_data.columns:
    raise KeyError("Column 'Survey ResponseID' is missing from purchase_data.")
if 'Survey ResponseID' not in survey_data.columns:
    raise KeyError("Column 'Survey ResponseID' is missing from survey_data.")

df = pd.merge(purchase_data, survey_data, on='Survey ResponseID', how='left')

# Compute total purchase value for each order.
df['total_purchase'] = df['Purchase Price Per Unit'] * df['Quantity']

# --- Aggregate at Customer Level ---
customer_agg = df.groupby('Survey ResponseID').agg({
    'total_purchase': 'sum',
    'Order Date': 'count'  # Number of orders per customer.
}).rename(columns={'Order Date': 'num_purchases'}).reset_index()

customer_df = pd.merge(survey_data, customer_agg, on='Survey ResponseID', how='left')
customer_df['total_purchase'] = customer_df['total_purchase'].fillna(0)
customer_df['num_purchases'] = customer_df['num_purchases'].fillna(0)

##############################################
# STEP 2: Define "Compulsive Purchase" Outcome
##############################################

# Define "compulsive_purchase" as those in the top 10% by number of purchases.
threshold = customer_df['num_purchases'].quantile(0.9)
customer_df['compulsive_purchase'] = (customer_df['num_purchases'] >= threshold).astype(int)

print("Threshold for compulsive purchase (top 10%):", threshold)
print(customer_df[['Survey ResponseID','num_purchases','compulsive_purchase']].head())

##############################################
# STEP 3: Visualization & Exploratory Analysis
##############################################

plt.figure(figsize=(8,6))
sns.histplot(customer_df['num_purchases'], bins=30, kde=True)
plt.axvline(threshold, color='red', linestyle='--', label='90th Percentile')
plt.title("Distribution of Number of Purchases")
plt.xlabel("Number of Purchases")
plt.ylabel("Count")
plt.legend()
plt.show()

plt.figure(figsize=(8,6))
sns.boxplot(x='compulsive_purchase', y='total_purchase', data=customer_df)
plt.title("Total Purchase Value by Compulsive Purchase Status")
plt.xlabel("Compulsive Purchase (0 = No, 1 = Yes)")
plt.ylabel("Total Purchase Value")
plt.show()

##############################################
# STEP 4: Regression Analysis (Logistic Regression)
##############################################

# Use demographic variables to predict compulsive purchase.
# For example, use: Q-demos-age, Q-demos-income, Q-demos-education, Q-demos-gender.
demo_vars = ['Q-demos-age', 'Q-demos-income', 'Q-demos-education', 'Q-demos-gender']
demo_data = customer_df[demo_vars].copy()

# Convert categorical variables into dummy/indicator variables.
demo_dummies = pd.get_dummies(demo_data, drop_first=True)
X = demo_dummies.copy()
y = customer_df['compulsive_purchase']

# Add a constant.
X_const = sm.add_constant(X)

# --- Force conversion to numeric and drop any rows with NaNs ---
X_const = X_const.apply(pd.to_numeric, errors='coerce')
y = pd.to_numeric(y, errors='coerce')

# Check if any columns are still of object dtype.
print("X_const dtypes:")
print(X_const.dtypes)
print("y dtype:", y.dtype)

# Combine predictors and outcome; drop rows with NaN.
data_for_logit = pd.concat([X_const, y.rename('compulsive_purchase')], axis=1).dropna()
print("Shape after dropping NaNs:", data_for_logit.shape)

# Fit logistic regression.
logit_model = sm.Logit(data_for_logit['compulsive_purchase'], data_for_logit[X_const.columns]).fit(disp=False)
print("\nLogistic Regression Summary:")
print(logit_model.summary())

# Predicted probabilities and ROC curve.
pred_probs = logit_model.predict(data_for_logit[X_const.columns])
from sklearn.metrics import roc_curve, auc
fpr, tpr, thresholds = roc_curve(data_for_logit['compulsive_purchase'], pred_probs)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:0.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Logistic Regression')
plt.legend(loc="lower right")
plt.show()

##############################################
# STEP 5: Machine Learning Approach (Random Forest)
##############################################

X_ml = demo_dummies.copy()
y_ml = customer_df['compulsive_purchase']

X_train, X_test, y_train, y_test = train_test_split(X_ml, y_ml, test_size=0.3, random_state=42, stratify=y_ml)
rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
roc_auc_ml = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])

print("\nRandom Forest Classifier Results:")
print("Accuracy: {:.2f}%".format(accuracy * 100))
print("ROC-AUC: {:.2f}".format(roc_auc_ml))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

importances = rf.feature_importances_
feat_names = X_ml.columns
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10,6))
sns.barplot(x=importances[indices], y=feat_names[indices], palette="viridis")
plt.xlabel("Feature Importance")
plt.ylabel("Feature")
plt.title("Random Forest Feature Importances")
plt.show()

##############################################
# STEP 6: Causal Analysis Using DoWhy
##############################################

customer_df['is_male'] = (customer_df['Q-demos-gender'].str.strip().str.lower() == 'male').astype(int)
confounder_vars = ['Q-demos-age', 'Q-demos-income', 'Q-demos-education']
confounders = pd.get_dummies(customer_df[confounder_vars], drop_first=True)
df_causal = pd.concat([customer_df[['compulsive_purchase', 'is_male']], confounders], axis=1)

model_causal = CausalModel(
    data=df_causal,
    treatment='is_male',
    outcome='compulsive_purchase',
    common_causes=confounders.columns.tolist()
)

model_causal.view_model(layout="dot")
display(Image(filename="causal_model.png"))

identified_estimand = model_causal.identify_effect()
print("\nIdentified Causal Estimand:")
print(identified_estimand)

estimate_causal = model_causal.estimate_effect(identified_estimand,
                                               method_name="backdoor.propensity_score_matching")
print("\nEstimated Causal Effect (Propensity Score Matching):")
print(estimate_causal)

refute_results = model_causal.refute_estimate(identified_estimand, estimate_causal,
                                              method_name="random_common_cause")
print("\nCausal Refutation Test (Random Common Cause):")
print(refute_results)

##############################################
# END OF ANALYSIS
##############################################

# Uncomment these lines if needed:
# !pip install dowhy
# !pip install scikit-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import dowhy
from dowhy import CausalModel
from IPython.display import Image, display

# Machine learning imports:
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, classification_report
from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer

##############################################
# STEP 1: Data Preparation and Aggregation
##############################################

# --- Load Purchase Data ---
# Purchase file is comma-separated.
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv')
purchase_data.columns = purchase_data.columns.str.strip()  # Clean column names

# --- Load Survey Data ---
survey_data = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')
survey_data.columns = survey_data.columns.str.strip()

# --- Inspect columns for debugging ---
print("Purchase Data Columns:")
print(purchase_data.columns.tolist())
print("Survey Data Columns:")
print(survey_data.columns.tolist())

# --- Process Survey Data: Create Life Change Dummies ---
# (Assumes that the survey file has a column "Q-life-changes")
survey_data['Q-life-changes'] = survey_data['Q-life-changes'].fillna('')

# Split the "Q-life-changes" string into a list per respondent.
life_change_lists = survey_data['Q-life-changes'].apply(
    lambda x: [item.strip() for item in x.split(',') if item.strip() != '']
)

# Use MultiLabelBinarizer to one-hot encode the life change events.
mlb = MultiLabelBinarizer()
life_change_dummies = pd.DataFrame(mlb.fit_transform(life_change_lists),
                                   columns=mlb.classes_,
                                   index=survey_data.index)
survey_data = pd.concat([survey_data, life_change_dummies], axis=1)

# --- Merge Data on "Survey ResponseID" ---
if 'Survey ResponseID' not in purchase_data.columns:
    raise KeyError("Column 'Survey ResponseID' is missing from purchase_data.")
if 'Survey ResponseID' not in survey_data.columns:
    raise KeyError("Column 'Survey ResponseID' is missing from survey_data.")

df = pd.merge(purchase_data, survey_data, on='Survey ResponseID', how='left')

# Compute total purchase value for each order.
df['total_purchase'] = df['Purchase Price Per Unit'] * df['Quantity']

# --- Aggregate at Customer Level ---
customer_agg = df.groupby('Survey ResponseID').agg({
    'total_purchase': 'sum',
    'Order Date': 'count'  # Number of orders per customer.
}).rename(columns={'Order Date': 'num_purchases'}).reset_index()

customer_df = pd.merge(survey_data, customer_agg, on='Survey ResponseID', how='left')
customer_df['total_purchase'] = customer_df['total_purchase'].fillna(0)
customer_df['num_purchases'] = customer_df['num_purchases'].fillna(0)

##############################################
# STEP 2: Define "Compulsive Purchase" Outcome
##############################################

# Define "compulsive_purchase" as those in the top 10% by number of purchases.
threshold = customer_df['num_purchases'].quantile(0.9)
customer_df['compulsive_purchase'] = (customer_df['num_purchases'] >= threshold).astype(int)

print("Threshold for compulsive purchase (top 10%):", threshold)
print(customer_df[['Survey ResponseID','num_purchases','compulsive_purchase']].head())

##############################################
# STEP 3: Visualization & Exploratory Analysis
##############################################

plt.figure(figsize=(8,6))
sns.histplot(customer_df['num_purchases'], bins=30, kde=True)
plt.axvline(threshold, color='red', linestyle='--', label='90th Percentile')
plt.title("Distribution of Number of Purchases")
plt.xlabel("Number of Purchases")
plt.ylabel("Count")
plt.legend()
plt.show()

plt.figure(figsize=(8,6))
sns.boxplot(x='compulsive_purchase', y='total_purchase', data=customer_df)
plt.title("Total Purchase Value by Compulsive Purchase Status")
plt.xlabel("Compulsive Purchase (0 = No, 1 = Yes)")
plt.ylabel("Total Purchase Value")
plt.show()

##############################################
# STEP 4: Regression Analysis (Logistic Regression)
##############################################

# Use demographic variables to predict compulsive purchase.
# For example, use: Q-demos-age, Q-demos-income, Q-demos-education, Q-demos-gender.
demo_vars = ['Q-demos-age', 'Q-demos-income', 'Q-demos-education', 'Q-demos-gender']
demo_data = customer_df[demo_vars].copy()

# Convert categorical variables into dummy/indicator variables.
demo_dummies = pd.get_dummies(demo_data, drop_first=True)
X = demo_dummies.copy()
y = customer_df['compulsive_purchase']

# Add a constant.
X_const = sm.add_constant(X)

# --- Convert predictors and outcome to numeric ---
X_const = X_const.apply(pd.to_numeric, errors='coerce')
y = pd.to_numeric(y, errors='coerce')

# Combine predictors and outcome; drop any rows with NaN.
data_for_logit = pd.concat([X_const, y.rename('compulsive_purchase')], axis=1).dropna()

# **Key Fix:** Convert the entire DataFrame to float.
data_for_logit = data_for_logit.astype(float)

print("X_const dtypes after conversion:")
print(data_for_logit[X_const.columns].dtypes)
print("y dtype:", data_for_logit['compulsive_purchase'].dtype)
print("Shape after dropping NaNs and conversion:", data_for_logit.shape)

# Fit logistic regression.
logit_model = sm.Logit(data_for_logit['compulsive_purchase'], data_for_logit[X_const.columns]).fit(disp=False)
print("\nLogistic Regression Summary:")
print(logit_model.summary())

# Calculate predicted probabilities and plot ROC curve.
pred_probs = logit_model.predict(data_for_logit[X_const.columns])
from sklearn.metrics import roc_curve, auc
fpr, tpr, thresholds = roc_curve(data_for_logit['compulsive_purchase'], pred_probs)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:0.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Logistic Regression')
plt.legend(loc="lower right")
plt.show()

##############################################
# STEP 5: Machine Learning Approach (Random Forest)
##############################################

X_ml = demo_dummies.copy()
y_ml = customer_df['compulsive_purchase']

X_train, X_test, y_train, y_test = train_test_split(X_ml, y_ml, test_size=0.3, random_state=42, stratify=y_ml)
rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
roc_auc_ml = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])

print("\nRandom Forest Classifier Results:")
print("Accuracy: {:.2f}%".format(accuracy * 100))
print("ROC-AUC: {:.2f}".format(roc_auc_ml))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

importances = rf.feature_importances_
feat_names = X_ml.columns
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10,6))
sns.barplot(x=importances[indices], y=feat_names[indices], palette="viridis")
plt.xlabel("Feature Importance")
plt.ylabel("Feature")
plt.title("Random Forest Feature Importances")
plt.show()

##############################################
# STEP 6: Causal Analysis Using DoWhy
##############################################

# For an example causal analysis, test whether being Male affects compulsive purchasing.
customer_df['is_male'] = (customer_df['Q-demos-gender'].str.strip().str.lower() == 'male').astype(int)
confounder_vars = ['Q-demos-age', 'Q-demos-income', 'Q-demos-education']
confounders = pd.get_dummies(customer_df[confounder_vars], drop_first=True)
df_causal = pd.concat([customer_df[['compulsive_purchase', 'is_male']], confounders], axis=1)

model_causal = CausalModel(
    data=df_causal,
    treatment='is_male',
    outcome='compulsive_purchase',
    common_causes=confounders.columns.tolist()
)

model_causal.view_model(layout="dot")
display(Image(filename="causal_model.png"))

identified_estimand = model_causal.identify_effect()
print("\nIdentified Causal Estimand:")
print(identified_estimand)

estimate_causal = model_causal.estimate_effect(identified_estimand,
                                               method_name="backdoor.propensity_score_matching")
print("\nEstimated Causal Effect (Propensity Score Matching):")
print(estimate_causal)

refute_results = model_causal.refute_estimate(identified_estimand, estimate_causal,
                                              method_name="random_common_cause")
print("\nCausal Refutation Test (Random Common Cause):")
print(refute_results)

##############################################
# END OF ANALYSIS
##############################################

# Uncomment if needed:
# !pip install dowhy scikit-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import dowhy
from dowhy import CausalModel
from IPython.display import Image, display

# Machine learning imports:
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, classification_report
from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer

##############################################
# STEP 1: Data Preparation and Aggregation
##############################################

# --- Load Purchase Data (comma-separated) ---
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv')
purchase_data.columns = purchase_data.columns.str.strip()  # Clean column names

# --- Load Survey Data (comma-separated) ---
survey_data = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')
survey_data.columns = survey_data.columns.str.strip()

# --- Debug: Print column names ---
print("Purchase Data Columns:")
print(purchase_data.columns.tolist())
print("Survey Data Columns:")
print(survey_data.columns.tolist())

# --- Process Survey Data: Create Life Change Dummies ---
# (Assumes that the survey file has a column "Q-life-changes")
survey_data['Q-life-changes'] = survey_data['Q-life-changes'].fillna('')

# Split the "Q-life-changes" string into a list for each respondent.
life_change_lists = survey_data['Q-life-changes'].apply(
    lambda x: [item.strip() for item in x.split(',') if item.strip() != '']
)

# Use MultiLabelBinarizer to one-hot encode the life change events.
mlb = MultiLabelBinarizer()
life_change_dummies = pd.DataFrame(mlb.fit_transform(life_change_lists),
                                   columns=mlb.classes_,
                                   index=survey_data.index)
# Append the life-change dummy columns to the survey DataFrame.
survey_data = pd.concat([survey_data, life_change_dummies], axis=1)

# --- Merge Data on "Survey ResponseID" ---
if 'Survey ResponseID' not in purchase_data.columns:
    raise KeyError("Column 'Survey ResponseID' is missing from purchase_data.")
if 'Survey ResponseID' not in survey_data.columns:
    raise KeyError("Column 'Survey ResponseID' is missing from survey_data.")

df = pd.merge(purchase_data, survey_data, on='Survey ResponseID', how='left')

# Compute total purchase value for each order.
df['total_purchase'] = df['Purchase Price Per Unit'] * df['Quantity']

# --- Aggregate at Customer Level ---
# Sum up total spending and count orders per customer.
customer_agg = df.groupby('Survey ResponseID').agg({
    'total_purchase': 'sum',
    'Order Date': 'count'  # number of orders per customer
}).rename(columns={'Order Date': 'num_purchases'}).reset_index()

# Merge aggregated purchase metrics back into the survey data.
customer_df = pd.merge(survey_data, customer_agg, on='Survey ResponseID', how='left')
customer_df['total_purchase'] = customer_df['total_purchase'].fillna(0)
customer_df['num_purchases'] = customer_df['num_purchases'].fillna(0)

##############################################
# STEP 2: Define "Compulsive Purchase" Outcome
##############################################

# Define "compulsive_purchase" as customers in the top 10% by number of purchases.
threshold = customer_df['num_purchases'].quantile(0.9)
customer_df['compulsive_purchase'] = (customer_df['num_purchases'] >= threshold).astype(int)

print("Threshold for compulsive purchase (top 10%):", threshold)
print(customer_df[['Survey ResponseID', 'num_purchases', 'compulsive_purchase']].head())

##############################################
# STEP 3: Visualization & Exploratory Analysis
##############################################

plt.figure(figsize=(8,6))
sns.histplot(customer_df['num_purchases'], bins=30, kde=True)
plt.axvline(threshold, color='red', linestyle='--', label='90th Percentile')
plt.title("Distribution of Number of Purchases")
plt.xlabel("Number of Purchases")
plt.ylabel("Count")
plt.legend()
plt.show()

plt.figure(figsize=(8,6))
sns.boxplot(x='compulsive_purchase', y='total_purchase', data=customer_df)
plt.title("Total Purchase Value by Compulsive Purchase Status")
plt.xlabel("Compulsive Purchase (0 = No, 1 = Yes)")
plt.ylabel("Total Purchase Value")
plt.show()

##############################################
# STEP 4: Regression Analysis (Logistic Regression)
##############################################

# Build a comprehensive set of predictors from survey data.
# Core demographics:
demo_cols = ['Q-demos-age', 'Q-demos-income', 'Q-demos-education', 'Q-demos-gender']
# Additional survey questions:
other_cols = [
    'Q-amazon-use-howmany', 'Q-amazon-use-hh-size', 'Q-amazon-use-how-oft',
    'Q-substance-use-cigarettes', 'Q-substance-use-marijuana', 'Q-substance-use-alcohol',
    'Q-personal-diabetes', 'Q-personal-wheelchair',
    'Q-sell-YOUR-data', 'Q-sell-consumer-data', 'Q-small-biz-use', 'Q-census-use', 'Q-research-society'
]
# Also include the life-change dummy columns that were added from Q-life-changes.
life_change_cols = list(mlb.classes_)

# Combine all feature columns.
all_feature_cols = demo_cols + other_cols + life_change_cols

# Subset the customer_df for these columns.
features_df = customer_df[all_feature_cols]

# Convert categorical features into dummy variables.
# This will one-hot encode all categorical responses.
features_dummies = pd.get_dummies(features_df, drop_first=True)

# Use the expanded features for our model.
X_full = features_dummies.copy()
y_full = customer_df['compulsive_purchase']

# Add a constant term.
X_full_const = sm.add_constant(X_full)

# --- Convert predictors and outcome to numeric, drop any NaNs ---
X_full_const = X_full_const.apply(pd.to_numeric, errors='coerce')
y_full = pd.to_numeric(y_full, errors='coerce')
data_for_logit = pd.concat([X_full_const, y_full.rename('compulsive_purchase')], axis=1).dropna()
# Convert everything to float.
data_for_logit = data_for_logit.astype(float)

print("Shape after dropping NaNs:", data_for_logit.shape)
print("Data types for predictors:")
print(data_for_logit[X_full_const.columns].dtypes)
print("Outcome dtype:", data_for_logit['compulsive_purchase'].dtype)

# Fit a logistic regression model.
logit_model = sm.Logit(data_for_logit['compulsive_purchase'], data_for_logit[X_full_const.columns]).fit(disp=False)
print("\nLogistic Regression Summary:")
print(logit_model.summary())

# Predicted probabilities and ROC curve.
pred_probs = logit_model.predict(data_for_logit[X_full_const.columns])
from sklearn.metrics import roc_curve, auc
fpr, tpr, thresholds = roc_curve(data_for_logit['compulsive_purchase'], pred_probs)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:0.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Logistic Regression')
plt.legend(loc="lower right")
plt.show()

##############################################
# STEP 5: Machine Learning Approach (Random Forest)
##############################################

# Use the same comprehensive set of features for Random Forest.
X_ml_full = features_dummies.copy()
y_ml_full = customer_df['compulsive_purchase']

# Split the data into training and test sets.
X_train, X_test, y_train, y_test = train_test_split(X_ml_full, y_ml_full, test_size=0.3, random_state=42, stratify=y_ml_full)

rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
roc_auc_ml = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])

print("\nRandom Forest Classifier Results:")
print("Accuracy: {:.2f}%".format(accuracy * 100))
print("ROC-AUC: {:.2f}".format(roc_auc_ml))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Feature importance visualization.
importances = rf.feature_importances_
feat_names = X_ml_full.columns
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10,6))
sns.barplot(x=importances[indices], y=feat_names[indices], palette="viridis")
plt.xlabel("Feature Importance")
plt.ylabel("Feature")
plt.title("Random Forest Feature Importances")
plt.show()

##############################################
# STEP 6: Causal Analysis Using DoWhy
##############################################

# For an example causal analysis, test whether being Male affects compulsive purchasing.
customer_df['is_male'] = (customer_df['Q-demos-gender'].str.strip().str.lower() == 'male').astype(int)

# For confounders, we use a subset of demographics.
confounder_vars = ['Q-demos-age', 'Q-demos-income', 'Q-demos-education']
confounders = pd.get_dummies(customer_df[confounder_vars], drop_first=True)
df_causal = pd.concat([customer_df[['compulsive_purchase', 'is_male']], confounders], axis=1)

model_causal = CausalModel(
    data=df_causal,
    treatment='is_male',
    outcome='compulsive_purchase',
    common_causes=confounders.columns.tolist()
)

model_causal.view_model(layout="dot")
display(Image(filename="causal_model.png"))

identified_estimand = model_causal.identify_effect()
print("\nIdentified Causal Estimand:")
print(identified_estimand)

estimate_causal = model_causal.estimate_effect(identified_estimand,
                                               method_name="backdoor.propensity_score_matching")
print("\nEstimated Causal Effect (Propensity Score Matching):")
print(estimate_causal)

refute_results = model_causal.refute_estimate(identified_estimand, estimate_causal,
                                              method_name="random_common_cause")
print("\nCausal Refutation Test (Random Common Cause):")
print(refute_results)

##############################################
# END OF ANALYSIS
##############################################

# Uncomment these lines if needed:
# !pip install dowhy scikit-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import dowhy
from dowhy import CausalModel
from IPython.display import Image, display

# Machine learning imports:
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, classification_report
from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer

##############################################
# STEP 1: Data Preparation and Aggregation
##############################################

# --- Load Purchase Data (comma-separated) ---
purchase_data = pd.read_csv('/content/drive/MyDrive/amazon/amazon-purchases.csv')
purchase_data.columns = purchase_data.columns.str.strip()  # Clean column names

# --- Load Survey Data (comma-separated) ---
survey_data = pd.read_csv('/content/drive/MyDrive/amazon/survey.csv')
survey_data.columns = survey_data.columns.str.strip()

# --- Debug: Print column names ---
print("Purchase Data Columns:")
print(purchase_data.columns.tolist())
print("Survey Data Columns:")
print(survey_data.columns.tolist())

# --- Process Survey Data: Create Life Change Dummies ---
survey_data['Q-life-changes'] = survey_data['Q-life-changes'].fillna('')

life_change_lists = survey_data['Q-life-changes'].apply(
    lambda x: [item.strip() for item in x.split(',') if item.strip() != '']
)

mlb = MultiLabelBinarizer()
life_change_dummies = pd.DataFrame(mlb.fit_transform(life_change_lists),
                                   columns=mlb.classes_,
                                   index=survey_data.index)
survey_data = pd.concat([survey_data, life_change_dummies], axis=1)

# --- Merge Data on "Survey ResponseID" ---
if 'Survey ResponseID' not in purchase_data.columns:
    raise KeyError("Column 'Survey ResponseID' is missing from purchase_data.")
if 'Survey ResponseID' not in survey_data.columns:
    raise KeyError("Column 'Survey ResponseID' is missing from survey_data.")

df = pd.merge(purchase_data, survey_data, on='Survey ResponseID', how='left')
df['total_purchase'] = df['Purchase Price Per Unit'] * df['Quantity']

customer_agg = df.groupby('Survey ResponseID').agg({
    'total_purchase': 'sum',
    'Order Date': 'count'
}).rename(columns={'Order Date': 'num_purchases'}).reset_index()

customer_df = pd.merge(survey_data, customer_agg, on='Survey ResponseID', how='left')
customer_df['total_purchase'] = customer_df['total_purchase'].fillna(0)
customer_df['num_purchases'] = customer_df['num_purchases'].fillna(0)

##############################################
# STEP 2: Define "Compulsive Purchase" Outcome
##############################################

threshold = customer_df['num_purchases'].quantile(0.8)
customer_df['compulsive_purchase'] = (customer_df['num_purchases'] >= threshold).astype(int)

print("Threshold for compulsive purchase (top 10%):", threshold)
print(customer_df[['Survey ResponseID', 'num_purchases', 'compulsive_purchase']].head())

##############################################
# STEP 3: Visualization & Exploratory Analysis
##############################################

plt.figure(figsize=(8,6))
sns.histplot(customer_df['num_purchases'], bins=30, kde=True)
plt.axvline(threshold, color='red', linestyle='--', label='90th Percentile')
plt.title("Distribution of Number of Purchases")
plt.xlabel("Number of Purchases")
plt.ylabel("Count")
plt.legend()
plt.show()

plt.figure(figsize=(8,6))
sns.boxplot(x='compulsive_purchase', y='total_purchase', data=customer_df)
plt.title("Total Purchase Value by Compulsive Purchase Status")
plt.xlabel("Compulsive Purchase (0 = No, 1 = Yes)")
plt.ylabel("Total Purchase Value")
plt.show()

##############################################
# STEP 4: Regression Analysis (Logistic Regression)
##############################################

# Build an expanded set of predictors:
demo_cols = ['Q-demos-age', 'Q-demos-income', 'Q-demos-education', 'Q-demos-gender']
other_cols = [
    'Q-amazon-use-howmany', 'Q-amazon-use-hh-size', 'Q-amazon-use-how-oft',
    'Q-substance-use-cigarettes', 'Q-substance-use-marijuana', 'Q-substance-use-alcohol',
    'Q-personal-diabetes', 'Q-personal-wheelchair',
    'Q-sell-YOUR-data', 'Q-sell-consumer-data', 'Q-small-biz-use', 'Q-census-use', 'Q-research-society'
]
life_change_cols = list(mlb.classes_)

all_feature_cols = demo_cols + other_cols + life_change_cols
features_df = customer_df[all_feature_cols]

# Convert all features into dummy/indicator variables.
features_dummies = pd.get_dummies(features_df, drop_first=True)

# Expanded feature matrix and outcome.
X_full = features_dummies.copy()
y_full = customer_df['compulsive_purchase']

# Add a constant term.
X_full_const = sm.add_constant(X_full)

# Convert predictors and outcome to numeric and drop NaNs.
X_full_const = X_full_const.apply(pd.to_numeric, errors='coerce')
y_full = pd.to_numeric(y_full, errors='coerce')
data_for_logit = pd.concat([X_full_const, y_full.rename('compulsive_purchase')], axis=1).dropna()
data_for_logit = data_for_logit.astype(float)

print("Shape after dropping NaNs:", data_for_logit.shape)
print("Rank of design matrix:", np.linalg.matrix_rank(data_for_logit[X_full_const.columns]),
      "out of", data_for_logit[X_full_const.columns].shape[1])

# If the rank is less than the number of predictors, you can attempt to drop some near‐collinear variables.
# (Our previous drop_collinear function already tried this, but suppose we still have a singularity.)

# Here we use a regularized estimator to handle the near-singularity.
# Using fit_regularized with L1 (or L2) penalty can help.
# You can adjust the strength of the penalty via the "alpha" parameter.
try:
    logit_model = sm.Logit(data_for_logit['compulsive_purchase'],
                           data_for_logit[X_full_const.columns]).fit_regularized(alpha=0.01, disp=False)
except Exception as e:
    print("Error during regularized logistic regression fitting:", e)
else:
    print("\nLogistic Regression (Regularized) Summary:")
    print(logit_model.summary())

# Predicted probabilities and ROC curve.
pred_probs = logit_model.predict(data_for_logit[X_full_const.columns])
from sklearn.metrics import roc_curve, auc
fpr, tpr, thresholds = roc_curve(data_for_logit['compulsive_purchase'], pred_probs)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:0.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Logistic Regression (Regularized)')
plt.legend(loc="lower right")
plt.show()


##############################################
# STEP 5: Machine Learning Approach (Random Forest)
##############################################

X_ml_full = features_dummies.copy()
y_ml_full = customer_df['compulsive_purchase']

X_train, X_test, y_train, y_test = train_test_split(X_ml_full, y_ml_full, test_size=0.3, random_state=42, stratify=y_ml_full)

rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
roc_auc_ml = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])

print("\nRandom Forest Classifier Results:")
print("Accuracy: {:.2f}%".format(accuracy * 100))
print("ROC-AUC: {:.2f}".format(roc_auc_ml))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

importances = rf.feature_importances_
feat_names = X_ml_full.columns
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10,6))
sns.barplot(x=importances[indices], y=feat_names[indices], palette="viridis")
plt.xlabel("Feature Importance")
plt.ylabel("Feature")
plt.title("Random Forest Feature Importances")
plt.show()

##############################################
# STEP 6: Causal Analysis Using DoWhy
##############################################

customer_df['is_male'] = (customer_df['Q-demos-gender'].str.strip().str.lower() == 'male').astype(int)
confounder_vars = ['Q-demos-age', 'Q-demos-income', 'Q-demos-education']
confounders = pd.get_dummies(customer_df[confounder_vars], drop_first=True)
df_causal = pd.concat([customer_df[['compulsive_purchase', 'is_male']], confounders], axis=1)

model_causal = CausalModel(
    data=df_causal,
    treatment='is_male',
    outcome='compulsive_purchase',
    common_causes=confounders.columns.tolist()
)

model_causal.view_model(layout="dot")
display(Image(filename="causal_model.png"))

identified_estimand = model_causal.identify_effect()
print("\nIdentified Causal Estimand:")
print(identified_estimand)

estimate_causal = model_causal.estimate_effect(identified_estimand,
                                               method_name="backdoor.propensity_score_matching")
print("\nEstimated Causal Effect (Propensity Score Matching):")
print(estimate_causal)

refute_results = model_causal.refute_estimate(identified_estimand, estimate_causal,
                                              method_name="random_common_cause")
print("\nCausal Refutation Test (Random Common Cause):")
print(refute_results)

##############################################
# END OF ANALYSIS
##############################################

# Install required packages (if not already installed)
!pip install dowhy --quiet

# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# For regression analysis
import statsmodels.api as sm
import statsmodels.formula.api as smf

# For machine learning
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# For causal inference
import dowhy
from dowhy import CausalModel

# Set visualization style
sns.set(style="whitegrid")
plt.rcParams['figure.figsize'] = (12,8)

# ---------------------------
# 1. LOAD THE DATA
# ---------------------------
# If your CSV is in your Google Drive or uploaded to Colab, adjust the path accordingly.
# For example, if the file is named 'amazon_data.csv' in the current directory:
data_path = '/content/categories amazon.csv'
df = pd.read_csv(data_path)

# Check the first few rows and DataFrame info
print("First 5 rows of the dataset:")
print(df.head())
print("\nDataset summary:")
print(df.info())
print("\nDescriptive statistics:")
print(df.describe())

# ---------------------------
# 2. EXPLORATORY DATA ANALYSIS (EDA)
# ---------------------------
# Visualize distributions for average spends across periods
spend_columns = ['pre_lockdown_avg_spend', 'during_lockdown_avg_spend', 'post_lockdown_avg_spend']
df[spend_columns].hist(bins=20, figsize=(14,4))
plt.suptitle("Distribution of Average Spends (Pre, During, Post Lockdown)")
plt.show()

# Visualize orders and average number of orders distributions
order_columns = ['pre_lockdown_orders', 'during_lockdown_orders', 'post_lockdown_orders']
df[order_columns].hist(bins=20, figsize=(14,4))
plt.suptitle("Distribution of Total Orders (Pre, During, Post Lockdown)")
plt.show()

avg_order_cols = ['pre_lockdown_average_number_orders', 'during_lockdown_average_number_orders', 'post_lockdown_average_number_orders']
df[avg_order_cols].hist(bins=20, figsize=(14,4))
plt.suptitle("Distribution of Average Number of Orders (Pre, During, Post Lockdown)")
plt.show()

# Pairplot for selected numerical columns to inspect relationships
selected_cols = spend_columns + order_columns + avg_order_cols
sns.pairplot(df[selected_cols])
plt.suptitle("Pairplot of Spend and Order Metrics", y=1.02)
plt.show()

# ---------------------------
# 3. CORRELATION HEATMAP
# ---------------------------
corr = df[selected_cols].corr()
plt.figure(figsize=(10,8))
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap of Spend and Order Metrics")
plt.show()

# ---------------------------
# 4. REGRESSION ANALYSIS
# ---------------------------
# Example 1: Regression of during_lockdown_avg_spend based on pre_lockdown_avg_spend and during_lockdown_orders
formula1 = 'during_lockdown_avg_spend ~ pre_lockdown_avg_spend + during_lockdown_orders + during_lockdown_average_number_orders'
model1 = smf.ols(formula=formula1, data=df).fit()
print("\nRegression Results: Predicting During Lockdown Average Spend")
print(model1.summary())

# Example 2: Regression of post_lockdown_avg_spend based on pre_lockdown_avg_spend and post_lockdown_orders
formula2 = 'post_lockdown_avg_spend ~ pre_lockdown_avg_spend + post_lockdown_orders + post_lockdown_average_number_orders'
model2 = smf.ols(formula=formula2, data=df).fit()
print("\nRegression Results: Predicting Post Lockdown Average Spend")
print(model2.summary())

# You can similarly explore regressions on orders if that is of interest.

# ---------------------------
# 5. MACHINE LEARNING: PREDICTING POST-LOCKDOWN AVERAGE SPEND
# ---------------------------
# Let’s use a Random Forest to predict 'post_lockdown_avg_spend' from a few predictors.
# Define predictors and target. Here we choose some spend and order metrics.
features = ['pre_lockdown_avg_spend', 'during_lockdown_avg_spend',
            'pre_lockdown_orders', 'during_lockdown_orders',
            'pre_lockdown_average_number_orders', 'during_lockdown_average_number_orders']
target = 'post_lockdown_avg_spend'

# Prepare the data
X = df[features].copy()
y = df[target].copy()

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train RandomForestRegressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Evaluate the model using cross-validation and on the test set
cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
rmse_cv = np.mean(np.sqrt(-cv_scores))
print("\nRandom Forest CV RMSE: {:.2f}".format(rmse_cv))

# Predict on the test set and compute RMSE
y_pred = rf_model.predict(X_test)
rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))
print("Random Forest Test RMSE: {:.2f}".format(rmse_test))

# Feature importance visualization
feature_importances = pd.Series(rf_model.feature_importances_, index=features)
feature_importances.sort_values(ascending=True).plot(kind='barh')
plt.title("Feature Importances for Predicting Post Lockdown Average Spend")
plt.xlabel("Importance")
plt.show()

# ---------------------------
# 6. CAUSAL ANALYSIS USING DoWhy
# ---------------------------
# As an example, suppose we are interested in estimating the causal effect of the change in average spend
# during lockdown ('change_during_avg_spend') on the total number of during_lockdown_orders.
#
# Note: Causal analysis requires specifying a causal graph (DAG). This simple example uses a minimal model.
# You should tailor the model based on domain knowledge and potential confounders.
#
# Define treatment, outcome, and potential confounders.
treatment = 'change_during_avg_spend'
outcome = 'during_lockdown_orders'
# For demonstration, we use a couple of pre‑lockdown metrics as common causes.
common_causes = ['pre_lockdown_avg_spend', 'pre_lockdown_orders', 'pre_lockdown_average_number_orders']

# Drop rows with missing values in the selected columns (if any)
causal_df = df[[treatment, outcome] + common_causes].dropna()

# Specify and estimate the causal model
model = CausalModel(
    data=causal_df,
    treatment=treatment,
    outcome=outcome,
    common_causes=common_causes
)

# View the assumed causal graph (DAG)
model.view_model()

# Identify the causal effect using backdoor criteria
identified_estimand = model.identify_effect()
print("\nIdentified estimand:")
print(identified_estimand)

# Estimate the causal effect using a linear regression method (for demonstration)
estimate = model.estimate_effect(identified_estimand,
                                 method_name="backdoor.linear_regression")
print("\nCausal Estimate using Linear Regression:")
print(estimate.value)

# Refute the estimate using a placebo test (as an example)
refute_results = model.refute_estimate(identified_estimand, estimate, method_name="placebo_treatment_refuter", placebo_type="permute")
print("\nRefutation Results:")
print(refute_results)

# ---------------------------
# END OF ANALYSIS
# ---------------------------

# Install required packages (if not already installed)
!pip install dowhy --quiet

# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# For regression analysis
import statsmodels.api as sm
import statsmodels.formula.api as smf

# For machine learning
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# For causal inference
import dowhy
from dowhy import CausalModel

# Set visualization style
sns.set(style="whitegrid")
plt.rcParams['figure.figsize'] = (12,8)

# ---------------------------
# 1. LOAD THE DATA
# ---------------------------
# Update the path to point to your detailed CSV file that includes spend and orders information.
data_path = '/content/spends data.csv'  # <-- Change this to your actual CSV file name/path
df = pd.read_csv(data_path)

# Print basic info about the dataset
print("Columns in the dataset:")
print(df.columns.tolist())
print("\nFirst 5 rows of the dataset:")
print(df.head())

# ---------------------------
# 2. VERIFY REQUIRED COLUMNS
# ---------------------------
# List of required columns for the analysis
required_cols = [
    'pre_lockdown_avg_spend', 'during_lockdown_avg_spend', 'post_lockdown_avg_spend',
    'pre_lockdown_orders', 'during_lockdown_orders', 'post_lockdown_orders',
    'pre_lockdown_average_number_orders', 'during_lockdown_average_number_orders', 'post_lockdown_average_number_orders'
]

# Check for missing columns
missing_cols = [col for col in required_cols if col not in df.columns]
if missing_cols:
    print("\nERROR: The following required columns are missing from your dataset:")
    print(missing_cols)
    print("\nPlease verify that you are loading the correct CSV file which contains all the necessary columns for the analysis.")
else:
    print("\nAll required columns found. Proceeding with the analysis...\n")

    # ---------------------------
    # 3. EXPLORATORY DATA ANALYSIS (EDA)
    # ---------------------------
    # Visualize distributions for average spends across periods
    spend_columns = ['pre_lockdown_avg_spend', 'during_lockdown_avg_spend', 'post_lockdown_avg_spend']
    df[spend_columns].hist(bins=20, figsize=(14,4))
    plt.suptitle("Distribution of Average Spends (Pre, During, Post Lockdown)")
    plt.show()

    # Visualize distributions for total orders
    order_columns = ['pre_lockdown_orders', 'during_lockdown_orders', 'post_lockdown_orders']
    df[order_columns].hist(bins=20, figsize=(14,4))
    plt.suptitle("Distribution of Total Orders (Pre, During, Post Lockdown)")
    plt.show()

    # Visualize distributions for average number of orders
    avg_order_cols = ['pre_lockdown_average_number_orders', 'during_lockdown_average_number_orders', 'post_lockdown_average_number_orders']
    df[avg_order_cols].hist(bins=20, figsize=(14,4))
    plt.suptitle("Distribution of Average Number of Orders (Pre, During, Post Lockdown)")
    plt.show()

    # Pairplot for selected numerical columns to inspect relationships
    selected_cols = spend_columns + order_columns + avg_order_cols
    sns.pairplot(df[selected_cols])
    plt.suptitle("Pairplot of Spend and Order Metrics", y=1.02)
    plt.show()

    # ---------------------------
    # 4. CORRELATION HEATMAP
    # ---------------------------
    corr = df[selected_cols].corr()
    plt.figure(figsize=(10,8))
    sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
    plt.title("Correlation Heatmap of Spend and Order Metrics")
    plt.show()

    # ---------------------------
    # 5. REGRESSION ANALYSIS
    # ---------------------------
    # Example 1: Regression of during_lockdown_avg_spend based on pre_lockdown_avg_spend and during_lockdown_orders
    formula1 = 'during_lockdown_avg_spend ~ pre_lockdown_avg_spend + during_lockdown_orders + during_lockdown_average_number_orders'
    model1 = smf.ols(formula=formula1, data=df).fit()
    print("\nRegression Results: Predicting During Lockdown Average Spend")
    print(model1.summary())

    # Example 2: Regression of post_lockdown_avg_spend based on pre_lockdown_avg_spend and post_lockdown_orders
    formula2 = 'post_lockdown_avg_spend ~ pre_lockdown_avg_spend + post_lockdown_orders + post_lockdown_average_number_orders'
    model2 = smf.ols(formula=formula2, data=df).fit()
    print("\nRegression Results: Predicting Post Lockdown Average Spend")
    print(model2.summary())

    # ---------------------------
    # 6. MACHINE LEARNING: PREDICTING POST-LOCKDOWN AVERAGE SPEND
    # ---------------------------
    # Define predictors and target. Adjust the list of features as needed.
    features = ['pre_lockdown_avg_spend', 'during_lockdown_avg_spend',
                'pre_lockdown_orders', 'during_lockdown_orders',
                'pre_lockdown_average_number_orders', 'during_lockdown_average_number_orders']
    target = 'post_lockdown_avg_spend'

    # Prepare the data
    X = df[features].copy()
    y = df[target].copy()

    # Split into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Initialize and train RandomForestRegressor
    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
    rf_model.fit(X_train, y_train)

    # Evaluate the model using cross-validation on the training set
    cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
    rmse_cv = np.mean(np.sqrt(-cv_scores))
    print("\nRandom Forest CV RMSE: {:.2f}".format(rmse_cv))

    # Predict on the test set and compute RMSE
    y_pred = rf_model.predict(X_test)
    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))
    print("Random Forest Test RMSE: {:.2f}".format(rmse_test))

    # Feature importance visualization
    feature_importances = pd.Series(rf_model.feature_importances_, index=features)
    feature_importances.sort_values(ascending=True).plot(kind='barh')
    plt.title("Feature Importances for Predicting Post Lockdown Average Spend")
    plt.xlabel("Importance")
    plt.show()

    # ---------------------------
    # 7. CAUSAL ANALYSIS USING DoWhy
    # ---------------------------
    # Example: Estimate the causal effect of 'change_during_avg_spend' on 'during_lockdown_orders'
    # Make sure these columns exist in your dataset. Adjust the variable names as needed.
    if 'change_during_avg_spend' in df.columns:
        treatment = 'change_during_avg_spend'
        outcome = 'during_lockdown_orders'
        # Use a couple of pre-lockdown metrics as common causes.
        common_causes = ['pre_lockdown_avg_spend', 'pre_lockdown_orders', 'pre_lockdown_average_number_orders']

        # Drop rows with missing values for the selected columns (if any)
        causal_df = df[[treatment, outcome] + common_causes].dropna()

        # Specify and estimate the causal model
        model = CausalModel(
            data=causal_df,
            treatment=treatment,
            outcome=outcome,
            common_causes=common_causes
        )

        # Visualize the assumed causal graph (DAG)
        model.view_model()

        # Identify the causal effect using backdoor criteria
        identified_estimand = model.identify_effect()
        print("\nIdentified estimand:")
        print(identified_estimand)

        # Estimate the causal effect using a linear regression method (for demonstration)
        estimate = model.estimate_effect(identified_estimand, method_name="backdoor.linear_regression")
        print("\nCausal Estimate using Linear Regression:")
        print(estimate.value)

        # Refute the estimate using a placebo test (as an example)
        refute_results = model.refute_estimate(identified_estimand, estimate,
                                               method_name="placebo_treatment_refuter", placebo_type="permute")
        print("\nRefutation Results:")
        print(refute_results)
    else:
        print("\nColumn 'change_during_avg_spend' is not available in the dataset. Skipping causal analysis example.")

    # ---------------------------
    # END OF ANALYSIS
    # ---------------------------

# Install required package for causal inference if not already installed
!pip install dowhy --quiet

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# For regression analysis
import statsmodels.api as sm
import statsmodels.formula.api as smf

# For machine learning
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# For causal inference
import dowhy
from dowhy import CausalModel

# Set visualization style
sns.set(style="whitegrid")
plt.rcParams['figure.figsize'] = (12,8)

# =============================================================================
# 1. LOAD THE DATA
# =============================================================================
# Update the path to your detailed CSV file containing spends, orders, and more.
data_path = '/content/spends data.csv'   # <-- Change to your CSV file name/path
df = pd.read_csv(data_path)

# Print basic info about the dataset
print("Columns in the dataset:")
print(df.columns.tolist())
print("\nFirst 5 rows of the dataset:")
print(df.head())

# =============================================================================
# 2. VERIFY REQUIRED COLUMNS & PREPARE DATA
# =============================================================================
# List of required columns for overall analysis
required_cols = [
    'pre_lockdown_avg_spend', 'during_lockdown_avg_spend', 'post_lockdown_avg_spend',
    'pre_lockdown_orders', 'during_lockdown_orders', 'post_lockdown_orders',
    'pre_lockdown_average_number_orders', 'during_lockdown_average_number_orders', 'post_lockdown_average_number_orders'
]

missing_cols = [col for col in required_cols if col not in df.columns]
if missing_cols:
    print("\nERROR: The following required columns are missing from your dataset:")
    print(missing_cols)
    print("Please verify that you are loading the correct CSV file.")
else:
    print("\nAll required columns found. Proceeding with the analysis...\n")

# -----------------------------------------------------------------------------
# Convert the treatment column "change_during_avg_spend" to numeric if it exists.
# Assume values: 'increasing' -> 1, 'no change' -> 0, 'decreasing' -> -1.
if 'change_during_avg_spend' in df.columns:
    change_map = {'increasing': 1, 'no change': 0, 'decreasing': -1}
    df['change_during_avg_spend_numeric'] = df['change_during_avg_spend'].map(change_map)
    if df['change_during_avg_spend_numeric'].isnull().any():
        print("Warning: Some values in 'change_during_avg_spend' could not be mapped to numeric values.")
else:
    print("Column 'change_during_avg_spend' not found. Skipping numeric conversion.")

# =============================================================================
# 3. OVERALL ANALYSIS: EDA, REGRESSION, MACHINE LEARNING, AND CAUSAL ANALYSIS
# =============================================================================
if not missing_cols:
    # ----- A. Exploratory Data Analysis (EDA) -----
    spend_columns = ['pre_lockdown_avg_spend', 'during_lockdown_avg_spend', 'post_lockdown_avg_spend']
    order_columns = ['pre_lockdown_orders', 'during_lockdown_orders', 'post_lockdown_orders']
    avg_order_cols = ['pre_lockdown_average_number_orders', 'during_lockdown_average_number_orders', 'post_lockdown_average_number_orders']

    # Histograms for average spends
    df[spend_columns].hist(bins=20, figsize=(14,4))
    plt.suptitle("Distribution of Average Spends (Pre, During, Post Lockdown)")
    plt.show()

    # Histograms for total orders
    df[order_columns].hist(bins=20, figsize=(14,4))
    plt.suptitle("Distribution of Total Orders (Pre, During, Post Lockdown)")
    plt.show()

    # Histograms for average number of orders
    df[avg_order_cols].hist(bins=20, figsize=(14,4))
    plt.suptitle("Distribution of Average Number of Orders (Pre, During, Post Lockdown)")
    plt.show()

    # Pairplot to inspect relationships
    selected_cols = spend_columns + order_columns + avg_order_cols
    sns.pairplot(df[selected_cols])
    plt.suptitle("Pairplot of Spend and Order Metrics", y=1.02)
    plt.show()

    # Correlation heatmap
    corr = df[selected_cols].corr()
    plt.figure(figsize=(10,8))
    sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
    plt.title("Correlation Heatmap of Spend and Order Metrics")
    plt.show()

    # ----- B. Regression Analysis -----
    # Example 1: Predicting during_lockdown_avg_spend from pre_lockdown_avg_spend and orders during lockdown
    formula1 = 'during_lockdown_avg_spend ~ pre_lockdown_avg_spend + during_lockdown_orders + during_lockdown_average_number_orders'
    model1 = smf.ols(formula=formula1, data=df).fit()
    print("\nRegression Results: Predicting During Lockdown Average Spend")
    print(model1.summary())

    # Example 2: Predicting post_lockdown_avg_spend from pre_lockdown_avg_spend and orders post lockdown
    formula2 = 'post_lockdown_avg_spend ~ pre_lockdown_avg_spend + post_lockdown_orders + post_lockdown_average_number_orders'
    model2 = smf.ols(formula=formula2, data=df).fit()
    print("\nRegression Results: Predicting Post Lockdown Average Spend")
    print(model2.summary())

    # ----- C. Machine Learning: Predicting Post-Lockdown Average Spend -----
    features = ['pre_lockdown_avg_spend', 'during_lockdown_avg_spend',
                'pre_lockdown_orders', 'during_lockdown_orders',
                'pre_lockdown_average_number_orders', 'during_lockdown_average_number_orders']
    target = 'post_lockdown_avg_spend'
    X = df[features].copy()
    y = df[target].copy()

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
    rf_model.fit(X_train, y_train)

    cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
    rmse_cv = np.mean(np.sqrt(-cv_scores))
    print("\nRandom Forest CV RMSE: {:.2f}".format(rmse_cv))

    y_pred = rf_model.predict(X_test)
    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))
    print("Random Forest Test RMSE: {:.2f}".format(rmse_test))

    # Plot feature importances
    feature_importances = pd.Series(rf_model.feature_importances_, index=features)
    feature_importances.sort_values(ascending=True).plot(kind='barh')
    plt.title("Feature Importances for Predicting Post Lockdown Average Spend")
    plt.xlabel("Importance")
    plt.show()

    # ----- D. Causal Analysis using DoWhy -----
    # We will estimate the causal effect of change_during_avg_spend_numeric on during_lockdown_orders.
    # Use a few pre-lockdown metrics as common causes.
    if 'change_during_avg_spend_numeric' in df.columns:
        treatment = 'change_during_avg_spend_numeric'
        outcome = 'during_lockdown_orders'
        common_causes = ['pre_lockdown_avg_spend', 'pre_lockdown_orders', 'pre_lockdown_average_number_orders']

        causal_df = df[[treatment, outcome] + common_causes].dropna()

        # Specify the causal model
        causal_model = CausalModel(
            data=causal_df,
            treatment=treatment,
            outcome=outcome,
            common_causes=common_causes
        )

        # View the causal graph (a temporary file "causal_model.png" will be created)
        causal_model.view_model()

        # Identify the causal effect using the backdoor criterion
        identified_estimand = causal_model.identify_effect()
        print("\nIdentified estimand:")
        print(identified_estimand)

        # Estimate the effect using linear regression as a demonstration
        try:
            effect_estimate = causal_model.estimate_effect(identified_estimand, method_name="backdoor.linear_regression")
            print("\nCausal Estimate using Linear Regression:")
            print(effect_estimate.value)

            # Refute the estimate (placebo test)
            refute_results = causal_model.refute_estimate(identified_estimand, effect_estimate,
                                                          method_name="placebo_treatment_refuter",
                                                          placebo_type="permute")
            print("\nRefutation Results:")
            print(refute_results)
        except Exception as e:
            print("\nError in estimating causal effect:", e)
    else:
        print("Column 'change_during_avg_spend_numeric' not found. Skipping causal analysis.")

# =============================================================================
# 4. SEGMENTED ANALYSIS BY MAJOR CATEGORY
# =============================================================================
if 'Major Category' in df.columns:
    # Get the list of unique major categories
    major_categories = df['Major Category'].unique()
    print("\nPerforming detailed analysis for each Major Category:")

    for maj in major_categories:
        df_maj = df[df['Major Category'] == maj]
        print(f"\n========== Analysis for Major Category: {maj} ==========")
        print(f"Number of observations: {len(df_maj)}")

        # ----- A. Summary Statistics -----
        print("\nSummary Statistics for Spend Metrics:")
        print(df_maj[spend_columns].describe())

        # ----- B. Boxplots for Average Spends -----
        plt.figure(figsize=(10,6))
        sns.boxplot(data=df_maj[spend_columns])
        plt.title(f"{maj}: Boxplot of Average Spends (Pre, During, Post Lockdown)")
        plt.show()

        # ----- C. Boxplots for Orders -----
        plt.figure(figsize=(10,6))
        sns.boxplot(data=df_maj[order_columns])
        plt.title(f"{maj}: Boxplot of Total Orders (Pre, During, Post Lockdown)")
        plt.show()

        # ----- D. Line Plots of the Mean Metrics over Time -----
        # Create a summary DataFrame with mean values for each period
        summary_df = pd.DataFrame({
            'Period': ['Pre', 'During', 'Post'],
            'Average Spend': [df_maj['pre_lockdown_avg_spend'].mean(),
                              df_maj['during_lockdown_avg_spend'].mean(),
                              df_maj['post_lockdown_avg_spend'].mean()],
            'Total Orders': [df_maj['pre_lockdown_orders'].mean(),
                             df_maj['during_lockdown_orders'].mean(),
                             df_maj['post_lockdown_orders'].mean()],
            'Average Number of Orders': [df_maj['pre_lockdown_average_number_orders'].mean(),
                                         df_maj['during_lockdown_average_number_orders'].mean(),
                                         df_maj['post_lockdown_average_number_orders'].mean()]
        })

        # Plot mean spend over time
        plt.figure(figsize=(10,6))
        sns.lineplot(x='Period', y='Average Spend', data=summary_df, marker='o')
        plt.title(f"{maj}: Mean Average Spend over Lockdown Periods")
        plt.show()

        # Plot mean total orders over time
        plt.figure(figsize=(10,6))
        sns.lineplot(x='Period', y='Total Orders', data=summary_df, marker='o', color='green')
        plt.title(f"{maj}: Mean Total Orders over Lockdown Periods")
        plt.show()

        # Plot mean average number of orders over time
        plt.figure(figsize=(10,6))
        sns.lineplot(x='Period', y='Average Number of Orders', data=summary_df, marker='o', color='red')
        plt.title(f"{maj}: Mean Average Number of Orders over Lockdown Periods")
        plt.show()

        # ----- E. Correlation Heatmap for the Major Category -----
        if len(df_maj) > 1:  # Only if enough data is available
            cols_to_plot = spend_columns + order_columns + avg_order_cols
            corr_maj = df_maj[cols_to_plot].corr()
            plt.figure(figsize=(10,8))
            sns.heatmap(corr_maj, annot=True, cmap="coolwarm", fmt=".2f")
            plt.title(f"{maj}: Correlation Heatmap")
            plt.show()
        else:
            print("Not enough data to plot correlation heatmap for this category.")
else:
    print("Column 'Major Category' not found in the dataset. Skipping segmented analysis.")

# =============================================================================
# END OF ANALYSIS
# =============================================================================

# Install DoWhy for causal inference (if not already installed)
!pip install dowhy --quiet

# =======================
# Import Required Libraries
# =======================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# For regression analysis
import statsmodels.api as sm
import statsmodels.formula.api as smf

# For machine learning
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# For causal analysis
import dowhy
from dowhy import CausalModel

# Set visualization style
sns.set(style="whitegrid")
plt.rcParams['figure.figsize'] = (12,8)

# =======================
# 1. LOAD THE DATA
# =======================
# Update the filename/path if needed
data_path = '/content/spends data.csv'
df = pd.read_csv(data_path)

# Print basic info
print("Columns in the dataset:")
print(df.columns.tolist())
print("\nFirst 5 rows of the dataset:")
print(df.head())

# =======================
# 2. PREPARE THE DATA
# =======================
# (a) Convert the change indicator (string) to a numeric variable.
#     Mapping: 'increasing' -> 1, 'no change' -> 0, 'decreasing' -> -1.
if "change_during_avg_spend" in df.columns:
    change_map = {"increasing": 1, "no change": 0, "decreasing": -1}
    df["change_during_avg_spend_numeric"] = df["change_during_avg_spend"].map(change_map)
    if df["change_during_avg_spend_numeric"].isnull().any():
        print("Warning: Some values in 'change_during_avg_spend' could not be mapped to numeric values.")
else:
    print("Column 'change_during_avg_spend' not found. Causal analysis will be skipped.")

# (b) Define the list of Major Categories for which you want detailed analysis.
major_categories_list = [
    "Electronics", "Sporting Goods", "Apparel", "Baby Product", "Health and Beauty", "Books",
    "Software", "Movies and TV", "Gift Cards", "Home", "Tools & Home Improvement", "Kitchen",
    "Lawn & Patio", "Music", "Office Product", "Personal Computers", "Pet Supplies", "Sports",
    "Toy", "Video Game", "Wireless Phone Accessory", "Misc.", "Grocery", "Hobbies",
    "Auto Accessory", "Jewelry", "Musical Instruments", "Collectibles", "Lab Supply"
]

# Filter the DataFrame to only include rows whose 'Major category' is in our list.
df = df[df["Major category"].isin(major_categories_list)]

# Define columns for later use:
spend_cols = ["pre_lockdown_avg_spend", "during_lockdown_avg_spend", "post_lockdown_avg_spend"]
order_cols = ["pre_lockdown_orders", "during_lockdown_orders", "post_lockdown_orders"]
avg_order_cols = ["pre_lockdown_average_number_orders", "during_lockdown_average_number_orders", "post_lockdown_average_number_orders"]

# =======================
# 3. ANALYSIS PER MAJOR CATEGORY
# =======================
for maj in major_categories_list:
    df_maj = df[df["Major category"] == maj]
    if df_maj.empty:
        continue  # Skip if no data for this category.

    print(f"\n========== Analysis for Major Category: {maj} ==========")

    # ----- A. Summary Statistics -----
    print("\nSpend Metrics Summary:")
    print(df_maj[spend_cols].describe())

    print("\nOrders Metrics Summary:")
    print(df_maj[order_cols].describe())

    # ----- B. Visualization of Spend Metrics -----
    # Boxplot of average spends
    plt.figure()
    sns.boxplot(data=df_maj[spend_cols])
    plt.title(f"{maj} - Boxplot of Average Spends (Pre, During, Post Lockdown)")
    plt.show()

    # Line plot of mean spend over periods
    mean_spend = df_maj[spend_cols].mean()
    periods = ["Pre", "During", "Post"]
    plt.figure()
    plt.plot(periods, mean_spend, marker="o", linestyle="--")
    plt.title(f"{maj} - Mean Average Spend Over Lockdown Periods")
    plt.xlabel("Period")
    plt.ylabel("Average Spend")
    plt.show()

    # ----- C. Regression Analysis -----
    # Example regression: Predict during_lockdown_avg_spend using:
    # pre_lockdown_avg_spend, during_lockdown_orders, and during_lockdown_average_number_orders.
    regression_formula = "during_lockdown_avg_spend ~ pre_lockdown_avg_spend + during_lockdown_orders + during_lockdown_average_number_orders"
    try:
        reg_model = smf.ols(formula=regression_formula, data=df_maj).fit()
        print(f"\nRegression Results for {maj} (Predicting during_lockdown_avg_spend):")
        print(reg_model.summary())
    except Exception as e:
        print(f"Regression error for {maj}: {e}")

    # ----- D. Machine Learning Prediction -----
    # Predict post_lockdown_avg_spend from several features.
    features = [
        "pre_lockdown_avg_spend", "during_lockdown_avg_spend",
        "pre_lockdown_orders", "during_lockdown_orders",
        "pre_lockdown_average_number_orders", "during_lockdown_average_number_orders"
    ]
    target = "post_lockdown_avg_spend"
    try:
        X = df_maj[features]
        y = df_maj[target]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
        rf_model.fit(X_train, y_train)
        y_pred = rf_model.predict(X_test)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        print(f"\nRandom Forest RMSE for {maj} (Predicting {target}): {rmse:.2f}")

        # Plot feature importances
        feat_importances = pd.Series(rf_model.feature_importances_, index=features)
        feat_importances.sort_values(ascending=True).plot(kind="barh")
        plt.title(f"{maj} - Feature Importances for Predicting {target}")
        plt.xlabel("Importance")
        plt.show()
    except Exception as e:
        print(f"Machine Learning error for {maj}: {e}")

    # ----- E. Causal Analysis (Using DoWhy) -----
    # Estimate the causal effect of change_during_avg_spend_numeric on during_lockdown_orders,
    # controlling for pre_lockdown_avg_spend, pre_lockdown_orders, and pre_lockdown_average_number_orders.
    if "change_during_avg_spend_numeric" in df_maj.columns:
        try:
            causal_df = df_maj[["change_during_avg_spend_numeric", "during_lockdown_orders",
                                "pre_lockdown_avg_spend", "pre_lockdown_orders", "pre_lockdown_average_number_orders"]].dropna()

            causal_model = CausalModel(
                data=causal_df,
                treatment="change_during_avg_spend_numeric",
                outcome="during_lockdown_orders",
                common_causes=["pre_lockdown_avg_spend", "pre_lockdown_orders", "pre_lockdown_average_number_orders"]
            )

            # Generate and display the causal graph (this creates a temporary file)
            causal_model.view_model()

            identified_estimand = causal_model.identify_effect()
            print("\nIdentified estimand:")
            print(identified_estimand)

            effect_estimate = causal_model.estimate_effect(identified_estimand, method_name="backdoor.linear_regression")
            print(f"\nCausal Effect Estimate for {maj} (Effect of change_during_avg_spend_numeric on during_lockdown_orders): {effect_estimate.value}")

            # Refutation: Run a placebo treatment refuter
            refute_results = causal_model.refute_estimate(identified_estimand, effect_estimate,
                                                          method_name="placebo_treatment_refuter",
                                                          placebo_type="permute")
            print("\nRefutation Results:")
            print(refute_results)
        except Exception as e:
            print(f"Causal analysis error for {maj}: {e}")
    else:
        print(f"Causal variable 'change_during_avg_spend_numeric' not available for {maj}. Skipping causal analysis.")

# =======================
# END OF ANALYSIS
# =======================

# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import f_oneway

# Set visualization style
sns.set(style="whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

# =============================================================================
# 1. LOAD THE DATA
# =============================================================================
# Update the file name/path as needed.
data_path = '/content/spends data.csv'
df = pd.read_csv(data_path)

# Display basic info
print("Columns in the dataset:")
print(df.columns.tolist())
print("\nFirst 5 rows of the dataset:")
print(df.head())

# =============================================================================
# 2. DEFINE MAJOR CATEGORIES & METRIC COLUMN NAMES
# =============================================================================
# List of major categories to consider (update if necessary)
major_categories_list = [
    "Electronics", "Sporting Goods", "Apparel", "Baby Product", "Health and Beauty", "Books",
    "Software", "Movies and TV", "Gift Cards", "Home", "Tools & Home Improvement", "Kitchen",
    "Lawn & Patio", "Music", "Office Product", "Personal Computers", "Pet Supplies", "Sports",
    "Toy", "Video Game", "Wireless Phone Accessory", "Misc.", "Grocery", "Hobbies",
    "Auto Accessory", "Jewelry", "Musical Instruments", "Collectibles", "Lab Supply"
]

# Filter the DataFrame to only include the major categories of interest.
df = df[df["Major category"].isin(major_categories_list)]

# Define the metrics (column names) to analyze.
metrics = {
    "avg_spend_pre": "pre_lockdown_avg_spend",
    "avg_spend_during": "during_lockdown_avg_spend",
    "avg_spend_post": "post_lockdown_avg_spend",
    "orders_pre": "pre_lockdown_orders",
    "orders_during": "during_lockdown_orders",
    "orders_post": "post_lockdown_orders",
    "avg_num_orders_pre": "pre_lockdown_average_number_orders",
    "avg_num_orders_during": "during_lockdown_average_number_orders",
    "avg_num_orders_post": "post_lockdown_average_number_orders"
}

# =============================================================================
# 3. CREATE A SUMMARY TABLE BY MAJOR CATEGORY
# =============================================================================
# For each metric, compute the mean, standard deviation, and count for each major category.
summary_table = df.groupby("Major category").agg({col: ['mean', 'std', 'count'] for col in metrics.values()})
summary_table = summary_table.sort_index()
print("\nSummary Table (by Major Category):")
print(summary_table)

# =============================================================================
# 4. VISUALIZATIONS
# =============================================================================
# (A) Boxplots for each metric across major categories.
for metric_name, col in metrics.items():
    plt.figure()
    sns.boxplot(x="Major category", y=col, data=df)
    plt.title(f"Distribution of {col} by Major Category")
    plt.xlabel("Major Category")
    plt.ylabel(col)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# (B) Line plots showing the mean value for each metric over time (Pre, During, Post)
# Prepare a DataFrame that aggregates the mean per major category for each period.
periods = ['Pre', 'During', 'Post']
# For each metric type (Spend, Orders, Avg Number Orders), create one combined plot.
metric_groups = {
    "Average Spend": [metrics["avg_spend_pre"], metrics["avg_spend_during"], metrics["avg_spend_post"]],
    "Total Orders": [metrics["orders_pre"], metrics["orders_during"], metrics["orders_post"]],
    "Average Number of Orders": [metrics["avg_num_orders_pre"], metrics["avg_num_orders_during"], metrics["avg_num_orders_post"]]
}

for group_name, cols in metric_groups.items():
    # Create an aggregated DataFrame with rows = major categories, columns = period means.
    agg_df = df.groupby("Major category")[cols].mean()
    agg_df.columns = periods  # Rename columns to Pre, During, Post
    agg_df = agg_df.sort_index()

    # Plotting
    agg_df.plot(marker="o")
    plt.title(f"{group_name} - Mean Values by Major Category")
    plt.xlabel("Major Category")
    plt.ylabel(group_name)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# =============================================================================
# 5. STATISTICAL TESTING: ONE-WAY ANOVA ACROSS MAJOR CATEGORIES
# =============================================================================
# For each metric, we will test if there is a significant difference among major categories.
anova_results = {}

for metric_name, col in metrics.items():
    # Prepare the groups: one list per major category (only if the group has more than one observation)
    groups = [group[col].dropna().values for name, group in df.groupby("Major category") if len(group[col].dropna()) > 1]
    if len(groups) > 1:
        f_val, p_val = f_oneway(*groups)
        anova_results[col] = {"F-value": f_val, "p-value": p_val}
    else:
        anova_results[col] = {"F-value": None, "p-value": None}

anova_df = pd.DataFrame(anova_results).T
anova_df.index.name = "Metric"
print("\nANOVA Results (One-Way ANOVA across Major Categories):")
print(anova_df)

# =============================================================================
# 6. FINAL ANALYSIS SUMMARY
# =============================================================================
print("\nFINAL ANALYSIS SUMMARY:")
for col, res in anova_results.items():
    if res["p-value"] is not None:
        significance = "SIGNIFICANT" if res["p-value"] < 0.05 else "NOT significant"
        print(f"Metric '{col}': F = {res['F-value']:.3f}, p = {res['p-value']:.4f} -> {significance} differences among major categories.")
    else:
        print(f"Metric '{col}': Not enough data for ANOVA.")

# ===============================
# Setup and Data Loading
# ===============================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import f_oneway

# For regression analysis
import statsmodels.api as sm
import statsmodels.formula.api as smf

# For machine learning
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Set visualization style
sns.set(style="whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

# -------------------------------
# 1. Load the Data
# -------------------------------
data_path = '/content/spends data.csv'  # <-- Change to your file path if needed
df = pd.read_csv(data_path)

# Rename "Major category" (with a space) to a simpler name
if "Major category" in df.columns:
    df.rename(columns={"Major category": "Major_category"}, inplace=True)

print("Columns in the dataset:")
print(df.columns.tolist())
print("\nFirst 5 rows of the dataset:")
print(df.head())

# -------------------------------
# 2. Create Change Metrics (Differences)
# -------------------------------
# These differences show the change from pre-lockdown to during/post lockdown.
df['change_avg_spend_during'] = df['during_lockdown_avg_spend'] - df['pre_lockdown_avg_spend']
df['change_avg_spend_post'] = df['post_lockdown_avg_spend'] - df['pre_lockdown_avg_spend']

df['change_orders_during'] = df['during_lockdown_orders'] - df['pre_lockdown_orders']
df['change_orders_post'] = df['post_lockdown_orders'] - df['pre_lockdown_orders']

df['change_avg_num_orders_during'] = (df['during_lockdown_average_number_orders']
                                      - df['pre_lockdown_average_number_orders'])
df['change_avg_num_orders_post'] = (df['post_lockdown_average_number_orders']
                                    - df['pre_lockdown_average_number_orders'])

# -------------------------------
# 3. Create a Summary Table by Major Category
# -------------------------------
change_cols = ['change_avg_spend_during', 'change_avg_spend_post',
               'change_orders_during', 'change_orders_post',
               'change_avg_num_orders_during', 'change_avg_num_orders_post']

summary_table = df.groupby("Major_category")[change_cols].agg(['mean', 'std', 'count'])
summary_table = summary_table.sort_index()
print("\nSummary Table of Change Metrics by Major Category:")
print(summary_table)

# -------------------------------
# 4. Visualizations: Which Categories Have the Largest Changes?
# -------------------------------

# (A) Bar charts of mean change in average spend (during vs pre) per major category
plt.figure(figsize=(14,8))
mean_change_spend_during = df.groupby("Major_category")['change_avg_spend_during'].mean().sort_values()
mean_change_spend_during.plot(kind='barh', color='skyblue')
plt.xlabel("Mean Change in Average Spend (During - Pre)")
plt.title("Mean Change in Average Spend During Lockdown by Major Category")
plt.show()

plt.figure(figsize=(14,8))
mean_change_spend_post = df.groupby("Major_category")['change_avg_spend_post'].mean().sort_values()
mean_change_spend_post.plot(kind='barh', color='lightgreen')
plt.xlabel("Mean Change in Average Spend (Post - Pre)")
plt.title("Mean Change in Average Spend Post Lockdown by Major Category")
plt.show()

# (B) Similarly for orders (total orders) changes
plt.figure(figsize=(14,8))
mean_change_orders_during = df.groupby("Major_category")['change_orders_during'].mean().sort_values()
mean_change_orders_during.plot(kind='barh', color='salmon')
plt.xlabel("Mean Change in Orders (During - Pre)")
plt.title("Mean Change in Orders During Lockdown by Major Category")
plt.show()

plt.figure(figsize=(14,8))
mean_change_orders_post = df.groupby("Major_category")['change_orders_post'].mean().sort_values()
mean_change_orders_post.plot(kind='barh', color='orchid')
plt.xlabel("Mean Change in Orders (Post - Pre)")
plt.title("Mean Change in Orders Post Lockdown by Major Category")
plt.show()

# (C) Boxplots: Distribution of changes across major categories
plt.figure(figsize=(14,8))
sns.boxplot(x="Major_category", y="change_avg_spend_during", data=df)
plt.xticks(rotation=45)
plt.title("Boxplot: Change in Average Spend (During - Pre) by Major Category")
plt.ylabel("Change in Average Spend (During - Pre)")
plt.show()

# -------------------------------
# 5. Statistical Testing: One-Way ANOVA on Change Metrics
# -------------------------------
anova_results = {}

for col in ['change_avg_spend_during', 'change_orders_during', 'change_avg_num_orders_during',
            'change_avg_spend_post', 'change_orders_post', 'change_avg_num_orders_post']:
    # Prepare groups: each group is an array of values for that change metric per major category.
    groups = [group[col].dropna().values for name, group in df.groupby("Major_category") if len(group[col].dropna()) > 1]
    if len(groups) > 1:
        f_val, p_val = f_oneway(*groups)
        anova_results[col] = {"F-value": f_val, "p-value": p_val}
    else:
        anova_results[col] = {"F-value": None, "p-value": None}

anova_df = pd.DataFrame(anova_results).T
anova_df.index.name = "Change Metric"
print("\nOne-Way ANOVA Results for Change Metrics Across Major Categories:")
print(anova_df)

# -------------------------------
# 6. Regression Analysis: Does Major Category Explain the Change?
# -------------------------------
# For example, regress the change in average spend during lockdown on major category.
# (This creates dummy variables internally for the categorical variable.)
try:
    reg_formula = "change_avg_spend_during ~ C(Major_category)"
    model_reg = smf.ols(formula=reg_formula, data=df).fit()
    print("\nRegression Results: Predicting Change in Average Spend (During - Pre) by Major Category")
    print(model_reg.summary())
except Exception as e:
    print("Error in regression analysis:", e)

# -------------------------------
# 7. Machine Learning Analysis: Predicting the Change in Average Spend
# -------------------------------
# Here we predict the change in average spend during lockdown using pre-lockdown metrics and major category.
# We one-hot encode the major category.
features_ml = ['pre_lockdown_avg_spend', 'pre_lockdown_orders', 'pre_lockdown_average_number_orders']
df_encoded = pd.concat([df[features_ml],
                        pd.get_dummies(df["Major_category"], prefix="maj", drop_first=True)],
                       axis=1)
target_ml = 'change_avg_spend_during'

# Remove any rows with missing values in the features/target
df_ml = df_encoded.join(df[target_ml]).dropna()

X = df_ml.drop(columns=[target_ml])
y = df_ml[target_ml]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("\nRandom Forest Regression:")
print("RMSE for predicting change in average spend (during - pre): {:.2f}".format(rmse))

# Feature importance visualization
feat_importances = pd.Series(rf.feature_importances_, index=X.columns)
feat_importances.sort_values(ascending=True).plot(kind='barh')
plt.title("Feature Importances for Predicting Change in Average Spend (During - Pre)")
plt.xlabel("Importance")
plt.show()

# -------------------------------
# 8. Final Summary
# -------------------------------
print("\nFINAL SUMMARY OF ANALYSIS:")
print("1. The summary table and bar charts above show which major categories have the largest mean changes in average spend and orders (during and post lockdown relative to pre-lockdown).")
print("2. ANOVA tests (see printed table) indicate whether these differences are statistically significant across major categories.")
print("3. Regression results using major category dummy variables quantify the differences (with a baseline category).")
print("4. A Random Forest model predicts the change in average spend using pre-lockdown features and major category, and feature importance shows which factors are most predictive.")

# ===============================
# Setup: Install and Import Libraries
# ===============================
!pip install dowhy --quiet

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import f_oneway

# Regression libraries
import statsmodels.api as sm
import statsmodels.formula.api as smf

# Machine learning libraries
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Causal analysis with DoWhy
import dowhy
from dowhy import CausalModel

# Set visualization style
sns.set(style="whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

# ===============================
# 1. Load and Prepare the Data
# ===============================
data_path = '/content/spends data.csv'  # <-- Adjust the path/filename as needed
df = pd.read_csv(data_path)

# Rename "Major category" for ease of use.
if "Major category" in df.columns:
    df.rename(columns={"Major category": "Major_category"}, inplace=True)

print("Columns in the dataset:")
print(df.columns.tolist())
print("\nFirst 5 rows:")
print(df.head())

# -------------------------------
# 1a. Reshape Data into Long Format for Each Metric
# -------------------------------
# We create three long-format DataFrames: one for average spend, one for total orders, and one for average number orders.
# Each will have a new column "State" with values: "Pre", "During", or "Post".

# Average Spend long format
df_spend = df[['Category', 'Major_category',
               'pre_lockdown_avg_spend', 'during_lockdown_avg_spend', 'post_lockdown_avg_spend']].copy()
df_spend_long = pd.melt(df_spend,
                        id_vars=['Category', 'Major_category'],
                        value_vars=['pre_lockdown_avg_spend', 'during_lockdown_avg_spend', 'post_lockdown_avg_spend'],
                        var_name='State', value_name='avg_spend')
# Map the state names for clarity
state_map = {
    'pre_lockdown_avg_spend': 'Pre',
    'during_lockdown_avg_spend': 'During',
    'post_lockdown_avg_spend': 'Post'
}
df_spend_long['State'] = df_spend_long['State'].map(state_map)

# Total Orders long format
df_orders = df[['Category', 'Major_category',
                'pre_lockdown_orders', 'during_lockdown_orders', 'post_lockdown_orders']].copy()
df_orders_long = pd.melt(df_orders,
                         id_vars=['Category', 'Major_category'],
                         value_vars=['pre_lockdown_orders', 'during_lockdown_orders', 'post_lockdown_orders'],
                         var_name='State', value_name='orders')
state_map_orders = {
    'pre_lockdown_orders': 'Pre',
    'during_lockdown_orders': 'During',
    'post_lockdown_orders': 'Post'
}
df_orders_long['State'] = df_orders_long['State'].map(state_map_orders)

# Average Number of Orders long format
df_avgnum = df[['Category', 'Major_category',
                'pre_lockdown_average_number_orders', 'during_lockdown_average_number_orders', 'post_lockdown_average_number_orders']].copy()
df_avgnum_long = pd.melt(df_avgnum,
                         id_vars=['Category', 'Major_category'],
                         value_vars=['pre_lockdown_average_number_orders', 'during_lockdown_average_number_orders', 'post_lockdown_average_number_orders'],
                         var_name='State', value_name='avg_num_orders')
state_map_avgnum = {
    'pre_lockdown_average_number_orders': 'Pre',
    'during_lockdown_average_number_orders': 'During',
    'post_lockdown_average_number_orders': 'Post'
}
df_avgnum_long['State'] = df_avgnum_long['State'].map(state_map_avgnum)

# ===============================
# 2. Combined Summary Tables and Visualizations Across States and Major Categories
# ===============================
# (A) Summary Table: Mean values per Major Category and State for each metric.
summary_spend = df_spend_long.groupby(['Major_category', 'State'])['avg_spend'].agg(['mean','std','count']).reset_index()
summary_orders = df_orders_long.groupby(['Major_category', 'State'])['orders'].agg(['mean','std','count']).reset_index()
summary_avgnum = df_avgnum_long.groupby(['Major_category', 'State'])['avg_num_orders'].agg(['mean','std','count']).reset_index()

print("\nSummary Table: Average Spend by Major Category and State")
print(summary_spend.head(20))
print("\nSummary Table: Total Orders by Major Category and State")
print(summary_orders.head(20))
print("\nSummary Table: Average Number of Orders by Major Category and State")
print(summary_avgnum.head(20))

# (B) Visualizations: Grouped Bar Charts for Each Metric
def plot_grouped_bar(df_long, value_col, title, ylabel):
    plt.figure(figsize=(14,8))
    sns.barplot(x="Major_category", y=value_col, hue="State", data=df_long, ci="sd")
    plt.title(title)
    plt.xlabel("Major Category")
    plt.ylabel(ylabel)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

plot_grouped_bar(df_spend_long, "avg_spend", "Average Spend by Major Category and State", "Avg Spend")
plot_grouped_bar(df_orders_long, "orders", "Total Orders by Major Category and State", "Orders")
plot_grouped_bar(df_avgnum_long, "avg_num_orders", "Average Number of Orders by Major Category and State", "Avg # Orders")

# (C) Boxplots to visualize distributions
def plot_boxplot(df_long, value_col, title, ylabel):
    plt.figure(figsize=(14,8))
    sns.boxplot(x="Major_category", y=value_col, hue="State", data=df_long)
    plt.title(title)
    plt.xlabel("Major Category")
    plt.ylabel(ylabel)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

plot_boxplot(df_spend_long, "avg_spend", "Boxplot of Average Spend by Major Category and State", "Avg Spend")
plot_boxplot(df_orders_long, "orders", "Boxplot of Total Orders by Major Category and State", "Orders")
plot_boxplot(df_avgnum_long, "avg_num_orders", "Boxplot of Average Number of Orders by Major Category and State", "Avg # Orders")

# ===============================
# 3. Statistical Testing: Two-Way ANOVA
# ===============================
# We run a two-way ANOVA (factors: Major_category and State) for each outcome.
# (A) For Average Spend:
model_spend = smf.ols('avg_spend ~ C(Major_category) + C(State) + C(Major_category):C(State)', data=df_spend_long).fit()
anova_spend = sm.stats.anova_lm(model_spend, typ=2)
print("\nTwo-Way ANOVA Results for Average Spend:")
print(anova_spend)

# (B) For Total Orders:
model_orders = smf.ols('orders ~ C(Major_category) + C(State) + C(Major_category):C(State)', data=df_orders_long).fit()
anova_orders = sm.stats.anova_lm(model_orders, typ=2)
print("\nTwo-Way ANOVA Results for Total Orders:")
print(anova_orders)

# (C) For Average Number of Orders:
model_avgnum = smf.ols('avg_num_orders ~ C(Major_category) + C(State) + C(Major_category):C(State)', data=df_avgnum_long).fit()
anova_avgnum = sm.stats.anova_lm(model_avgnum, typ=2)
print("\nTwo-Way ANOVA Results for Average Number of Orders:")
print(anova_avgnum)

# ===============================
# 4. Regression Analysis
# ===============================
# Example: Regress average spend on Major_category, State, and their interaction.
regression_formula = 'avg_spend ~ C(Major_category) * C(State)'
reg_model = smf.ols(formula=regression_formula, data=df_spend_long).fit()
print("\nRegression Results: Predicting Average Spend by Major Category and State")
print(reg_model.summary())

# ===============================
# 5. Machine Learning: Predicting Post COVID Average Spend
# ===============================
features_ml = [
    'pre_lockdown_avg_spend', 'during_lockdown_avg_spend',
    'pre_lockdown_orders', 'during_lockdown_orders',
    'pre_lockdown_average_number_orders', 'during_lockdown_average_number_orders'
]
target_ml = 'post_lockdown_avg_spend'

# Instead of taking only features_ml, include the target variable as well.
df_ml = df.copy()
# Now concatenate the features and target, plus the one-hot encoded Major_category.
df_ml = pd.concat([df_ml[features_ml + [target_ml]],
                   pd.get_dummies(df_ml['Major_category'], prefix='maj', drop_first=True)], axis=1)

# Now, separate features (X) and target (y)
X = df_ml.drop(columns=[target_ml])
y = df_ml[target_ml]

# Split the data and run the model
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred = rf_model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("\nRandom Forest Regression:")
print("RMSE for predicting post COVID average spend: {:.2f}".format(rmse))

# Plot feature importances
importances = pd.Series(rf_model.feature_importances_, index=X.columns)
importances.sort_values(ascending=True).plot(kind='barh')
plt.title("Feature Importances for Predicting Post COVID Average Spend")
plt.xlabel("Importance")
plt.show()


# ===============================
# 6. Causal Analysis with DoWhy
# ===============================
# As an example, we estimate the causal effect of being in the "During" state versus "Pre" on average spend.
# To do this, we create a subset of the long-format spend data that includes only the Pre and During states.
df_causal = df_spend_long[df_spend_long['State'].isin(['Pre', 'During'])].copy()

# Create a binary treatment variable: treatment = 1 if state is "During", 0 if "Pre"
df_causal['treatment'] = df_causal['State'].apply(lambda x: 1 if x == 'During' else 0)

# For covariates, we include the pre_lockdown average spend (which we can merge from the wide data) and Major_category.
# Here, we assume that each row’s “Pre” value is available as a covariate for both Pre and During observations.
# To do this, merge the Pre values (from df_spend_long) back into df_causal.
pre_vals = df_spend_long[df_spend_long['State']=='Pre'][['Category','avg_spend']].rename(columns={'avg_spend':'pre_avg_spend'})
df_causal = df_causal.merge(pre_vals, on='Category', how='left')

# Specify the causal model:
# Outcome: avg_spend, Treatment: treatment, Covariates: pre_avg_spend and Major_category.
# We convert Major_category to dummy variables (or treat it as a confounder).
model_causal = CausalModel(
    data=df_causal,
    treatment='treatment',
    outcome='avg_spend',
    common_causes=['pre_avg_spend', 'Major_category']
)

# Visualize the causal graph (this will generate a file "causal_model.png")
model_causal.view_model()

# Identify the causal effect using backdoor adjustment.
identified_estimand = model_causal.identify_effect()
print("\nIdentified Causal Estimand:")
print(identified_estimand)

# Estimate the effect using linear regression
estimate = model_causal.estimate_effect(identified_estimand, method_name="backdoor.linear_regression")
print("\nCausal Effect Estimate (During vs Pre on Avg Spend):")
print(estimate.value)

# Refute the estimate using a placebo treatment refuter.
refutation = model_causal.refute_estimate(identified_estimand, estimate, method_name="placebo_treatment_refuter", placebo_type="permute")
print("\nCausal Refutation Results:")
print(refutation)

# ===============================
# 7. Final Summary
# ===============================
print("\nFINAL SUMMARY:")
print("1. The above summary tables and grouped bar/box plots show how average spend, total orders,")
print("   and average number of orders vary across the 30 major categories and across Pre, During, and Post COVID states.")
print("2. Two-way ANOVA tests indicate whether there are statistically significant effects for Major_category,")
print("   Time (State), and their interaction.")
print("3. The regression model quantifies these effects using dummy variables.")
print("4. The Random Forest model predicts post COVID average spend with a reported RMSE and shows feature importances.")
print("5. The DoWhy causal analysis (comparing During vs. Pre) provides an estimate of the causal effect on average spend,")
print("   along with a refutation check to assess robustness.")